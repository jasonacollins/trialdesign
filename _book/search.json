[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Course notes on trial design",
    "section": "",
    "text": "Overview\nIn these notes, I introduce the principles and method of experimental research. You will learn the theory and good practices to design experiments in the field and test hypotheses. We will review confounds which can appear in an experimental study and how to ensure that an experimental project delivers the right insights. We will also examine the generalisability of experiments and how we should review the existing experimental literature.\nThrough the book, I will largely focus on trials for public policy or business applications, in the lab or field, rather than for academic purposes. Many of the principles hold across academic, business and policy settings (in fact, they are often the same as a trial may involve both an academic and a business or policy question), but focusing our attention to the applied behavioural science question will allow us to achieve more depth.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "experimental-foundations/experimental-foundations.html",
    "href": "experimental-foundations/experimental-foundations.html",
    "title": "Experimental foundations",
    "section": "",
    "text": "In this part, I will examine the foundations of the use of experiments to understand behavioural phenomena (and in fact, phenomena across the sciences). We will learn about why we should experiment and some types of experiments. We will also foreshadow some of the considerations in designing an experiment that we will examine through this unit.\nBy the end of this part, you will be able to answer the following questions:\n\nWhat is an experiment? 2.Why do we run experiments?\nWhat are the different types of experiments?\nWhat are some of the elements of experimental design?",
    "crumbs": [
      "Experimental foundations"
    ]
  },
  {
    "objectID": "experimental-foundations/why-experiment.html",
    "href": "experimental-foundations/why-experiment.html",
    "title": "1  Why experiment?",
    "section": "",
    "text": "For several decades, adults with severe head injury were treated using steroid injections. This made perfect sense in principle: steroids reduce swelling, and it was believed that swelling inside the skull killed people with head injuries, crushing their brain. However, these assumptions were not subject to proper tests for some time.\nThen, a decade ago, this assumption was tested in a randomised trial. The study was controversial, and many opposed it, because they thought they already knew that steroids were effective. In fact, when the results were published in 2005, they showed people receiving steroid injections were more likely to die: this routine treatment had been killing people, and in large numbers, because head injuries are so common. These results were so extreme that the trial had to be stopped early, to avoid any additional harm being caused.\nThis is a particularly dramatic example of why fair tests of new and existing interventions are important: without them, we can inflict harm unintentionally, without ever knowing it: and when new interventions become common practice without good evidence, then there can be resistance to testing them in the future.\nHaynes et al. (2012) Test, Learn, Adapt: Developing Public Policy with Randomised Controlled Trials\n\nUnderstanding what works is difficult. The world is complicated. Logic and theory can provide insight, but they do not always lead us to the right answer.\nFurther, we often believe that we want to believe. As Richard Feynman said, “The first principle is that you must not fool yourself – and you are the easiest person to fool.”\nOne approach to understanding the world might be to develop a model (a description of the relationships between the variables you are interested in) and then gather data to test the model. However, it is challenging to make causal statements without a counterfactual as to what would otherwise occur. Would those with head injuries have recovered if they had not been given steroids? There are a variety of approaches to making cause-effect statements in those circumstances (the subject of the other unit this session, Principles of Causal Inference), but these all rest on assumptions of varying robustness.\nOne major problem is what Jim Manzi (2012) calls “causal density”. Study the movement of a planet, and you can assume a single causal factor, gravity. If you examine a new way of disclosing information about your credit card to customers, there are so many possible causes of behaviour that, no matter how sophisticated your tools and models, there is always the possibility of some uncontrolled factor causing what you observe. The high causal density makes inferring causation nearly impossible.\nExperiments provide a more direct way of creating a counterfactual and untangling the causally dense environment by constructing a control group against which to compare outcomes. This provides a foundation for us to move beyond mere statements about correlation and to make causal statements.\nImportantly, experiments don’t provide a definitive answer. But even where there is still room for experts to disagree, the room for reasonable disagreement is often narrowed. They can protect us from being fooled into believing what we want to believe.\n\n\n\n\nHaynes, L., Service, O., Goldacre, B., and Torgerson, D. (2012). Test, Learn, Adapt: Developing Public Policy with Randomised Controlled Trials. https://www.gov.uk/government/publications/test-learn-adapt-developing-public-policy-with-randomised-controlled-trials\n\n\nManzi, J. (2012). Uncontrolled. Basic Books. https://www.hachettebookgroup.com/titles/jim-manzi/uncontrolled/9780465029310/?lens=basic-books",
    "crumbs": [
      "Experimental foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Why experiment?</span>"
    ]
  },
  {
    "objectID": "experimental-foundations/the-experimental-approach-in-economics-and-psychology.html",
    "href": "experimental-foundations/the-experimental-approach-in-economics-and-psychology.html",
    "title": "2  The experimental approach in economics and psychology",
    "section": "",
    "text": "Economists and psychologists use controlled experiments to test what choices people make in specific circumstances.\nMost economics experiments are not pure simulations or role-playing exercises. They involve real people who make real choices to make or lose money.\nResearchers design an experiment that captures the features of some “real world” settings, such as markets. Some participants are assigned the roles of buyers and sellers making trades. Participants have an incentive to think carefully about their decisions since the money they earn from trading is theirs to keep.\nDuring the experiment, researchers can change features of the environment, such as the rules of exchange and the incentives. By observing how the participants’ behaviour changes as the rules or incentives change, they can examine the effects of the intervention. They can then compare the actual results of the experiment with theoretical predictions about how people would respond to the change. \nIn 2002, Vernon Smith won the Nobel Prize in Economics for “having established laboratory experiments as a tool in empirical economic analysis, especially in the study of alternative market mechanisms.”",
    "crumbs": [
      "Experimental foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The experimental approach in economics and psychology</span>"
    ]
  },
  {
    "objectID": "experimental-foundations/the-role-of-experiments.html",
    "href": "experimental-foundations/the-role-of-experiments.html",
    "title": "3  The role of experiments",
    "section": "",
    "text": "Nobel Memorial Prize winner Alvin Roth (1995) categorised the three major uses of experiments in economics as follows:\nSpeaking to theorists: These are experiments designed to test the predictions of theories in a precise, controlled and measured environment that allows the observations to be interpreted in relation to the theory. The experiment is used to fill gaps that are hard to fill with raw empirical data. They are intended to feed back into the theoretical literature, help refine theoretical ideas, and discover irregularities that can help formulate new theories. Experiments can also help to distinguish among theories.\nFor example, game theory was a major driver of the emergence of experimental economics from the 1950s. The body of theory was tested, ultimately leading to fields such as behavioural game theory that seek to explain the gap between the original theoretical predictions and the empirical evidence as to how people play games.\nKahneman and Tversky’s testing of expected utility theory also falls into this category. They tested the theory in controlled experiments to see where it might hold or not. The failure of the theory to predict some of the phenomena fed into new theories, such as Kahneman and Tversky’s own prospect theory.\nSearching for facts: This involves studying the effects of variables about which existing theory may have little to say. It can also be thought of as the search for irregularities or the isolation of the cause of previously observed regularities.\nSome of Kahneman and Tversky’s (1979) early work also falls into this category. They created experiments to test whether people would depart from the rules of logic and probability, identifying a rich set of irregularities that are now the common fodder of behavioural economics.\nWhispering in the ears of princes: Experiments can facilitate a dialogue between experimenters and policymakers or business decision makers. The questions in these experiments are motivated by the types of questions raised by regulatory agencies, government service providers, marketing and pricing teams, or business leaders. Experiments are used to test interventions relating to government policy and programs, and business strategy and tactics.\nThe work of the Behavioural Insights Team is perhaps the best-known example of this, although experiments have been used in both business and policy for almost as long as there has been a discipline of experimental economics. That said, they are rarely used relative to their potential.\n\n\n\n\nKahneman, D., and Tversky, A. (1979). Prospect theory: An analysis of decision under risk. Econometrica, 47(2), 263–291. https://doi.org/10.2307/1914185\n\n\nRoth, A. E. (1995). Introduction to experimental economics (J. H. Kagel and A. E. Roth, Eds.). Princeton University Press. http://doi.org/10.2307/j.ctvzsmff5.5",
    "crumbs": [
      "Experimental foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The role of experiments</span>"
    ]
  },
  {
    "objectID": "experimental-foundations/randomised-trials.html",
    "href": "experimental-foundations/randomised-trials.html",
    "title": "4  Randomised trials",
    "section": "",
    "text": "Our main focus in these notes will be on randomised trials done in the lab or field (e.g. schools, firms, health centres, community) to establish the causal impact of an intervention. The causal impact is the difference in outcomes caused by the intervention. It is the comparison of what happened with the program with what happened without the intervention.\nThe fundamental problem of causal inference: we can never observe the same people at the same time both with and without the intervention. We never observe the counterfactual. So we have to mimic the counterfactual.\n\n\nWhat is the best way to mimic the counterfactual?\n\nCompare the outcome of people who took up the program with the outcome of people who did not take up the programCompare the outcome of people before they take up the program with the outcome of people after they take up the programCompare the outcome of people randomly assigned into the program with the outcome of people randomly out of the program\n\n\n\nCompare the outcome of people randomly assigned into the program with the outcome of people randomly out of the program\n\n\n\n\nWhich of these is NOT a component of a READy workflow? (See here: https://andreashandel.github.io/MADAcourse/content/module-ready-workflow/ready-overview.html)\n\nReproducibleEfficientAnalyticalDocumented\n\n\n\nREADy stands for: Reproducible, efficient, automated, and documented\n\nFalse\nFalse\nTrue\nFalse",
    "crumbs": [
      "Experimental foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Randomised trials</span>"
    ]
  },
  {
    "objectID": "experimental-foundations/lab-versus-field.html",
    "href": "experimental-foundations/lab-versus-field.html",
    "title": "5  Lab versus field",
    "section": "",
    "text": "5.1 Types of experiments\nA common claim against laboratory experiments is that they do not enable conclusions to be drawn about the “real world” Those critiques tend to rest on three foundations:\nThese three critiques mean that lab experiments can have limited relevance for predicting field behaviour unless the aspects of behaviour being studied are general across environments, stakes and subject pools.\nField experiment, by contrast:\nLab experiments are not, however, without their advantages:\nThe result of this balance of costs and benefits means that lab and field experiments should be seen as being methodologically complementary. Each can enable insights that the other can’t. Experiments in one can be used to inform the other.\nWe will return to the question of lab versus field experiments in more detail later when we explore the generalisability of experiments.\nOne major experimental consideration is whether to experiment in the lab or the field.",
    "crumbs": [
      "Experimental foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Lab versus field</span>"
    ]
  },
  {
    "objectID": "experimental-foundations/lab-versus-field.html#types-of-experiments",
    "href": "experimental-foundations/lab-versus-field.html#types-of-experiments",
    "title": "5  Lab versus field",
    "section": "",
    "text": "5.1.1 Criteria that define a field experiment\nHarrison and List (2004) described six criteria that can be used to delineate between lab and field experiments:\nThe nature of the subject pool: Is the subject pool made up of standard subjects (e.g. students), a more representative subject pool, or a target population? Decision making may vary across pools.\nInformation the subjects bring to the task: Do participants being with them experience in the commodity or task that forms the basis of the experiment? Decisions may vary with the experience brought to the task.\nThe nature of the commodity: Does the experiment use actual rather than abstractly defined goods? The artificiality of abstract goods can affect behaviour.\nThe nature of the task or trading rules applied: Is the task the same that is naturally undertaken in the field? As field experience can enable the development of task specific heuristics, variation in the task can shift decision making.\nThe nature of the stakes: Are the stakes in commensurate to that in the field? Stakes in the laboratory and field can vary markedly, which might be the difference between indifference and engagement.\nThe nature of the environment that the subject operates in: Is the environment the one in which the decision naturally occurs? Different settings might engender role playing, or suggest particular strategies or heuristics to use in making decisions.\n\n\n5.1.2 A taxonomy of experiments\nThis criteria can be used to develop a more refined taxonomy of experiments than simply “lab” or “field”, although this is a spectrum rather than a precise taxonomy. Harrison and List developed a spectrum as follows:\nConventional lab experiments: Lab experiments typically use a standard subject pool of students (cheap and convenient), an abstract framing, and an imposed set of rules.\nArtefactual field experiments: Artefactual experiments mimic lab experiments, except that they use non-standard subjects such as people from the market or context of interest.\nAs one example, Michael Haigh and John List got traders from the Chicago Board of Trade to play games to test whether they exhibit myopic loss aversion. (You might recall from 23713 Behavioural economics and corporate decision making that myopic loss aversion was Thaler and Benartzi’s explanation for the equity premium puzzle.) Would professional traders with experience trading make better decisions? Surprisingly, they found that the traders were even more myopically loss averse in a lab environment than the typical student subjects.\nAnother example is work by Joe Henrich and colleagues, who recruited subjects from 15 small-scale societies to play the ultimatum game. (Recall that the ultimatum game involves a proposer offering a split with a respondent. If the respondent accepts, they keep the split. If they reject, they both get nothing.) These subjects came from three foraging societies, six societies that practice slash-and-burn horticulture, four nomadic herding groups, and three sedentary agriculturalist societies.\nAlthough no group exhibited the optimal game theoretic solution (offer the smallest possible non-zero sum, accept), Henrich and colleagues found that there was material behavioural variability across the groups. No group offered, on average, less than 25%. But two groups had average initial offers over 50%. Rejection rates also varied materially. Henrich and colleagues argued that economic organisation and the degree of market integration explained a substantial portion of this variation.\nFramed field experiment: Framed field experiments incorporate elements of the naturally-occurring environment, such as the commodity, tasks, stakes and information sets of the subjects. Subjects understand they are taking part in an experiment and that their behaviour is recorded and scrutinised.\nAs an example framed field experiment, John List tested expected utility and prospect theory at a sportscard show. He endowed experienced traders with a mug or bar of chocolate and give them an opportunity to trade (as per the famous experiments reported by Daniel Kahneman, Jack Knetsch and Richard Thaler). List found that, among inexperienced consumers, prospect theory adequately explained their behaviour. However, those with intense market experience behaved in accordance with neoclassical predictions.\nNatural field experiments: Natural field experiments are similar to framed field experiments except that the environment is one where the subjects naturally undertake these tasks and where the subjects do not know that they are in an experiment.\nNatural field experiments are different from “natural experiments”, a subject you will look at in depth in Principles of Causal Inference. Natural field experiments use treatments constructed by the experimenter to test a hypothesis. Natural experiments use naturally created randomness across treatments to draw conclusions from naturally occurring data. The experimenter has less (usually no) control in natural experiments. Field experiments might be seen as providing a bridge between lab experiments and naturally occurring data, giving mixture of realism and control that you can’t achieve otherwise.\nYou came across a natural field experiment in a previous unit where we saw Marianne Bertrand and Sendhil Mullainathan’s tests of labour market discrimination. They sent manipulated CVs tin response to ads in Boston and Chicago, finding that white names received 50% more callbacks for interviews than African-American soundings names.",
    "crumbs": [
      "Experimental foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Lab versus field</span>"
    ]
  },
  {
    "objectID": "experimental-foundations/lab-versus-field.html#control",
    "href": "experimental-foundations/lab-versus-field.html#control",
    "title": "5  Lab versus field",
    "section": "5.2 Control",
    "text": "5.2 Control\nA primary requirement of a good experiment is that it tests or controls for alternative hypotheses. If you were to test the success of an intervention to increase savings in December without controlling for the possibility that expenditure tends to increase in the lead up to Christmas, you are going to have a poor experiment.\nThere are two ways in which we achieve control: directly and indirectly. These are not either/or options. Rather, we tend to use both direct and indirect control together in an experiment.\n\n5.2.1 Direct experimental control\nIn a lab experiment, you can directly control many variables. You choose which to keep constant across the experimental participants, such as the rules of the game that they play or their initial endowment. Variables held constant are control variables.\nYou also choose which variables you will vary. Variables that are changed are called treatment variables. If you want to to test the effect of a text message to some people and not others, that controlled variation is your experimental treatment.\nTypically you will test hypotheses or behavioural interventions by changing one variable at a time. You only change variables which are directly relevant to the hypothesis being tested, otherwise holding the environment fixed. This can help to avoid confounds.\nIn the field, you control fewer variables, although you maintain direct control of the treatment effects.\n\n\n5.2.2 Indirect experimental control\nMany variables are difficult to control directly, particularly in the field. For our advertising example above, it’s hard to cancel Christmas. You might think that you could compare sales year-on-year, but is this Christmas the same as the last (a salient problem this year!). More subtlety, the customers who will see your intervention may have different propensities to save that those who don’t. You can’t directly control that.\nYou can measure variables which you think may affect the propensity to save: gender, age, income, etc, and adjust the analysis after the fact. But there will be more factors than you capture, and likely some important ones that you don’t even realise are relevant. These uncontrolled factors will ultimately undermine your experimental conclusions.\nThere is, however, an indirect way to achieve control: randomisation. By randomly assigning experimental participants to different treatments, we can eliminate differences between the subjects as a cause of differences between treatments. We will discuss how randomisation works on the next page.\n\n\n\n\nHarrison, G. W., and List, J. A. (2004). Field Experiments. Journal of Economic Literature, 47.",
    "crumbs": [
      "Experimental foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Lab versus field</span>"
    ]
  },
  {
    "objectID": "experimental-foundations/are-randomised-controlled-trials-ethical.html",
    "href": "experimental-foundations/are-randomised-controlled-trials-ethical.html",
    "title": "6  Are randomised controlled trials ethical?",
    "section": "",
    "text": "Randomized experiments—long the gold standard in medicine—are increasingly used throughout the social sciences and professions to evaluate business products and services, government programs, education and health policies, and global aid. We find robust evidence—across 16 studies of 5,873 participants from three populations spanning nine domains—that people often approve of untested policies or treatments (A or B) being universally implemented but disapprove of randomized experiments (A/B tests) to determine which of those policies or treatments is superior. This effect persists even when there is no reason to prefer A to B and even when recipients are treated unequally and randomly in all conditions (A, B, and A/B). This experimentation aversion may be an important barrier to evidence-based practice.\nMeyer et al. (2019)\n\nIs it ethical to withhold an intervention that may benefit someone by assigning them to a control group?\nA common response to this question is that this effectively already occurs in many instances without trials:\n\nInterventions are often piloted, which is equivalent to excluding a group of people who could benefit.\nInterventions are often scaled up, meaning that it takes time for everyone to receive the intervention.\n\nA randomised controlled trial might be considered more ethical than either of those scenarios as it is similarly a phased introduction, but with a mechanism to determine its effectiveness and improve future outcomes. Absent a robust test, we need to be clear about the limits of our knowledge. There are many cases where an intervention assumed to be helpful was later found to be ineffective or harmful.\nFurther, trials often have protocols that in the case of large early effects indicating success or harm, they can be ceased or implemented more rapidly.\nAnother related question is whether it is fair to experiment on people at all. Don’t we risk harm?\nOne response is that every time you roll out a new program, product, communication or tool that may change behaviour or affect their wellbeing, you are running an experiment. It’s just that if you’re doing it absent a control group or some other mechanism to determine effectiveness, your experiment does not even have the benefit of enabling you to know whether it works, or is helping or harming people.\n\n\n\n\nMeyer, M. N., Heck, P. R., Holtzman, G. S., Anderson, S. M., Cai, W., Watts, D. J., and Chabris, C. F. (2019). Objecting to experiments that compare two unobjectionable policies or treatments. Proceedings of the National Academy of Sciences, 116(22), 10723–10728. https://doi.org/10.1073/pnas.1820701116",
    "crumbs": [
      "Experimental foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Are randomised controlled trials ethical?</span>"
    ]
  },
  {
    "objectID": "before-the-trial/before-the-trial.html",
    "href": "before-the-trial/before-the-trial.html",
    "title": "Before the trial",
    "section": "",
    "text": "In this part, I cover some steps you will take before trial design.\nBy the end of this module, you will be able to answer the following questions:\n\nHow might you get buy-in for your trial?\nHow do you define and diagnose a behavioural problem?\nHow do you design or select an intervention?",
    "crumbs": [
      "Before the trial"
    ]
  },
  {
    "objectID": "before-the-trial/organisational-buy-in.html",
    "href": "before-the-trial/organisational-buy-in.html",
    "title": "7  Organisational buy-in",
    "section": "",
    "text": "Before a trial, there is a need to get the organisation to buy into the trial and execute the experiment. Many proposed experiments have been scuttled by a failure to accept experimentation as a way of determining what works, by running into organisational barriers or by changes in priorities.\nJohn List is a pioneer of the use of field experiments in economics. We have seen some of his experiments in this and previous units. He has run experiments involving airline pilots, traders, fisherman, card collectors and CEOs. He has even started his own company to test ideas around hiring and job performance.\nIn an article containing tips on how to run a field experiment (List, 2011), he offered the following ideas:\nHave a champion within the organization — the higher up the better: Make the experiment a “we” project instead of an “us versus them” pursuit. Senior champions can be the catalyst for others in the organisation to help.\nUnderstand organizational dynamics: If someone is hampering your efforts, turn this person to your side. Insiders can always find a way to stop your field experiment.\nOrganizations that have “skin in the game” are more likely to execute your design and use your results to further organizational objectives: If the organization has invested resources, even sunk costs, the organisation is more likely to complete the project and use the results afterwards.\nRun the field experiment yesterday rather than tomorrow: If you have an open window, take advantage of it as the unexpected will often close it.\nChange the nature of the discussion of the cost of the experiment: Counter claims that the experiment will cost the firm too much money with a response that we are “costing” the firm too much money by not experimenting.\nMake clear that you do not have all the answers: An organization may not welcome an outsider who claims to arrive with all of the answers. Rather, say that you have the tools to discover them.\nBe open to running experiments that might not provide high-powered research findings in the short run: Get your foot in the door by conducting experiments that are not intellectually satisfying so you can run more intellectually interesting experiments in the future.\n\n\n\n\nList, J. A. (2011). Why Economists Should Conduct Field Experiments and 14 Tips for Pulling One Off. Journal of Economic Perspectives, 25(3), 3–16. https://doi.org/10.1257/jep.25.3.3",
    "crumbs": [
      "Before the trial",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Organisational buy-in</span>"
    ]
  },
  {
    "objectID": "trial-basics.html",
    "href": "trial-basics.html",
    "title": "8  Trial basics",
    "section": "",
    "text": "8.1 Randomisation\nA good experiment tends to have a number of design features. These include that:\nIn this chapter, we will look at the basic structure of a trial that has these features. We will look at the concept of control and how this is implemented in randomised controlled trials. We will examine how to develop a question and how to measure trial outcomes.\nWe will also cover another important consideration at the beginning of any trial, ethics.\nIn its simplest form, randomised trials work by splitting trial participants into two or more groups by random lot. Each group is then given a different intervention, with one of those groups typically a “control group” or “test group” that receives no intervention or the status quo.\nGroups might be allocated randomly by drawing numbers out of a hat, flipping a coin or, more commonly in experimental work, using a pseudo-random number generator. It is not the experimenter that decides who gets an intervention or not, but rather chance.\nRandomisation works as a control technique because it enables experimenters to hold approximately equal the sources of experimental bias, such as uncontrolled variables, between the control and treatment groups. These might even be variables of which we are ignorant.\nIn Uncontrolled: The Surprising Payoff of Trial-and-Error for Business Politics and Society, Jim Manzi gives the following example:\nRandomisation also avoids the need for a detailed understanding of the mechanism underlying the difference in outcomes between groups. James Lind conducted what many considered to be the first clinical trial in 1747 when he gave six scurvy stricken sailors citrus juice, while denying the treatment to another six. He did not need to know that vitamin C was the mechanism, nor anything about human biology to see if it worked.\nRandomisation relies on the law of large numbers, the idea that as sample size increases the sample average converges to the expected value. In the case of the experiment to reduce blood pressure, as the size of the groups increased, we would expect the proportion of people with the hypertension genetic variant to converge to around 10% in each group.\nTo think of what that means intuitively, if you had only ten in each group, there is a material chance the groups could have zero, one, or more people with the hypertension variant, although it would rarely be three or more. Therefore, with a small sample, the relative proportions of the variant vary markedly between groups. It is only be collecting large samples that we can expect the groups to approximately equal proportions.\nIn the third week of this module (week four of this unit), we will examine how to determine the required sample size.\nAfter randomisation and application of the treatment, outcomes are then measured for each group. Within certain statistical parameters (also to be covered later in this unit), we can then take the differences between the two groups to be as a result of the different interventions we received. The treatment “caused” the differences, although as noted above, we may not understand the mechanism.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Trial basics</span>"
    ]
  },
  {
    "objectID": "trial-basics.html#randomisation",
    "href": "trial-basics.html#randomisation",
    "title": "8  Trial basics",
    "section": "",
    "text": "[S]uppose researchers in 1950 wanted to test the efficacy of a pill designed to reduce blood pressure but did not know that about 10 percent of the human species has a specific gene variant that predisposes them to adult-onset hypertension. If the researchers selected 3,000 people, and randomly assigned 1,500 to a test group who are given the pill and 1,500 to a control group who received a placebo, then about 150 patients in each group should have the gene variant of interest (though the researchers would have no explicit information about this and wouldn’t even have thought to investigate it). Therefore, when these researchers compared the change in blood pressure before and after taking the pill for the test group versus the control group, their estimate would not be biased by a much higher proportion of patients with the gene variant of interest in one group or the other",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Trial basics</span>"
    ]
  },
  {
    "objectID": "trial-basics.html#developing-an-intervention-for-trial",
    "href": "trial-basics.html#developing-an-intervention-for-trial",
    "title": "8  Trial basics",
    "section": "8.2 Developing an intervention for trial",
    "text": "8.2 Developing an intervention for trial\nApplied behavioural science projects are typically designed to change behaviour (or at least to improve an outcome caused by a certain behaviour). The trial is one step of that project.\nThrough that project, you will ask many questions that will inform the interventions that you wish to trial. For instance, your sequence of questions might be:\n\nWhat is the problem you are trying to solve?\nWhat behaviour is leading to the outcome?\nWhat is our theory of the current behaviour?\nWhat interventions might influence the behaviour?\n\nYou might answer those questions through process such as that described in Module 1 [add link to page 1.4]. Once you have answered them, you will have a shortlist of interventions that you could trial. The purpose of the trial is to differentiate between the effectiveness of those behavioural interventions. Are any of the interventions more effective in improving your outcome of interest?\nThere are some other practical questions you need to ask in selecting interventions for trial.\nFirst, you want the choice of interventions to help answer an interesting question. You want to learn something. For example, has the intervention been found effective or ineffective in similar contexts before? Will you learn something new from trying it again? If text message reminders have been found consistently effective in similar scenarios, a test of text messages versus a control of no message may not be informative. However, a trial varying the content of the message and the theoretical underpinning of that message might be.\nSecond, you want to select interventions that you can deliver consistently at scale. For the experiment, you want everyone within each group (control and treatments) to receive the same interventions as others in the group. But more importantly, what would happen if your trial was successful? What will it practically look like at scale relative to your perfect world conception of the intervention? If you have found an intervention to be highly effective, but it is not feasible to roll out at scale, your experiment is not useful.\nThe process of selecting interventions for an applied behavioural science trial is usually conceptually simpler than developing interventions for an academic experiment designed to test a theory. In that case, it is important that there is not a confounding theory that gives an equally plausible rationale for the behaviour observed in the experiment. The experimenter needs to understand the most plausible hypotheses that should be controlled for, which might depend on recent developments in theory. An experiment may only appear good at the time, as subsequent work may expose its theoretical flaws.\nBut that is not to say that you shouldn’t think about the conceptual and theoretical underpinnings of your interventions in an applied behavioural science trial. There will typically be a theoretical basis for your choice of interventions. That basis will help determine what you should measure. It will inform the interpretation of your results. It can provide the foundation for you to take those interventions into other contexts.\n\n8.2.1 Watch\nThis video from UNICEF discusses some of the issues in this week’s content, plus foreshadows many of the topics we will be covering over the next few weeks.\nhttps://youtu.be/Wy7qpJeozec\n\n\n8.2.2 Required reading\nAmes and Hiscox (2016) Guide to developing behavioural interventions for randomised controlled trials: Nine guiding questions, Canberra: Department of the Prime Minister and Cabinet, https://behaviouraleconomics.pmc.gov.au/sites/default/files/files/guide-to-developing-behavioural-interventions-for-randomised-controlled-trials.pdf",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Trial basics</span>"
    ]
  },
  {
    "objectID": "trial-basics.html#are-randomised-controlled-trials-ethical",
    "href": "trial-basics.html#are-randomised-controlled-trials-ethical",
    "title": "8  Trial basics",
    "section": "8.3 Are randomised controlled trials ethical?",
    "text": "8.3 Are randomised controlled trials ethical?\n\nRandomized experiments—long the gold standard in medicine—are increasingly used throughout the social sciences and professions to evaluate business products and services, government programs, education and health policies, and global aid. We find robust evidence—across 16 studies of 5,873 participants from three populations spanning nine domains—that people often approve of untested policies or treatments (A or B) being universally implemented but disapprove of randomized experiments (A/B tests) to determine which of those policies or treatments is superior. This effect persists even when there is no reason to prefer A to B and even when recipients are treated unequally and randomly in all conditions (A, B, and A/B). This experimentation aversion may be an important barrier to evidence-based practice.\nMeyer et al (2019)\n\nIs it ethical to withhold an intervention that may benefit someone by assigning them to a control group?\nA common response to this question is that this effectively already occurs in many instances without trials:\n\nInterventions are often piloted, which is equivalent to excluding a group of people who could benefit.\nInterventions are often scaled up, meaning that it takes time for everyone to receive the intervention.\n\nA randomised controlled trial might be considered more ethical than either of those scenarios as it is similarly a phased introduction, but with a mechanism to determine its effectiveness and improve future outcomes. Absent a robust test, we need to be clear about the limits of our knowledge. There are many cases where an intervention assumed to be helpful was later found to be ineffective or harmful.\nFurther, trials often have protocols that in the case of large early effects indicating success or harm, they can be ceased or implemented more rapidly.\nAnother related question is whether it is fair to experiment on people at all? Don’t we risk harm?\nOne response is that every time you roll out a new program, product, communication or other tool that may change behaviour or affect their wellbeing, you are running an experiment. It’s just that if you’re doing it absent a control group or some other mechanism to determine effectiveness, your experiment does not even have the benefit of enabling you to know whether it works, or is helping or harming people.\nDiscussion\nDo you find the above arguments compelling?\n\n8.3.1 References\nMeyer et al (2019) “Objecting to experiments that compare two unobjectionable policies or treatments” Proceedings of the National Academy of Sciences, 116(22), 10723-10728, https://doi.org/10.1073/pnas.1820701116",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Trial basics</span>"
    ]
  },
  {
    "objectID": "trial-basics.html#ethical-principles-for-trial-design",
    "href": "trial-basics.html#ethical-principles-for-trial-design",
    "title": "8  Trial basics",
    "section": "8.4 Ethical principles for trial design",
    "text": "8.4 Ethical principles for trial design\nExperimental research is full of examples of ethical failures. One of the most notorious is the Tuskegee Syphilis Study, in which almost 400 hundred African American men were denied safe and effective treatment for almost 40 years.\nToday there are many frameworks for ethical conduct of trials. One landmark framework comes from the Belmont Report (1979), a US Congress initiated national commission into ethical guidelines for research involving human subjects. It proposed three central principles for the conduct of trials: respect for persons, beneficence, and justice.\nThe Menlo Report (2012) was later developed in the light of increased digitisation of research practices and the difficulty in applying the ideas in the Belmont report in the digital age. It affirmed the three principles from the Menlo report and proposed a fourth: respect for law and public interest.\nThese principles form the basis of many Institutional Review Board ethical approvals in university and government. Each principle is described below:\n\n8.4.1 Respect for Persons\nThe Menlo report described respect for persons as having the following components: - Participation as a research subject is voluntary, and follows from informed consent - Treat individuals as autonomous agents and respect their right to determine their own best interests - Respect individuals who are not targets of research yet are impacted - Individuals with diminished autonomy, who are incapable of deciding for themselves, are entitled to protection.\nPractically, respect for persons has tended to revolve around the principle of informed consent. People should be given information about the experiment in a comprehensible format and then voluntarily agree to participate.\nIt is easy to come up with scenarios where informed consent does not appear appropriate or would undermine the very purpose of the experiment. For example, in a previous unit (23713 Behavioural Economics and Corporate Decision Making) we discussed research by Marianne Bertrand and Sendhil Mullainathan (2004), who sent fictitious CVs in response to ads with randomly assigned African American or White sounding names. White names received 50% more callbacks for interviews. Obtaining informed consent from employers would be impractical and make the experiment pointless.\nThere have been hundreds of discrimination studies of this nature. The lack of consent has been justified for reasons including the limited harm to employers and the social benefit of an accurate measure of discrimination. These justifications have enabled experiments of this nature to pass Institution Review Board processes.\n\n\n8.4.2 Beneficence\nThe Menlo report described beneficience as having the following components: - Do not harm - Maximize probable benefits and minimize probable harms - Systematically assess both risk of harm and benefit.\nOne major potential source of harm is “informational risk”, the risk that information gathered in a trial may be disclosed. This is often dealt with through anonymisation, although there are ample examples of failures or re-identification of data after anonymisation processes. This leads to requirements to develop data protection plans and sharing protocols.\n\n\n8.4.3 Justice\nThe Menlo report described justice as having the following components: - Each person deserves equal consideration in how to be treated, and the benefits of research should be fairly distributed according to individual need, effort, societal contribution, and merit - Selection of subjects should be fair, and burdens should be allocated equitably across impacted subjects.\nEarly conceptions of this concept focused on protection. More focus today, however, is on access, with groups such as women and minority groups needing to be explicity included in trials so that they can benefit from the knowledge gained.\n\n\n8.4.4 Respect for Law and Public Interest\nThe Menlo report described respect for law and public interest as having the following components: - Engage in legal due diligence - Be transparent in methods and results - Be accountable for actions.\nThe Belmont report took respect for law and public interest to be part of beneficience, but the Menlo report argues it deserves explicit consideration. It extends beyond the participants to society and law more generally.\n\n\n8.4.5 An example\n\nFor one week in January 2012, approximately 700,000 Facebook users were placed in an experiment to study “emotional contagion,” the extent to which a person’s emotions are impacted by the emotions of the people with whom they interact. … Participants in the Emotional Contagion experiment were put into four groups: a “negativity-reduced” group, for whom posts with negative words (e.g., sad) were randomly blocked from appearing in the News Feed; a “positivity-reduced” group, for whom posts with positive words (e.g., happy) were randomly blocked; and two control groups, one for the positivity-reduced group and one for the negativity-reduced group. The researchers found that people in the positivity-reduced group used slightly fewer positive words and slightly more negative words relative to the control group. Likewise, they found that people in the negativity-reduced group used slightly more positive words and slightly fewer negative words. Thus, the researchers found evidence of emotional contagion …\nSalganik (2018) Bit by Bit: Social Research in the Digital Age\n\nUsing the principles above, what ethical questions arise in that experiment?\n\n\n8.4.6 Optional reading\nKenneally and Dittrich (2012) The Menlo Report: Ethical Principles Guiding Information and Communication Technology Research, Tech. Report., U.S. Department of Homeland Security, https://www.dhs.gov/sites/default/files/publications/CSD-MenloPrinciplesCORE-20120803_1.pdf",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Trial basics</span>"
    ]
  },
  {
    "objectID": "trial-basics.html#references-1",
    "href": "trial-basics.html#references-1",
    "title": "8  Trial basics",
    "section": "8.5 References",
    "text": "8.5 References\nAllcott and Kessler (2019) “The Welfare Effects of Nudges: A Case Study of Energy Use Social Comparisons”, American Economic Journal: Applied Economics, 11(1), 236-276, https://doi.org/10.1257/app.20170328\nBelmont Report (1979) The Belmont Report: Ethical Principles and Guidelines for the Protection of Human Subjects of Research, US Department of Health, Education, and Welfare, https://videocast.nih.gov/pdf/ohrp_belmont_report.pdf\nBertrand and Mullainathan (2004) “Are Emily and Greg More Employable Than Lakisha and Jamal? A Field Experiment on Labor Market Discrimination”, The American Economic Review, 94(4), 991-1013, https://doi.org/10.1257/0002828042002561\nHaynes et al (2012) Test, Learn, Adapt: Developing Public Policy with Randomised Controlled Trials, Cabinet Office, https://www.gov.uk/government/publications/test-learn-adapt-developing-public-policy-with-randomised-controlled-trials\nManzi et al (2012) Uncontrolled: The Surprising Payoff of Trial-and-Error for Business Politics and Society, Basic Books\nSalganik (2018) Bit by Bit: Social Research in the Digital Age Princeton University Press",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Trial basics</span>"
    ]
  },
  {
    "objectID": "running-a-trial/running-a-trial.html",
    "href": "running-a-trial/running-a-trial.html",
    "title": "Running a trial",
    "section": "",
    "text": "In this part, I will cover the steps to design and run a trial. I will largely going to consider trials for public policy or business applications rather than for academic purposes. Many of the principles hold across academic, business and policy settings (in fact, they are often the same, as a trial may involve both an academic and a business or policy question).\nApplied behavioural science projects are typically designed to change behaviour (or improve an outcome caused by a certain behaviour). The trial is one step of that project.\nBy the end of this module, you will be able to answer the following questions:\n\nWhat are the steps in a randomised trial?\nHow can you articulate the causal links between a program and the outcomes?\nHow do you choose outcomes and indicators to measure the success of a trial?",
    "crumbs": [
      "Running a trial"
    ]
  },
  {
    "objectID": "running-a-trial/level-of-outcome.html",
    "href": "running-a-trial/level-of-outcome.html",
    "title": "9  Level of outcome",
    "section": "",
    "text": "9.1 Measurement\nWhat outcome do you want for you, your customers, your employees or citizens?\nConsider a trial to test whether text messages containing one of several behaviourally-informed messages, such as a social norm and a loss frame, can increase the level of repayment of credit card debt.\nThe level of repayment of the credit card debt is one outcome. But what of:\nWhich of these outcomes are most interested in? This likely would have been asked in the “define” stage of the project that led to the trial, but it is easy to forget broader objectives once a trial is designed. They need to be kept in mind, particularly for the purposes of measurement.\nWhen designing a trial, whether you can measure an outcome is almost as important as what outcome you are interested in. The outcome variable for the experiment needs to be measurable.\nConsider the credit card example above. If you are the financial institution that issued the credit card you can likely measure both the change in repayments and the change in the credit card balance over time.\nIf you offer a suite of financial products to customers, you may be able to monitor changes in balances of other savings and debt products for a subset of the customers. You may also be able to infer use of financial products from other providers by observing transactions. Together these can help you understand the broader distribution of debt balances and net financial position. However, this will be a partial picture and could be biased, particularly if those who hold many products with you have different characteristics than those who hold just a credit card.\nSubjective financial wellbeing, although being a broader measure, may actually be easier to obtain. You could add a survey component to your trial and obtain a direct measurement. The challenge there, however, is whether a minor intervention such as a text message can shift subjective financial wellbeing enough for you to detect it in a trial.\nBeyond outcomes, you might also want to measure process pieces. For example, if looking at debt on credit card, you might collect data on payment patterns. What day and time do they pay? How much each payment? What method of payment? Where did the payment come from? This could enable you to better understand how or why your intervention works and provide further ideas for testing.\nDue to practical requirements, measurement may simply involve analysis of the data you already collect. Many measures of interest are already collected, and if your experiment can capitalise on that, it may improve feasibility and reduce cost. However, this may also create an arbitrary break to the extent of the outcomes that you want to examine.",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Level of outcome</span>"
    ]
  },
  {
    "objectID": "running-a-trial/level-of-outcome.html#measurement",
    "href": "running-a-trial/level-of-outcome.html#measurement",
    "title": "9  Level of outcome",
    "section": "",
    "text": "9.1.1 Example: reducing power consumption\nPower companies often want to limit their customers’ electricity demand. This might be for environmental reasons or to reduce peak demand.\nOne method to achieve this is to give that person or household a comparison of their power consumption with that of their neighbours. People have a desire to conform, and look to cues to inform their decisions. If shown that their power usage is above their neighbours, they tend to reduce their use.\n\nThat is one possible outcome: reduced electricity usage. A related direct outcome is the financial saving through reduced power usage. Do they pay a smaller power bill?\nBut these are narrow outcomes. Broader questions could be asked.\nWhat was the net change in energy usage by the household? Was there substitution into gas or other energy sources? What is the emissions profile of these changes? What is the cost?\nMore broadly, what was the total change in household expenditure due to the intervention? Did they pay for energy savings appliances or fitouts to their house? What did they spend any energy bill savings on? How much time did they expend changing their energy usage?\nEven those questions might be seen as narrow. If the individual or household’s objectives were purely financial, there may be a success. They have saved on their power bill. Their reduction in use also aligns with the environmental or peak demand reduction objectives of the electricity provider. But what if their objective is satisfaction in life? Or comfort? Did they freeze during winter and swelter during summer to maintain their self-image? You have just compared them negatively with their neighbour. Did their happiness change when they saw that they compared poorly? Did it increase mental stress?\nMeasuring many of those outcomes are difficult. But that does not mean that they don’t matter.\nHunt Allcott and Judd Kessler sought a broader measure of the benefits of the comparisons by asking customers their “willingness to pay” for the home energy reports that contained comparisons of their energy use. This willingness to pay measure supported the argument that there was a net welfare gain from the the comparison. However, because the willingness to pay measures capture the broader costs and benefits of the energy reports beyond simple changes in energy usage, they found that the welfare gains were much smaller than assessed using narrow measures.\nPerhaps most interestingly, one third of the recipients would be willing to pay to not receive the report. For those customers, the psychological or other costs outweighed any information benefit.",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Level of outcome</span>"
    ]
  },
  {
    "objectID": "running-a-trial/alternative-trial-design-processes.html",
    "href": "running-a-trial/alternative-trial-design-processes.html",
    "title": "10  Alternative trial design processes",
    "section": "",
    "text": "10.1 Test, Lean, Adapt\nBeyond the set of steps for randomised trials that we have just explored, there are many other conceptions of the steps in a randomised controlled trial.\nOne of the best known in applied behavioural science is described by Haynes et al. (2012) in Test, Learn, Adapt: Developing Public Policy with Randomised Controlled Trials. This paper is written in relation to public policy problems, but the process can be applied equally to business problems. They describe nine steps:",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Alternative trial design processes</span>"
    ]
  },
  {
    "objectID": "running-a-trial/alternative-trial-design-processes.html#test-lean-adapt",
    "href": "running-a-trial/alternative-trial-design-processes.html#test-lean-adapt",
    "title": "10  Alternative trial design processes",
    "section": "",
    "text": "10.1.1 Test\nStep 1: Identify two or more interventions to compare. The purpose of a trial is to differentiate between different interventions when we do not know which is most effective.\nStep 2: Define the outcome that the intervention is intended to influence. We will discuss this on the next page.\nStep 3: Decide on the randomisation unit.\nStep 4: Determine how many units are required for robust results.\nStep 5: Assign each unit to one of the interventions using a robustly random method. By “robust”, we need to protect against bias creeping into the trial. We cannot allow those with vested interests to lean on which units are allocated to which treatments, or which units might be excluded altogether outside of those identified to be excluded as part of the original plan.\nStep 6: Introduce the interventions to the assigned groups. This may sound easy, but monitoring is required to ensure that the interventions are being introduced in the way intended to the right units. There also needs to be a consistency of treatment between the trial and what would be done at scale. For example, is a test of gold-plated interventions delivered by enthusiastic, recently trained practitioners going to give a fair measure of the effectiveness of an intervention delivered month-after-month by those not involved in the experiment? If specific effort is applied during the trial but cannot be applied when scaled, the trial is likely to be a poor guide as to effectiveness when scaled.\n\n\n10.1.2 Learn\nStep 7: Measure the results and determine the impact of the interventions. Your trial may show that none of the interventions are successful, but that’s a success. You have learnt something. The method of measurement should have been decided before randomisation and captured in the pre-analysis plan. We will cover pre-analysis plans in week 4.\n\n\n10.1.3 Adapt\nStep 8: Adapt your policy intervention to reflect your findings.\nStep 9: Return to step 1. This process worth is iterative. The trial gives you information that you may be able to implement at scale to improve outcomes, but it does not provide the perfect solution. You can continue to iterate toward better outcomes. Further, for many interventions, what is most effective may also change over time.",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Alternative trial design processes</span>"
    ]
  },
  {
    "objectID": "running-a-trial/alternative-trial-design-processes.html#other-processes",
    "href": "running-a-trial/alternative-trial-design-processes.html#other-processes",
    "title": "10  Alternative trial design processes",
    "section": "10.2 Other processes",
    "text": "10.2 Other processes\nA selection of processes in around the development of a trial include:\nideas42\nideas42’s behavioural design process\nThey describe a five-stage process:\n\nDefine the problem and try to remove any embedded assumptions about why it may be occurring.\nDiagnose what behavioral bottlenecks may be driving the problem.\nDesign interventions that directly address the key bottlenecks we’ve diagnosed.\nTest the intervention to see whether the design successfully addresses the problem, typically using a randomised controlled trial.\nScale the solution to a larger population or adapt it to other contexts if an intervention proves effective.\n\nBETA 4D\nBETA’s 4D framework, a four stage process:\n\nDiscover: Define the policy, program or service delivery issue and develop a behavioural problem statement.\nDiagnose: Understand the current behaviour and its drivers and develop a clearly defined hypothesis of behaviour change.\nDesign: Design an intervention to address the behavioural problem. Design an evaluation to test the intervention.\nDeliver: Implement the intervention and evaluation and share the results.\n\nBETA has also published a guidance note on Developing behavioural interventions for randomised controlled trials: Nine guiding questions, which provides more detail on undertaking the first two stages of the 4D framework.\n\n\n\n\nHaynes, L., Service, O., Goldacre, B., and Torgerson, D. (2012). Test, Learn, Adapt: Developing Public Policy with Randomised Controlled Trials. https://www.gov.uk/government/publications/test-learn-adapt-developing-public-policy-with-randomised-controlled-trials",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Alternative trial design processes</span>"
    ]
  },
  {
    "objectID": "4-error-control-effect-sizes-and-sample-arrangement.html",
    "href": "4-error-control-effect-sizes-and-sample-arrangement.html",
    "title": "11  Error control, effect sizes and sample arrangement",
    "section": "",
    "text": "11.1 Type 1 and Type 2 errors\nIn this chapter, I explore some particular elements of hypothesis testing. I will discuss how to reduce error, interpret effect size and arrange the random sample. I will close the chapter with a discussion of how pre-registration and pre-analysis plans can improve our practices.\nStatistical tests are often thought of in terms of the errors they can generate.\nThe first error is where the test rejects a null hypothesis that is true. You find an effect where none exists. This is known as a Type I error, or false positive.\nWe set the rate at which Type I errors occur. The significance level \\alpha is the rate of Type I errors. If we use a significance level of 0.05, we have a 5% probability of rejecting the null hypothesis when it is true, generating a Type I error.\nThe second error is when we fail to reject a false null hypothesis. We do not find an effect where one exists. This is known as a Type II error, or false negative.\nThe Type II error rate is unknown, but can be calculated if we make a number of assumptions. We will examine this in the following pages. The type II error rate is denoted by \\beta.\nThe relationship between these errors and correct inference is shown in the following table.\nThe following diagrams provide another view on these errors.\nAs per our running example, suppose we are estimating two sample means for how many people submit their tax return on time. We want to know whether the difference between them represents a true effect of the intervention. Let us suppose that the null hypothesis is true and there is no effect.\nAs we have discussed, the estimate of the effect is with error. Our estimate may vary from the true value. That estimate will fall within a probability distribution of mean \\mu and standard deviation \\frac{\\sigma}{\\sqrt{n}}. The curve below represents that probability distribution.\nWhen we set \\alpha=0.05, we are setting a critical value such that there is a 0.05 chance that the estimate will be above the critical value, despite the null hypothesis being true. The red shaded area is the probability of type I error.\n[For this version of the diagram, only show the bell curve on the left. Remove the curve on the right and the shaded green area. Change “Any mean” to “Critical value”.]\nLet us now assume there is an effect of our intervention. The alternative hypothesis is true.\nOur estimate of this effect will again be with error, falling within a probability distribution of mean \\mu and standard deviation \\frac{\\sigma}{\\sqrt{n}}, but this time with \\mu representing a positive effect. This is represented by the curve on the right.\nAs you can see in the diagram, there is a probability that even if the effect is true, the measured value of the effect will fall below the critical value. You will fail to reject the null even though the alternative hypothesis is true. The green shaded area represents this probability of a Type II error.\n[Tweaks to make to diagram: change “Any mean” to “Critical value”; delete “Null” and “Theoretical non-null value” with \\bar{x}_1]\nVersions of diagram: 1. As in week 3.4 - normal distribution 2. As in week 3.5 - critical value 3. As here - version just null hypothesis curve 4. As here - version both curves 5. (for 4.3) Shifting H1 curve to the left or right - illustrate more/less chance of type II error if small/large effect size 6. (for 4.3) Larger sample size - reduce chance of type II error - bell curves getting taller and narrower, so green area shrinks 7. Trade-off between type 1 and 2 errors - changing significance level - slider??",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Error control, effect sizes and sample arrangement</span>"
    ]
  },
  {
    "objectID": "4-error-control-effect-sizes-and-sample-arrangement.html#type-1-and-type-2-errors",
    "href": "4-error-control-effect-sizes-and-sample-arrangement.html#type-1-and-type-2-errors",
    "title": "11  Error control, effect sizes and sample arrangement",
    "section": "",
    "text": "Null hypothesis (H_0) is true\nNull hypothesis (H_1) is false\n\n\n\n\nDon’t reject H_0\nTrue negative. Probability = 1-\\alpha\nType II error (false negative). Probability = \\beta\n\n\nReject H_0\nType I error (false positive). Probability = \\alpha\nTrue positive. Probability = 1-\\beta",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Error control, effect sizes and sample arrangement</span>"
    ]
  },
  {
    "objectID": "4-error-control-effect-sizes-and-sample-arrangement.html#type-i-error-control",
    "href": "4-error-control-effect-sizes-and-sample-arrangement.html#type-i-error-control",
    "title": "11  Error control, effect sizes and sample arrangement",
    "section": "11.2 Type I error control",
    "text": "11.2 Type I error control\nWe set the rate of Type 1 errors through the significance level. A standard significance level of \\alpha=0.05 gives a 5% false positive rate if the null hypothesis is true.\nThere are many recent arguments that \\alpha should be smaller than 0.05, such as Benjamin et al’s (2018) argument that \\alpha should be set at 0.005. Many of these relate to the replication crisis that we will cover this later in this course. We simply want to generate less false positives.\n\n11.2.1 Multiple comparisons\nOne scenario where there is a longer history of using a smaller alpha is where you are testing multiple hypotheses. This could arise because you have multiple treatment groups or because you are measuring many potential outcomes.\nIf you are testing many hypotheses, it becomes increasingly likely that at least one of them will meet the statistical threshold merely by chance. If you conduct 20 tests and all of null hypothesis for each test is true, you expect one false positive.\nhttps://imgs.xkcd.com/comics/significant.png\nA common correction applied to \\alpha in instances of multiple comparisons is the Bonferroni correction. If you are testing m hypotheses, set the significance level for each hypothesis at \\frac{\\alpha}{m}. The Bonferroni correction is conservative in that it decreases the family-wise error rate - which is the probability of making one or more false discoveries - to below 0.05.\nA Bonferroni type correction is typically applied where there is large-scale multiple testing. In genomic association studies, where they are testing of the order of a million genetic variants, they will normally set the significance level at 5x10^{-8}\nA smaller significance level and associated higher critical value means, however, that we will get a higher rate of Type II errors. We will discuss this in the following pages.\n\n\n11.2.2 Replication data sets\nAn alternative method of Type I error control is use of a replication sample, which is a subset of the data that is excluded from the initial analysis. If there are any significant results in the first analysis, testing for those hypotheses that were significant in the first is conducted on the replication sample.\nThis approach is common in machine learning applications, but is starting to become more common in statistical analysis. However, it is rarely used in experimental analysis.\nWe will cover the concept of replication more broadly in week 6.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Error control, effect sizes and sample arrangement</span>"
    ]
  },
  {
    "objectID": "4-error-control-effect-sizes-and-sample-arrangement.html#type-ii-error-control",
    "href": "4-error-control-effect-sizes-and-sample-arrangement.html#type-ii-error-control",
    "title": "11  Error control, effect sizes and sample arrangement",
    "section": "11.3 Type II error control",
    "text": "11.3 Type II error control\nThe probability of a Type II error is \\beta, the probability that you will not reject the null hypothesis when it is false.\nA related concept is power, which is the probability that you will reject the null hypothesis when it is false. Power is equal to 1 - \\beta.\nQuestions related to the Type II error rate for a trial are typically framed in terms of power. How can I design my trial so that it has enough power to reject the null hypothesis in circumstances where it is not true? Alternatively, how can I increase the probability that my trial will achieve statistical significance when the null hypothesis is not true?\n\n11.3.1 Calculating power\nStatistical power can be calculated with the following variables:\nThe significance level: The stricter the significance level that you use in your experiment, the lower the power of the experiment. There is a trade-off between Type I and Type II errors. As you decrease the probability of false positives by using a stricter significance level, you also decrease the probability of true positives. You should not relax the significance level to achieve power, but should be aware of the consequences of making it more strict.\nThe effect size: What is the magnitude of the effect of your intervention? The larger the effect size, the more power the experiment has to detect an effect and generate a statistically significant outcome.\nUnfortunately, as you have not yet run the experiment, you do not know what the effect size is. If there are other studies with which parallels can be drawn, you can make an educated guess as to its likely size. You might generate your estimate through a literature review, pilot studies or other related experiments.\nEstimates of the effect size should be conservative. As we will see later in this unit, the experimental literature is full of over-exaggerated effect sizes. An overestimate of the effect size will overestimate the power of the experiment.\nYou also need to understand the variability of the effect size (such as its standard deviation in the population of interest) to calculate power. A highly-variable effect will have a higher standard error in its measurement, so more probability of getting an extreme result that generates an error.\nAs a result, when an effect size estimate is made, it it typically made as a standardised effect size. This is the magnitude of the effect divided by the standard deviation. How many standard deviations is the effect?\nYou don’t have the ability to increase the effect size. However, variation in the effect size is affected by things such as measurement error. More precise measurement can increase power.\nThe sample size: Power is a function of the sample size of the experiment. It is the factor over which you have the most control.\nIncreasing the sample size reduces the standard error of your estimated effect size. This means that, if the alternative hypothesis is true, you will have a greater probability of detecting an effect of any given size. This can be seen in the following diagram: a larger sample size results in lower standard error, which is reflected in a narrower probability distribution for your estimate.\n[ADD GRAPHIC OF NORMAL DISTRIBUTION GETTING NARROWER]\nBeyond increasing power, larger samples have other benefits such as reducing the extent to which we overestimate the effect size. We will discuss this more in coming material.\nPower is calculated by asking, given the critical value implied by the chosen significance level, what effect size is required such that 80% of the probability distribution for the estimated effect size will be above the critical value.\n\n\n11.3.2 Pre- and post-experiment power analysis\nPower calculations should normally be done before an experiment. It is used to determine what sample size is required to obtain the requisite power. A common practice is obtaining a sample size sufficient to achieve 80% power, although there is little reason to limit yourself to that level of power if you can increase power at modest cost.\nPower calculations are also often done after an experiment. You will see in the literature that post-experiment power calculations are regularly used to justify that the experiment had sufficient power, with the effect size found in the experiment the basis for the power calculation. This is poor practice, as a significant result in an underpowered experiment will tend to exaggerate the effect size. If you then use this exaggerated effect to calculate power, it gives the impression that the experiment was adequately powered. We will discuss this exaggeration of effect size later in the unit.\nIf a power calculation is done after the experiment, it should only be done using a well-grounded assumed effect size, not the effect size observed in the experiment.\nAn alternative use of post-experiment power calculations is to examine whether published experiments should be showing the proportion of significant results that they do. If a set of experiments has average power of 80%, only 80% of them should find a statistically significant effect. A higher proportion suggests publication bias (a topic later in the unit). The reading below examines a chapter of Daniel Kahneman’s Thinking, Fast and Slow using this tool.\n\n\n11.3.3 Optional reading\nSchimmack (2017) “Reconstruction of a Train Wreck: How Priming Research Went Off the Rails”, Replicability-Index, https://replicationindex.com/2017/02/02/reconstruction-of-a-train-wreck-how-priming-research-went-of-the-rails/ (Note Daniel Kahneman’s response in the comments.)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Error control, effect sizes and sample arrangement</span>"
    ]
  },
  {
    "objectID": "4-error-control-effect-sizes-and-sample-arrangement.html#effect-sizes",
    "href": "4-error-control-effect-sizes-and-sample-arrangement.html#effect-sizes",
    "title": "11  Error control, effect sizes and sample arrangement",
    "section": "11.4 Effect sizes",
    "text": "11.4 Effect sizes\nStatistical significance is not the same as practical importance. A statistically significant result may not be large enough to matter in practice. You are interested not just in whether a treatment affects people, but also how much.\nFor example, which is the more interesting result? A statistically significant experimental treatment that could boost the financial wellbeing of all Australian by $2 (p=0.04). Or a non-significant experimental treatment that could boost the financial wellbeing of all Australians by $1000 (p=0.06)?\nEffect sizes from experiments should be interpreted and reported with caution. While often reported as a point estimate, we can provide a confidence interval around the estimated effect size. The confidence interval for an effect size with a p-value marginally below 0.05 will have a confidence interval with a lower end only marginally above 0. As we will discuss later in this unit, effect sizes are often exaggerations of the true effect size.\n\n11.4.1 Cohen’s d\nOne common way in which effect size’s are talked about is “Cohen’s d”. Cohen’s d is defined as the difference of two means divided by the standard deviation of the data. It is calculated as:\nd=\\frac{\\bar{x}_1-\\bar{x}_0}{s}=\\frac{\\mu_1-\\mu_0}{s}\nwhere s is the pooled standard deviation of the data (you don’t have to know how to calculate that.)\nCohen’s d has the benefit of translating effect sizes in different experiments onto a common scale. You can speak of how many standard deviations an effect size is.\nOne legacy of Cohen’s that you will often encounter is that he also labelled different sizes of Cohen’s d. A Cohen’s d of 0.2 is a small effect, 0.5 is medium, and 0.8 is a large. When people are calculating power for an experiment, they will often think in terms of whether the effect is small, medium or large, and use the associated number. One way this can mislead, however, is that many effect sizes in the social sciences are far less than 0.2.\n\n\n11.4.2 Summarising effect sizes\nOne place you often see transparent communication of effect sizes are in meta-analyses (summaries of the literature) or multi-lab replications. (More on these later in the unit.) Below is one example. (Note that the effect sizes are standardised as Cohen’s d.)\n\n11.4.2.1 The jam experiment\nOn two Saturdays in a California supermarket, Mark Lepper and Sheena Iyengar set up tasting displays of either six or 24 jars of jam. Consumers could taste as many jams as they wished, and if they approached the tasting table they received a $1 discount coupon to buy the jam.\nFor attracting initial interest, the large display of 24 jams did a better job, with 60 per cent of people who passed the display stopping. Forty per cent of people stopped at the six jam display. But only three per cent of those who stopped at the 24 jam display purchased any of the jam, compared with almost 30 per cent who stopped at the six jam display.\nThis result has become the classical example of the “paradox of choice”. More choice can lead us to fail to make a choice.\nLater, Benjamin Scheibehenne and friends surveyed the literature on the choice overload hypothesis. This chart is a a plot of the effect sizes across the literature. Of those that are significant - that is, those for which the 95% confidence interval does not contain zero - they have a large point estimate of effect size, yet a 95% confidence interval barely excluding zero.\nLooking across these experiments, in some cases, choice increases purchases. In others it reduces them. Scheibehenne and friends determined that the mean effect size of changing the number of choices across the studies was effectively zero.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Error control, effect sizes and sample arrangement</span>"
    ]
  },
  {
    "objectID": "4-error-control-effect-sizes-and-sample-arrangement.html#randomisation-techniques",
    "href": "4-error-control-effect-sizes-and-sample-arrangement.html#randomisation-techniques",
    "title": "11  Error control, effect sizes and sample arrangement",
    "section": "11.5 Randomisation techniques",
    "text": "11.5 Randomisation techniques\nAs we discussed in week 2, randomisation provides indirect control of uncontrolled variables. It provides us with a way to infer that differences in outcomes are due to the treatments and not due to the individual characteristics of the experimental participants.\nHowever, randomisation is not a panacea, nor is it always practical to undertake a pure randomisation. The below discusses some complications that can be involved in randomisation.\n\n11.5.1 Blocking\nRandomisation can occasionally lead to a large correlation between treatments and uncontrolled nuisance variables within a trial. For example, your control and intervention groups, by chance, may end up having people with higher incomes in one group than the other, or an unbalanced mix of sexes. If you are running only a small number of trials (often only one), this lack of balance can bias your results.\nThe following excerpt gives an example where two groups in a field trial became unbalanced.\n\n[I]n 2012, we came up with a seemingly costless simple intervention: Get people to sign a tax or insurance audit form before they reported critical information (versus after, the common business practice).\nWe ran studies showing that when people signed an honesty declaration before reporting information, they thought about how they were honest people, and were less likely to misreport compared to when they signed after they had filled out the form. While our original set of studies found that this intervention worked in the lab and in one field experiment, we no longer believe that signing before versus after is a simple costless fix. …\nIn an attempt to replicate and extend our original findings, three people on our team (Kristal, Whillans and Bazerman) found no evidence for the observed effects across five studies with 4,559 participants. We brought the original team together and reran an identical lab experiment from the original paper (Experiment 1). The only thing we changed was the sample size: we had 20 times more participants per condition. And we found no difference in the amount of cheating between signing at the top of the form and signing at the bottom.\nIn light of these findings, we reanalyzed the field study in the original paper and became concerned with a failure of random assignment (such that the number of miles driven before the intervention was delivered was significantly different between the two groups). What we originally thought to be a reporting difference (between customers who signed at the top versus bottom of the form) now seems more likely to be a difference in actual driving behavior—not the honest or dishonest reporting of it.\nKristal et al (2020)\n\nThere are experimental designs that can reduce the effect of unbalanced groups. These work by holding a set of variables constant within a subset of trials (“a block”). These variables are often called blocking variables.\nFor example, suppose we are going to test discrimination in hiring by sending CVs in response to job ads. We believe that large and small firms will respond differently. We can split the firms into two blocks, small and large, and then randomise within each of those blocks. This will balance the small and large firms across the control and intervention groups and ensure we don’t get an unbalanced experiment on that dimension.\n\n\n11.5.2 Within-subject designs\nMost of the experiments we have discussed involve what is called a “between-subject” design. The treatment and control groups comprise different subjects, with comparisons made between those subjects.\nAn alternative design is within-subject design, whereby experimental subjects make decisions in all treatments.\nSuppose we are working on increasing on-time credit card payments by sending a reminder. We might run the trial over two periods, sending a reminder to half the participants for the first payment period, then a reminder to the other half in the second period. The control and treatment groups across the two periods are balanced as they contain the same people. This within-subjects design is called a crossover study, as participants cross over from one group to the other.\nUnder a within-subject design, each subject is effectively their own control, meaning that we do not need to worry about the different characteristics of decision makers. Apart from avoiding unbalanced treatment and control groups, this means that there is usually less variation in treatment effects, increasing the power of the experiment.\nA major disadvantage of a within-subject design is that there may be “order effects”. The intervention in one period may flow into another period. There may be effects such as fatigue. The reverse order that participants receive the treatment in a “crossover” study, such as the example above, can be used to attempt to account for these order effects, although it complicates the analysis.\nWithin-subject designs tend to be used where we have a limited number of experimental participants or are looking for efficiencies in the conduct of the experiment, as the design can increase power with fewer participants relative to a between-subjects design. You might also use it where you are interested in the longitudinal aspect of the interventions.\n\n\n11.5.3 Cluster randomisation\nSometimes it is not practicable to randomise experimental subjects individually. For example, suppose you are implementing a trial to improve on-time tax return submissions by phoning taxpayers. You want to test what scripts and tools for call centre staff are most effective in increasing on-time submission. Training in the tools are provided at staff briefings at the beginning of each shift, making it impracticable to randomise across call centre staff within the centre.\nA common approach to deal with this problem is cluster randomisation. In cluster randomisation, groups of subjects are randomised. If you have 20 call centres, randomise those 20 into the treatment and control.\nCluster randomisation is also used to control for contamination across individuals, as a change in behaviour in one might change the behaviour of others. In our example above, even if it were possible to train staff separately with different treatments, they may become aware of other approaches of staff in their centre and change their behaviour as a result. If that is a concern, cluster randomisation might address this.\nCluster randomisation has costs. It introduces greater complexity into the analysis, including introducing potential intracluster correlation that should be accounted for. Cluster randomisation also reduces power by effectively reducing the sample size.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Error control, effect sizes and sample arrangement</span>"
    ]
  },
  {
    "objectID": "4-error-control-effect-sizes-and-sample-arrangement.html#module-2-summary",
    "href": "4-error-control-effect-sizes-and-sample-arrangement.html#module-2-summary",
    "title": "11  Error control, effect sizes and sample arrangement",
    "section": "12.1 Module 2 summary",
    "text": "12.1 Module 2 summary\nIn this module, we have examined the process of conducting a randomised controlled trial, how to test hypotheses, and the types of errors that can be generated.\nAs a warm up for the next module, watch this interview of Professor Dorothy Bishop by Sabine Hossenfelder.\nhttps://youtu.be/v778svukrtU",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Error control, effect sizes and sample arrangement</span>"
    ]
  },
  {
    "objectID": "4-error-control-effect-sizes-and-sample-arrangement.html#references",
    "href": "4-error-control-effect-sizes-and-sample-arrangement.html#references",
    "title": "11  Error control, effect sizes and sample arrangement",
    "section": "12.2 References",
    "text": "12.2 References\nBenjamin et al (2018) “Redefine statistical significance”, Nature Human Behaviour, 2, 6-10, https://doi.org/10.1038/s41562-017-0189-z\nIyengar and Lepper (2000) “When choice is demotivating: Can one desire too much of a good thing?”, Journal of Personality and Social Psychology, 79(6), 995–1006, https://doi.org/10.1037/0022-3514.79.6.995\nKristal et al (2020) “When We’re Wrong, It’s Our Responsibility as Scientists to Say So”, Scientific American, https://blogs.scientificamerican.com/observations/when-were-wrong-its-our-responsibility-as-scientists-to-say-so/\nList, Sadoff and Magner (2010) “So you want to run an experiment, now what?Some simple rules of thumb for optimal experimental design”, Experimental Economics, https://doi.org/10.1007/s10683-011-9275-7\nScheibehenne, Greifeneder and Todd (2010) “Can There Ever Be Too Many Options? A Meta-Analytic Review of Choice Overload”, Journal of Consumer Research, 37(3), 409–425, https://doi.org/10.1086/651235",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Error control, effect sizes and sample arrangement</span>"
    ]
  },
  {
    "objectID": "interpreting-trial-results/interpreting-trial-results.html",
    "href": "interpreting-trial-results/interpreting-trial-results.html",
    "title": "Interpreting trial results",
    "section": "",
    "text": "In this part, I will examine the challenges of interpreting and scaling trial results. How do we read and interpret published trial results? Can we trust them? What happens when we scale our or other’s interventions, move from lab to field, or change the context?\nBy the end of this part, you will be able to answer the following questions:\n\nWhat is the replication crisis and its causes?\nHow generalisable are trials to new contexts?",
    "crumbs": [
      "Interpreting trial results"
    ]
  },
  {
    "objectID": "interpreting-trial-results/navigating-the-literature.html",
    "href": "interpreting-trial-results/navigating-the-literature.html",
    "title": "12  Navigating the literature",
    "section": "",
    "text": "12.1 Reading research\nIt may be useful in your journey of running a trial to conduct a literature review of existing articles. Literature refers here to a collection of published materials on a particular area of research or topic, such as books and journal articles. A literature review seeks to summarize the information that existing studies have gathered on these issues. It will help you answer the following questions:\nOnce you have selected the studies, it is important to be able to read or skim-read them in an educated way.\nAs part of your review of the literature, it is important to be able to understand the research articles that describe the motivation and results of trials.\nThey will often be organised as follows:\nThe following video will help you understand how to read the tables.",
    "crumbs": [
      "Interpreting trial results",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Navigating the literature</span>"
    ]
  },
  {
    "objectID": "interpreting-trial-results/navigating-the-literature.html#reading-research",
    "href": "interpreting-trial-results/navigating-the-literature.html#reading-research",
    "title": "12  Navigating the literature",
    "section": "",
    "text": "Introduction ​\nTheory (not always in empirical papers; this section may be hard to read and you are probably ok to skip if you understood the introduction)​\nData (this will typically include a table of descriptive statistics, see video below on how to read such a table)​\nIntervention (sometimes, the paper will describe the intervention in details)​\nResults (this will typically include regression outputs, see video below on how to read such a table)​\nConclusion",
    "crumbs": [
      "Interpreting trial results",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Navigating the literature</span>"
    ]
  },
  {
    "objectID": "interpreting-trial-results/the-replication-crisis.html",
    "href": "interpreting-trial-results/the-replication-crisis.html",
    "title": "13  The replication crisis",
    "section": "",
    "text": "13.1 Example 1: Disfluency\nThere is a large experimental literature in behavioural science. It is the product of over 50-years of experimentation, initially conducted in the lab, but increasingly today in the field.\nHowever, over the last decade, there have been increased questions about the reliability of the published results of randomised controlled trials. Replication is the ability to re-perform the experiment and collect new data. An experiment is said to replicate if it returns a similar result.\nMany results have failed to replicate, and they have failed at a rate that suggests this is a systemic issue.\nBelow are two examples of replication failures, together with large scale replication projects conducted in psychology and economics.\nAdam Alter and friends (Alter et al., 2007) exposed 40 students to two versions of the cognitive reflection task. One question in the cognitive reflection task is the following:\nThe two versions differed in that one used small light grey font that made the questions hard to read. Those exposed to the harder to read questions achieved higher scores. Slowing people down made them do better.\nEight years later Frederick Meyer and friends (Meyer et al., 2015) conducted a replication across many labs involving thousands of people. Whereas the original study found a large effect, the replication found nothing. You can see the original small sample outlier on the bottom left of the chart.",
    "crumbs": [
      "Interpreting trial results",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>The replication crisis</span>"
    ]
  },
  {
    "objectID": "interpreting-trial-results/the-replication-crisis.html#example-1-disfluency",
    "href": "interpreting-trial-results/the-replication-crisis.html#example-1-disfluency",
    "title": "13  The replication crisis",
    "section": "",
    "text": "A bat and a ball cost $1.10 in total. The bat costs $1.00 more than the ball. How much does the ball cost?",
    "crumbs": [
      "Interpreting trial results",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>The replication crisis</span>"
    ]
  },
  {
    "objectID": "interpreting-trial-results/the-replication-crisis.html#example-2-honest-priming",
    "href": "interpreting-trial-results/the-replication-crisis.html#example-2-honest-priming",
    "title": "13  The replication crisis",
    "section": "13.2 Example 2: Honest priming",
    "text": "13.2 Example 2: Honest priming\nVerschuere and friends (Verschuere et al., 2018) replicated a highly cited experiment by Nina Mazar, On Amir and Dan Ariely (Mazar et al., 2008) across multiple labs. The abstract of the paper reads as follows:\n\nThe self-concept maintenance theory holds that many people will cheat in order to maximize self-profit, but only to the extent that they can do so while maintaining a positive self-concept. Mazar, Amir, and Ariely (2008, Experiment 1) gave participants an opportunity and incentive to cheat on a problem-solving task. Prior to that task, participants either recalled the Ten Commandments (a moral reminder) or recalled 10 books they had read in high school (a neutral task). Results were consistent with the self-concept maintenance theory. When given the opportunity to cheat, participants given the moral-reminder priming task reported solving 1.45 fewer matrices than did those given a neutral prime (Cohen’s d = 0.48); moral reminders reduced cheating. Mazar et al.’s article is among the most cited in deception research, but their Experiment 1 has not been replicated directly. This Registered Replication Report describes the aggregated result of 25 direct replications (total N = 5,786), all of which followed the same preregistered protocol. In the primary meta-analysis (19 replications, total n = 4,674), participants who were given an opportunity to cheat reported solving 0.11 more matrices if they were given a moral reminder than if they were given a neutral reminder (95% confidence interval = [−0.09, 0.31]). This small effect was numerically in the opposite direction of the effect observed in the original study (Cohen’s d = −0.04).\n\nFigure 2 from the paper demonstrates the result:",
    "crumbs": [
      "Interpreting trial results",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>The replication crisis</span>"
    ]
  },
  {
    "objectID": "interpreting-trial-results/the-replication-crisis.html#estimating-the-reproducibility-of-psychological-science",
    "href": "interpreting-trial-results/the-replication-crisis.html#estimating-the-reproducibility-of-psychological-science",
    "title": "13  The replication crisis",
    "section": "13.3 Estimating the reproducibility of psychological science",
    "text": "13.3 Estimating the reproducibility of psychological science\nThese examples are illustrative of a wider problem. The Open Science Collaboration (Open Science Collaboration, 2015) ran replications of 100 experiments previously reported in three top psychology journals. The abstract captures the disappointing result:\n\nReproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47% of original effect sizes were in the 95% confidence interval of the replication effect size; 39% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.\n\nThe results are summarised in this diagram. If the original studies were representative of the results we should expect, the balls should be clustered around the diagonal line. Instead, most are below it, representing the decline in the effect size, with many not significantly different from zero.",
    "crumbs": [
      "Interpreting trial results",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>The replication crisis</span>"
    ]
  },
  {
    "objectID": "interpreting-trial-results/the-replication-crisis.html#replication-in-economics",
    "href": "interpreting-trial-results/the-replication-crisis.html#replication-in-economics",
    "title": "13  The replication crisis",
    "section": "13.4 Replication in economics",
    "text": "13.4 Replication in economics\nThe replication problem is not just in psychology, although economics has not had such high profile failures to date.\nColin Camerer and friends (Camerer et al., 2016) replicated 18 studies from two top economics journals. They found:\n\nThe replicability of some scientific findings has recently been called into question. To contribute data about replicability in economics, we replicated 18 studies published in the American Economic Review and the Quarterly Journal of Economics between 2011 and 2014. All of these replications followed predefined analysis plans that were made publicly available beforehand, and they all have a statistical power of at least 90% to detect the original effect size at the 5% significance level. We found a significant effect in the same direction as in the original study for 11 replications (61%); on average, the replicated effect size is 66% of the original. The replicability rate varies between 67% and 78% for four additional replicability indicators, including a prediction market measure of peer beliefs.\n\nAn interesting aspect to these replications was that a prediction market and survey were run in advance to enable academics to predict whether each of the studies would replicate. As can be seen in the diagram, those studies that replicated were predicted to be more likely to replicate than those that did not, although both the survey and prediction market were slightly optimistic in the probability of replication.\n\n\n\n\n\nAlter, A. L., Oppenheimer, D. M., Epley, N., and Eyre, R. N. (2007). Overcoming intuition: Metacognitive difficulty activates analytic reasoning. Journal of Experimental Psychology: General, 136(4), 569–576. https://doi.org/10.1037/0096-3445.136.4.569\n\n\nCamerer, C. F., Dreber, A., Forsell, E., Ho, T.-H., Huber, J., Johannesson, M., Kirchler, M., Almenberg, J., Altmejd, A., Chan, T., Heikensten, E., Holzmeister, F., Imai, T., Isaksson, S., Nave, G., Pfeiffer, T., Razen, M., and Wu, H. (2016). Evaluating replicability of laboratory experiments in economics. Science, 351(6280), 1433–1436. https://doi.org/10.1126/science.aaf0918\n\n\nMazar, N., Amir, O., and Ariely, D. (2008). The Dishonesty of Honest People: A Theory of Self-Concept Maintenance. Journal of Marketing Research, 45(6), 633–644. https://doi.org/10.1509/jmkr.45.6.633\n\n\nMeyer, A., Frederick, S., Burnham, T. C., Guevara Pinto, J. D., Boyer, T. W., Ball, L. J., Pennycook, G., Ackerman, R., Thompson, V. A., and Schuldt, J. P. (2015). Disfluent fonts don’t help people solve math problems. Journal of Experimental Psychology: General, 144(2), e16–e30. https://doi.org/10.1037/xge0000049\n\n\nOpen Science Collaboration. (2015). Estimating the reproducibility of psychological science. Science, 349(6251), aac4716. https://doi.org/10.1126/science.aac4716\n\n\nVerschuere, B., Meijer, E. H., Jim, A., Hoogesteyn, K., Orthey, R., McCarthy, R. J., Skowronski, J. J., Acar, O. A., Aczel, B., Bakos, B. E., Barbosa, F., Baskin, E., Bègue, L., Ben-Shakhar, G., Birt, A. R., Blatz, L., Charman, S. D., Claesen, A., Clay, S. L., … Yıldız, E. (2018). Registered Replication Report on Mazar, Amir, and Ariely (2008). Advances in Methods and Practices in Psychological Science, 1(3), 299–317. https://doi.org/10.1177/2515245918781032",
    "crumbs": [
      "Interpreting trial results",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>The replication crisis</span>"
    ]
  },
  {
    "objectID": "interpreting-trial-results/replication-and-reproducibility.html",
    "href": "interpreting-trial-results/replication-and-reproducibility.html",
    "title": "14  Replication and reproducibility",
    "section": "",
    "text": "14.1 Reproducibility\nReproducibility is the ability of a different analyst to re-perform the same analysis of experimental data.\nReproducible research is research that provides the full materials require to reproduce academic research. It includes features such as the code and data that was used and details on the computational environment in which it was examined.\nReplication is the ability to re-perform the experiment and collect new data. An experiment is said to replicate if it returns a similar result.\nThe terms reproduction and replication are sometimes used interchangeably, so their precise names don’t matter, but the distinct concepts do.\nEven with the same data, people can come to different conclusions about the hypotheses.\nThey can transform the data in different ways. They can use different tests for the hypotheses. There are many possible analytic approaches. The result is that experimental data does not in itself provide the conclusion. Absent reproducible research, it is not clear what choices were made and the effect of those choices on the conclusion cannot be examined.\nAs one dramatic illustration involved an analysis of data to see whether soccer referees are more likely to give red cards to dark-skinned players than to light-skinned players (Silberzahn et al., 2018). Twenty-nine analyst teams were tasked with answering this question. They found effect sizes ranging from 0.89 (dark-skinned players receive fewer) to 2.93 (dark-skinned players receive almost three times as many). Twenty of the 29 teams found a statistically significant positive effect, with the other nine failing to find a statistically significant relationship.",
    "crumbs": [
      "Interpreting trial results",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Replication and reproducibility</span>"
    ]
  },
  {
    "objectID": "interpreting-trial-results/replication-and-reproducibility.html#statcheck",
    "href": "interpreting-trial-results/replication-and-reproducibility.html#statcheck",
    "title": "14  Replication and reproducibility",
    "section": "14.2 Statcheck",
    "text": "14.2 Statcheck\nOne way to perform a quick robustness check across a paper is by using statcheck. To use statcheck, you upload a pdf, HTML or docx of the paper. Statcheck then extracts details of the tests reported in the paper and checks that the reported numbers are consistent. (An R package for statcheck allows you to run your own statcheck implementation.)\nAs an example, I uploaded the Dietvorst et al. (2015) pdf, but the analysis did not work. I then accessed a HTML download of the paper using the UTS library website. Uploading the HTML version of the paper resulted in the following.\n\nThe results appear consistent with the reported tests.\nStatcheck does not work on all papers and only checks that the tests are consistent with the reported numbers, but it is a quick way to look for red flags.\n\n\n\n\nDietvorst, B. J., Simmons, J. P., and Massey, C. (2015). Algorithm aversion: People erroneously avoid algorithms after seeing them err. Journal of Experimental Psychology: General, 144, 114–126. https://doi.org/10.1037/xge0000033\n\n\nSilberzahn, R., Uhlmann, E. L., Martin, D. P., Anselmi, P., Aust, F., Awtrey, E., Bahník, Š., Bai, F., Bannard, C., Bonnier, E., Carlsson, R., Cheung, F., Christensen, G., Clay, R., Craig, M. A., Dalla Rosa, A., Dam, L., Evans, M. H., Flores Cervantes, I., … Nosek, B. A. (2018). Many Analysts, One Data Set: Making Transparent How Variations in Analytic Choices Affect Results. Advances in Methods and Practices in Psychological Science, 1(3), 337–356. https://doi.org/10.1177/2515245917747646",
    "crumbs": [
      "Interpreting trial results",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Replication and reproducibility</span>"
    ]
  },
  {
    "objectID": "interpreting-trial-results/publication-bias.html",
    "href": "interpreting-trial-results/publication-bias.html",
    "title": "15  Publication bias",
    "section": "",
    "text": "15.1 Analysis to identify publication bias\nIn 2011, the Journal of Personality and Social Psychology (a big, flagship journal) published a paper by Daryl Bem (2011) entitled “Feeling the Future: Experimental Evidence of Anomalous Retroactive Influences on Cognition and Affect.”\nIn the paper, Bem described nine experiments. In the first experiment, participants were shown pictures of two curtains side-by-side on a screen. One had a picture behind it, the other a blank wall. The participants were asked to click on the curtain they felt had the picture behind it. They were then shown if they had selected the correct curtain.\nSome of the pictures shown to the participants were “erotic”. Where there was an erotic picture, participants selected the pictures more often than expected by chance: 53.1% of the time. For non-erotic pictures, the probability of success did not vary significantly from chance. The p-value for selecting the erotic pictures was 0.01, a significant result.\nWould Bem’s research have been published in a top psychology journal if he had not obtained a statistically significant result? Would the journal waste its resources publishing a paper suggesting people do not have ESP?\nThis point is the essence of publication bias. Publication bias occurs where the outcome of an experiment influences whether it is published or not.\nStudies that find significant effects are more likely to be published. This incentivises those conducting experiments to only write up their positive results. Studies that do not generate statistically significant results end up in the file drawer. Ultimately, the published literature ceases to be a representative sample of the evidence. Instead, it is biased.\nWhile it is easy to see the incentives that might generate publication bias, measuring publication bias is more difficult and sometimes controversial.\nOne common way is through the use of a funnel plot, which is used to analyse for publication bias across a literature. The effect sizes of all experiments examining a particular intervention are plotted against the precision of the studies. Precision is usually proxied by study size or the standard error.\nA literature in which all experimental results are published should see a spread of results around the effect size, with smaller or less precise studies having more variation around that point. This results in a funnel shape of results, as in the first diagram.\nWhere there is publication bias, there is often an asymmetry in that the results on one side of the funnel plot (typically the small sample studies that delivered results in the unintended direction) are missing.\nImage from Cressey (2017)\nAsymmetric funnel plots are not definitive of publication bias, and rest on several assumptions such as a lack of systematic link between size of effect and size of study (which there may be if people use a larger sample because they believe the effect is small).",
    "crumbs": [
      "Interpreting trial results",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Publication bias</span>"
    ]
  },
  {
    "objectID": "interpreting-trial-results/publication-bias.html#remedying-publication-bias",
    "href": "interpreting-trial-results/publication-bias.html#remedying-publication-bias",
    "title": "15  Publication bias",
    "section": "15.2 Remedying publication bias",
    "text": "15.2 Remedying publication bias\nA primary method proposed for reducing publication bias is the use of pre-registration. This provides a basis for understanding the full scope of the studies that have been undertaken.\nHowever, this is not a complete remedy as many pre-registered studies are not published and their results not available. That prevents us from obtaining a complete view of the experiments that have been conducted.\n\n\n\n\nBem, D. J. (2011). Feeling the future: Experimental evidence for anomalous retroactive influences on cognition and affect. Journal of Personality and Social Psychology, 100(3), 407–425. https://doi.org/10.1037/a0021524\n\n\nCressey, D. (2017). Tool for detecting publication bias goes under spotlight. Nature. https://doi.org/10.1038/nature.2017.21728",
    "crumbs": [
      "Interpreting trial results",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Publication bias</span>"
    ]
  },
  {
    "objectID": "interpreting-trial-results/p-hacking-and-the-garden-of-forking-paths.html",
    "href": "interpreting-trial-results/p-hacking-and-the-garden-of-forking-paths.html",
    "title": "16  P-hacking and the garden of forking paths",
    "section": "",
    "text": "16.1 P-hacking\nIn conducting their analysis, researchers have many decisions to make. Should they collect more data? How should they process the data? Should they transform it in any way? Should they exclude outliers? What statistical tests will they use? What outcomes will they test? And so on.\nP-hacking occurs where researchers use this flexibility in their data collection, analysis and reporting to inflate their rate of significant findings. Result not significant yet? Collect some more data. Result not significant with that extreme data point? Exclude it. No significant change after 6 months? Try 12. No significant effect for people predicting what picture is behind the curtain? Let’s restrict the analysis to the erotic ones.\nA p-value of 0.05 sets an effective false positive rate of 0.05. You will only see data that extreme 5% of the time if the null hypothesis is true. But if you look at the data in many different ways and select only those arrangements that result in low p-values, the p-value ceases to be informative about the false positive rate. The false positive rate will be much higher (Simmons et al., 2011).\nP-hacking is considered to be a major cause of the replication crisis. Even without publication bias, p-hacking can dramatically inflate the rate of false positives in the literature that will fail to replicate if tested.",
    "crumbs": [
      "Interpreting trial results",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>P-hacking and the garden of forking paths</span>"
    ]
  },
  {
    "objectID": "interpreting-trial-results/p-hacking-and-the-garden-of-forking-paths.html#p-hacking",
    "href": "interpreting-trial-results/p-hacking-and-the-garden-of-forking-paths.html#p-hacking",
    "title": "16  P-hacking and the garden of forking paths",
    "section": "",
    "text": "16.1.1 P-hacking without intent\nP-hacking should not be taken to infer nefarious research practices (Brian Wansink and the Cornell Food Lab excepted (Lee, 2018)). Nor is p-hacking necessarily a case of the researcher making many comparisons and throwing out those results that don’t meet their needs.\nRather, researchers might do a single analysis given their assumptions and the data. However, that single analysis is contingent on the data they see. If the data had been different they might have done a different analysis. There is effectively a large number of comparisons they could have done, and they pick the a more prospective approach. Andrew Gelman and Eric Loken (2013) call this the garden of forking paths. Even though seemingly benign, the selection of the most prospective path effectively inflates the false positive rate.\n\n\n16.1.2 An example\nAndrew Gelman Jennifer Hill and Aki Vehtari (2020) write:\n\nWe demonstrate the last two problems mentioned above — multiple potential comparisons and the statistical significance filter — using the example of a research article published in a leading journal of psychology. The article begins:\n\nEach month many women experience an ovulatory cycle that regulates fertility. Whereas research finds that this cycle influences women’s mating preferences, we propose that it might also change women’s political and religious views. Building on theory suggesting that political and religious orientation are linked to reproductive goals, we tested how fertility influenced women’s politics, religiosity, and voting in the 2012 U.S. presidential election. In two studies with large and diverse samples, ovulation had drastically different effects on single versus married women. Ovulation led single women to become more liberal, less religious, and more likely to vote for Barack Obama. In contrast, ovulation led married women to become more conservative, more religious, and more likely to vote for Mitt Romney. In addition, ovulatory-induced changes in political orientation mediated women’s voting behavior. Overall, the ovulatory cycle not only influences women’s politics, but appears to do so differently for single versus married women.\n\nOne problem here is that there are so many different things that could be compared, but all we see is some subset of the comparisons. Some of the choices available in this analysis include the days of the month characterized as peak fertility, the dividing line between single and married (in this particular study, unmarried but partnered women were counted as married), data exclusion rules based on reports of menstrual cycle length and timing, and the decision of which interactions to study. Given all these possibilities, it is no surprise at all that statistically significant comparisons turned up; this would be expected even were the data generated purely by noise.\n\n\n\n16.1.3 Preventing p-hacking\nThere are many proposed solutions for p-hacking. The major solution is pre-registration of analysis plans. These transparently commit researchers to an approach, meaning that we can then take the p-value as a prima facie indication of seeing data that extreme under the assumption that the null hypothesis is true. These are discussed further on the next page.\n\n\n\n\nGelman, A., Hill, J., and Vehtari, A. (2020). Regression and Other Stories. Cambridge University Press. https://doi.org/10.1017/9781139161879\n\n\nGelman, A., and Loken, E. (2013). The garden of forking paths: Why multiple comparisons can be a problem, even when there is no “fishing expedition” or “p-hacking” and the research hypothesis was posited ahead of time. https://sites.stat.columbia.edu/gelman/research/unpublished/p_hacking.pdf\n\n\nLee, S. M. (2018). Sliced And Diced: The Inside Story Of How An Ivy League Food Scientist Turned Shoddy Data Into Viral Studies. BuzzFeed News. https://www.buzzfeednews.com/article/stephaniemlee/brian-wansink-cornell-p-hacking\n\n\nSimmons, J. P., Nelson, L. D., and Simonsohn, U. (2011). False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant. Psychological Science, 22(11), 1359–1366. https://doi.org/10.1177/0956797611417632",
    "crumbs": [
      "Interpreting trial results",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>P-hacking and the garden of forking paths</span>"
    ]
  },
  {
    "objectID": "interpreting-trial-results/pre-analysis-plans-and-preregistration.html",
    "href": "interpreting-trial-results/pre-analysis-plans-and-preregistration.html",
    "title": "17  Pre-analysis plans and pre-registration",
    "section": "",
    "text": "A pre-analysis plan is a step-by-step plan setting out how you will analyse the data from your experiment. It is written before data collection, or at least before seeing the data.\nComponents of a pre-analysis plan typically include:\n\nA description of the sample that will be used, including proposed sample size and randomisation techniques\nThe hypotheses to be tested\nHow any variables will be constructed (i.e. you may be planning to manipulate data in the analysis, such as by excluding outlier values or transforming it to log levels)\nWhat statistical tests will be used\nHow you will deal with multiple outcomes\n\nCommitment to a pre-analysis plan is required to be able to take p-values at their face value.Why? A p-value is the probability of the data on the assumption that the null hypotheses is true. As we noted before, our measurement of the data is a test statistic such as a z-score.But your measurement of the data relies on the analysis you choose. If you change that analysis based on the data you see (e.g. by excluding outliers or measuring at a different point in time or using a different outcome measure) your p-value is no longer the probability of the data given the null hypothesis is true. The p-value is now the probability of the data given the null hypothesis is true and given you made a series of choices.\nTo put this into more practical terms, all of the choices that are specified in a pre-analysis plan affect the chance of “success” of a trial. The more degrees of freedom you allow yourself in your analysis, the more likely you are to stumble upon a p-value lower than 0.05 by chance. Don’t find anything at 6 months, try 12. Maybe exclude the outlier. And so on.\nThese researcher degrees of freedom are one reason behind the “replication crisis” that we will explore in the third module.\n\n17.0.1 Preregistration\nPreregistration is publication of the pre-analysis plan in a public archive. This acts as a form of commitment to the pre-analysis plan and enables others to verify that you have done the analysis as you stated.\nAs an example, BETA at PM&C pre-registers many of it studies. You can see some of the pre-registered studies here.",
    "crumbs": [
      "Interpreting trial results",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Pre-analysis plans and pre-registration</span>"
    ]
  },
  {
    "objectID": "interpreting-trial-results/type-m-and-type-s-errors.html",
    "href": "interpreting-trial-results/type-m-and-type-s-errors.html",
    "title": "18  Type S and M errors",
    "section": "",
    "text": "18.1 An example\nWhen a null hypothesis is rejected, people tend to report and make decisions based on the point estimate of the sign and magnitude of the effect. We have already highlighted that effect sizes should be treated with caution, but there are alternative ways to examine the properties of this effect size.\nOne of these is the concept of Type S (Sign) and Type M (Magnitude) errors (Gelman and Carlin, 2014).\nA Type S error occurs when the sign of the estimated effect is in the opposite direction to the true effect. The Type S error rate is the probability of the sign being in the opposite direction.\nA Type M error occurs when the magnitude of the estimated effect is much different from (larger than) the true effect. Type M errors are expressed in terms of the expected exaggeration factor, the expected ratio of the size of the estimated effect divided by the size of the underlying effect.\nThese errors tend to occur in low powered studies. As one example, suppose we have an effect size that cannot realistically be more than 2 percentage points, and a standard error of 8 percentage points. The below diagram shows the distribution of estimated effect sizes that would occur with this underlying data.\nFor a statistically significant result, the effect size needs to be ~16 percentage points (around 2 standard deviations greater than zero). This is an 8-fold exaggeration of the true effect size. There is also a 24% probability that, in the case of a significant result, it is in the wrong direction.\nThis shows that the problem with a low-powered study is not just the high probability of a Type II error. The problem is that even if the researcher gets a statistical significant result, the effect size can have a high probability of being massively exaggerated or even in the opposite direction. This is not just a case of bad luck: in the case of a low powered study, the effect size can only be significant if it is exaggerated.\nThe net result of this framework is that low-powered studies will always generate at least one form of error, be that Type II or Type M.\nContinuing the example of the ovulatory cycle and voting, Andrew Gelman, Jennifer Hill and Aki Vehtari (2020) write:",
    "crumbs": [
      "Interpreting trial results",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Type S and M errors</span>"
    ]
  },
  {
    "objectID": "interpreting-trial-results/type-m-and-type-s-errors.html#an-example",
    "href": "interpreting-trial-results/type-m-and-type-s-errors.html#an-example",
    "title": "18  Type S and M errors",
    "section": "",
    "text": "In addition, relative to our understanding of the vast literature on voting behavior, the claimed effects seem implausibly large — a type M error. For example, the paper reports that, among women in relationships, 40% in the ovulation period supported Romney, compared to 23% in the non-fertile part of their cycle. Given that opinion polls find very few people switching their vote preferences during the campaign for any reason, these numbers seem unrealistic. The authors might respond that they don’t care about the magnitude of the difference, just the sign, but (a) with a magnitude of this size, we are talking noise (not just sampling error but also errors in measurement), and (b) one could just as easily explain this as a differential nonresponse pattern: maybe liberal or conservative women in different parts of their cycle are more or less likely to participate in a survey. It would be easy enough to come up with a story about that.\n\n\n\n\n\nGelman, A., and Carlin, J. (2014). Beyond Power Calculations: Assessing Type S (Sign) and Type M (Magnitude) Errors. Perspectives on Psychological Science, 9(6), 641–651. https://doi.org/10.1177/1745691614551642\n\n\nGelman, A., Hill, J., and Vehtari, A. (2020). Regression and Other Stories. Cambridge University Press. https://doi.org/10.1017/9781139161879",
    "crumbs": [
      "Interpreting trial results",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Type S and M errors</span>"
    ]
  },
  {
    "objectID": "interpreting-trial-results/validity.html",
    "href": "interpreting-trial-results/validity.html",
    "title": "19  Validity",
    "section": "",
    "text": "Validity is the extent to which the results of an experiment support a more general conclusion.\nThere are many forms of validity. Here we briefly define four (Salganik, 2018).\nStatistical conclusion validity is the extent to which the experimental statistical analysis was done directly. For example, did the experimenter calculate the p-values correctly?\nInternal validity is the extent to which the experimental treatment is actually responsible for the change in value of the dependent variable. You are able to link cause and effect while controlling for the effect of outside variables (usually by randomisation). Internal validity also concerns whether the experimental procedures were performed correctly. Internal validity tends to be higher in the lab, although the failure of many lab experiments to replicate indicates a problem of low validity.\nConstruct validity concerns whether the data matches the theoretical constructs. If you believe that a social norm triggers someone to pay their tax on time, does your treatment manipulate social norms while holding other constructs (such as prompts) constant?\nExternal validity is the extent that experimental findings can be generalised to the population from which the participants in the experiment were drawn. Field experiments tend to provide higher external validity than those constrained to the lab.\n\n19.0.1 The validity of lab and field experiments\nA core driver of whether a lab or field experiment is more appropriate is whether internal or external validity are more important.\nJohn List and Omar Al-Ubaydli (2014) provide one perspective on this trade-off:\n\nBob wants to purchase Susan’s mug for $5, but they live far apart and so he will need to send a check. The mug is worth $3 to Susan and $9 to Bob, implying a societal surplus of $9 - $3 = $6 if the transaction occurs. However, if Bob sends the money first, will Susan send the mug? If Susan sends the mug first, will Bob send the money? Signing a legally enforceable contract would facilitate the trade.\nHowever, what if property rights are poorly enforced (e.g., if they live in different countries)? Then their fear may result in them forgoing the trade and the surplus of $6. …\n\nHow can we test whether we could expect the trade to occur? Perhaps we could look at the trust game?\n\nThis is what Berg et al. (1995) did using the ‘trust game’ – a microcosm of Bob and Susan’s quandary. The sender starts with $10 and the responder starts with $0. The sender can send any amount to the responder, retaining the remainder. The responder receives triple whatever the sender transfers. The responder then decides how to divide the tripled amount between the two, terminating the game. For example, if the sender transfers $4, the sender retains $6 and the responder receives $4 x 3 = $12; finally, the responder can choose to return anywhere between $0 and $12.\nThe desirable outcome is for the sender to transfer all $10, and for the responder to return at least $10 ($15 under egalitarianism). This mimics Bob and Susan trading the mug. It leaves the two with $30 between them – much more than just $10 with the sender. But the sender may doubt the responder’s trustworthiness. The responder may just choose to pocket whatever the sender transfers since the sender has no legal recourse. Anticipating this, like Bob and Susan, the sender may just decide to avoid transacting, retaining the $10.\nBerg et al.’s results suggest that such fears were potentially ill-founded. Even when the trust game was played with anonymous strangers, on average, senders would send $5.16, and responders would return $4.66. The authors appealed to altruism to explain the results, i.e., the players feel bad about the other party receiving a low payoff, motivating behaviour closer to what emerges under fully enforced property rights. Subsequent studies (Fehr et al. 1993) have confirmed these results, and interviews reveal that participants’ altruism is their most common stated motivation.\n\nBut should this experiment provide comfort to Bob? List and Al-Ubaydli describe a related field experiment.\n\nI went to professional sports memorabilia markets and recruited professional traders to play laboratory trust games. Like the literature, I found a modest positive causal effect of property rights.\nI then ran a complementary field experiment. As a sports card enthusiast, I was aware that the market was awash with player cards of different quality (grade), and that the professional sellers were better at discerning grade than your average fan milling around the exhibition. Critically, an individual requesting a high-grade card and receiving a low-grade card was rarely aware at the time of the transaction and, if they found out subsequently, they had no legal recourse. Thus, buying a card required a buyer to trust – like Bob buying the mug – the sender in the trust game. I recruited some archetypal sports fans to go to professional sellers and request a specific card at a specific grade in exchange for a predetermined price, without revealing to the seller that this was an experiment. If traders were completely selfish, then they would return the lowest grade of card whatever price was offered to them, confirming the need to enforce property rights. Alternatively, if the traders behaved ‘altruistically’ like the laboratory experiment participants, then they should offer higher quality when offered higher prices.\nThe results were pretty grim for anyone who believes in the humanity of sports card dealers – card quality was insensitive to price offers. Note that these were the same traders who had apparently exhibited altruistic tendencies in the preceding laboratory experiment. …\nThese results painted a bleaker picture of anonymous trade in the absence of property rights. Were Bob and Susan to read the entire literature, what would be the epistemologically ideal way for them to update their beliefs on the causal effect of property rights on trading behaviour? Which results generalise to their setting more accurately – laboratory or field?\n\n\n\n19.0.2 The validity of randomised controlled trials\nMuch of this unit has focussed on how we can use randomised controlled trials to obtain accurate measures of treatment effects, as opposed to delving into how the results could be used. This is effectively a focus on internal validity.\nThis unit’s focus is matched in much of the literature about randomised controlled trials. External validity if often an afterthought. And in an article arguing against placing randomised controlled trials on a pedestal, Angus Deaton and Nancy Cartwright (2018) agree that this can have value:\n\nSuppose a trial has (probabilistically) established a result in a specific setting. If ‘the same’ result holds elsewhere, it is said to have external validity. External validity may refer just to the replication of the causal connection or go further and require replication of the magnitude of the ATE. Either way, the result holds—everywhere, or widely, or in some specific elsewhere—or it does not.\nThis binary concept of external validity is often unhelpful because it asks the results of an RCT to satisfy a condition that is neither necessary nor sufficient for trials to be useful, and so both overstates and understates their value. It directs us toward simple extrapolation—whether the same result holds elsewhere—or simple generalization—it holds universally or at least widely—and away from more complex but equally useful applications of the results. The failure of external validity interpreted as simple generalization or extrapolation says little about the value of the results of the trial.\n\nBut this paragraph starts to shape the critique:\n\nEstablishing causality does nothing in and of itself to guarantee that the causal relation will hold in some new case, let alone in general. Nor does the ability of an ideal RCT to eliminate bias from selection or from omitted variables mean that the resulting ATE from the trial sample will apply anywhere else.\n\n\n\n\n\nDeaton, A., and Cartwright, N. (2018). Understanding and misunderstanding randomized controlled trials. Social Science & Medicine, 210, 2–21. https://doi.org/10.1016/j.socscimed.2017.12.005\n\n\nList, J., and Al-Ubaydii, O. (2014). The generalisability of experimental results in economics. CEPR. https://cepr.org/voxeu/columns/generalisability-experimental-results-economics\n\n\nSalganik, M. (2018). Bit by Bit. Princeton University Press. https://press.princeton.edu/books/paperback/9780691196107/bit-by-bit",
    "crumbs": [
      "Interpreting trial results",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Validity</span>"
    ]
  },
  {
    "objectID": "interpreting-trial-results/generalisability-effect-sizes.html",
    "href": "interpreting-trial-results/generalisability-effect-sizes.html",
    "title": "20  Generalisability: effect sizes",
    "section": "",
    "text": "A term often used in substitute of validity, particularly external validity, is generalisability. The term is most commonly used when describing whether the results of a field trial can be taken to inform how an intervention might work in another context or at scale.\nThere are limited systematic evaluations of the generalisability of applied behavioural science trials. However, research on the generalisability of impact evaluations in a development context provides insight into this question.\nEva Vivalt (2020) reviewed 635 papers containing 15,024 estimates of effect sizes relating to 20 types of interventions in international development. In her paper, she assessed the extent the results from a particular intervention could be used to predict the sign of the effect or the magnitude of the effect of a similar study in another context. This question is effectively an examination of the Type M and Type S errors we discussed.\nShe found that an inference about a study’s effect using another similar study will have the correct sign 61% of the time (comparing the median intervention-outcome pair in each study). When comparing effect sizes, a naive prediction of the result in the new study is likely to be wrong by about 249%.\nListen to Eva Vivalt on the 80,000 Hours podcast, or read the transcript.\n\n\nAnother perspective on generalisability comes from analysis by DellaVigna and Linos (2022) of the implementation of behavioural interventions by two “Nudge Units” in the United States. They compared the results from 126 randomised controlled trials run by the Nudge Units to a sample of trials in academic journals. They wrote:\nIn the Academic Journals papers, the average impact of a nudge is very large—an 8.7 percentage point take-up effect, which is a 33.4% increase over the average control. In the Nudge Units sample, the average impact is still sizable and highly statistically significant, but smaller at 1.4 percentage points, an 8.0% increase. We document three dimensions which can account for the difference between these two estimates: (i) statistical power of the trials; (ii) characteristics of the interventions, such as topic area and behavioral channel; and (iii) selective publication. A meta-analysis model incorporating these dimensions indicates that selective publication in the Academic Journals sample, exacerbated by low statistical power, explains about 70 percent of the difference in effect sizes between the two samples. Different nudge characteristics account for most of the residual difference.\n\n\n\n\nDellaVigna, S., and Linos, E. (2022). RCTs to Scale: Comprehensive Evidence From Two Nudge Units. Econometrica, 90(1), 81–116. https://doi.org/10.3982/ECTA18709\n\n\nVivalt, E. (2020). How much can we generalize from impact evaluations? Journal of the European Economic Association, 18(6), 3045–3089. https://doi.org/10.1093/jeea/jvaa019",
    "crumbs": [
      "Interpreting trial results",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Generalisability: effect sizes</span>"
    ]
  },
  {
    "objectID": "interpreting-trial-results/generalisability-context.html",
    "href": "interpreting-trial-results/generalisability-context.html",
    "title": "21  Generalisability: context",
    "section": "",
    "text": "Recall the jam experiment we discussed previously.\nOn two Saturdays in a California supermarket, Sheena Iyengar and Mark Lepper (2000) set up tasting displays of either six or 24 jars of jam. Consumers could taste as many jams as they wished, and if they approached the tasting table they received a $1 discount coupon to buy the jam.\nFor attracting initial interest, the large display of 24 jams did a better job, with 60 per cent of people who passed the display stopping. Forty per cent of people stopped at the six jam display. But only three per cent of those who stopped at the 24 jam display purchased any of the jam, compared with almost 30 per cent who stopped at the six jam display.\nThis experiment has gained famed as showing the paradox of choice. But how much weight should we place on this one experiment? In his book Uncontrolled, Jim Manzi (2012) writes:\n\nFirst, note that all of the inference is built on the purchase of a grand total of thirty-five jars of jam. Second, note that if the results of the jam experiment were valid and applicable with the kind of generality required to be relevant as the basis for economic or social policy, it would imply that many stores could eliminate 75 percent of their products and cause sales to increase by 900 percent. That would be a fairly astounding result—and indicates that there may be a problem with the measurement.\nMeasurement problems could easily arise because the experiment was done for a total of ten hours in only one store, and shoppers were grouped in hourly chunks. There could be all kinds of reasons that those people who happened to show up during the five hours of limited assortment could have systematically different propensity to respond to $ 1 off a specific line of jams than those who arrived in the other five-hour period: a soccer game finished at some specific time, and several of the parents who share similar propensities versus the average shopper came in nearly together; a bad traffic jam in one part of town with non-average propensity to respond to the coupon dissuaded several people from going to the store at one time versus another; etc. This is one reason retail experiments for such in-store promotional tactics are typically executed for twenty or thirty randomly assigned stores for a period of weeks.\n\nBenjamin Scheibehenne and friends (2010) surveyed the broader literature on the choice overload hypothesis. In some cases, choice increased purchases. In others it reduced them. Scheibehenne and friends determined that the mean effect size of changing the number of choices across the studies was effectively zero.\nFrom this result, Manzi continues:\n\nFirst, individual experiments need to encompass as much variation in background conditions as is feasible. It is almost impossible to run an experiment in one store that can produce valid conclusions. Social science RFTs that are executed across several school districts, court systems, welfare offices, or whatever are a much more reliable guide to action than single-site experiments. Experiments also need to run long enough to encompass changing background conditions over time. The combination of more sites and more time creates many more observations, and therefore reliability.\nSecond, the ultimate test of the validity of causal conclusions derived from an experiment is the ability to predict the results of future tests. We need to build the kind of distribution of multiple experiments that were summarized for the impact of breadth of choice on sales and satisfaction in Scheibehenne’s meta-analysis. Such a distribution allows us to measure the scope (if any) of reliable prediction based on some sequence of experiments. In the case of the jam experiment, the researchers in the original experiment themselves were careful about their explicit claims of generalizability, and significant effort has been devoted to the exact question of finding conditions under which choice overload occurs consistently, but popularizers telescoped the conclusions derived from one coupon-plus-display promotion in one store on two Saturdays, up through assertions about the impact of product selection for jam for this store, to the impact of product selection for jam for all grocery stores in America, to claims about the impact of product selection for all retail products of any kind in every store, ultimately to fairly grandiose claims about the benefits of choice to society.\n\n\n\n\n\nIyengar, S. S., and Lepper, M. R. (2000). When choice is demotivating: Can one desire too much of a good thing? Journal of Personality and Social Psychology, 79(6), 995–1006. https://doi.org/10.1037/0022-3514.79.6.995\n\n\nManzi, J. (2012). Uncontrolled. Basic Books. https://www.hachettebookgroup.com/titles/jim-manzi/uncontrolled/9780465029310/?lens=basic-books\n\n\nScheibehenne, B., Greifeneder, R., and Todd, P. M. (2010). Can there ever be too many options? A meta-analytic review of choice overload. Journal of Consumer Research, 37(3), 409–425. https://doi.org/10.1086/651235",
    "crumbs": [
      "Interpreting trial results",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Generalisability: context</span>"
    ]
  },
  {
    "objectID": "interpreting-trial-results/heterogeneity.html",
    "href": "interpreting-trial-results/heterogeneity.html",
    "title": "22  Heterogeneity",
    "section": "",
    "text": "Experimental treatments can have different effects on different people.\nIn most randomised controlled trials, however, there is a focus on average treatment effects. This is partly a legacy of the sample sizes we use. Partition into smaller groups requires a larger total sample size for accurate inference.\nThere is considerable benefit in considering heterogeneity in experiments. It can provide insight into how the intervention can be targeted to those who will benefit most, how the treatment works, or how the treatment can be improved.\nAs one illustration, Costa and Kahn (2013) examined how people’s change in energy use in response to a home energy report varied with their political ideology. Their analysis is summarised in the following chart.\n\nKnowledge of the political leanings of the recipients of the energy report could help targeting the reports and influence their design. It might also provide fodder to develop further hypotheses to make them more effective.\n\n22.0.1 Heterogeneity across cultures\nOne way in which heterogeneity shows itself is across cultures. The artefactual field trial we examined earlier, where Joe Henrich and friends (2001) played the ultimatum game in small scale societies, demonstrates this.\nHowever, there is also a commonality across cultures. In the Many Labs 2 study (Klein et al., 2018), researchers performed replications of various findings in behavioural science across a range of different countries and cultures. A core theme of the result was that the same studies tended to replicate across countries and cultures, or tended to fail everywhere.\n\nOne important implication of this is that, contrary to some claims, heterogeneity of study participants is unlikely to be an explanation for the replication crisis.\n\n\n\n\nCosta, D. L., and Kahn, M. E. (2013). Energy conservation ’nudges’ and environmentalist ideology: Evidence from a randomized residential electricity field experiment. Journal of the European Economic Association, 11(3), 680–702. https://doi.org/10.1111/jeea.12011\n\n\nHenrich, J., Boyd, R., Bowles, S., Camerer, C., Fehr, E., Gintis, H., and McElreath, R. (2001). In Search of Homo Economicus: Behavioral Experiments in 15 Small-Scale Societies. American Economic Review, 91(2), 73–78. https://doi.org/10.1257/aer.91.2.73\n\n\nKlein, R. A., Vianello, M., Hasselman, F., Adams, B. G., Adams, R. B., Alper, S., Aveyard, M., Axt, J. R., Babalola, M. T., Bahník, Š., Batra, R., Berkics, M., Bernstein, M. J., Berry, D. R., Bialobrzeska, O., Binan, E. D., Bocian, K., Brandt, M. J., Busching, R., … Nosek, B. A. (2018). Many Labs 2: Investigating Variation in Replicability Across Samples and Settings. Advances in Methods and Practices in Psychological Science, 1(4), 443–490. https://doi.org/10.1177/2515245918810225",
    "crumbs": [
      "Interpreting trial results",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Heterogeneity</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Alter, A. L., Oppenheimer, D. M., Epley, N., and Eyre, R. N. (2007).\nOvercoming intuition: Metacognitive difficulty activates analytic\nreasoning. Journal of Experimental Psychology: General,\n136(4), 569–576. https://doi.org/10.1037/0096-3445.136.4.569\n\n\nBem, D. J. (2011). Feeling the future: Experimental evidence for\nanomalous retroactive influences on cognition and affect. Journal of\nPersonality and Social Psychology, 100(3), 407–425. https://doi.org/10.1037/a0021524\n\n\nCamerer, C. F., Dreber, A., Forsell, E., Ho, T.-H., Huber, J.,\nJohannesson, M., Kirchler, M., Almenberg, J., Altmejd, A., Chan, T.,\nHeikensten, E., Holzmeister, F., Imai, T., Isaksson, S., Nave, G.,\nPfeiffer, T., Razen, M., and Wu, H. (2016). Evaluating replicability of\nlaboratory experiments in economics. Science,\n351(6280), 1433–1436. https://doi.org/10.1126/science.aaf0918\n\n\nCosta, D. L., and Kahn, M. E. (2013). Energy conservation ’nudges’ and\nenvironmentalist ideology: Evidence from a randomized residential\nelectricity field experiment. Journal of the European Economic\nAssociation, 11(3), 680–702. https://doi.org/10.1111/jeea.12011\n\n\nCressey, D. (2017). Tool for detecting publication bias goes under\nspotlight. Nature. https://doi.org/10.1038/nature.2017.21728\n\n\nDeaton, A., and Cartwright, N. (2018). Understanding and\nmisunderstanding randomized controlled trials. Social Science &\nMedicine, 210, 2–21. https://doi.org/10.1016/j.socscimed.2017.12.005\n\n\nDellaVigna, S., and Linos, E. (2022). RCTs to Scale: Comprehensive\nEvidence From Two Nudge Units. Econometrica, 90(1),\n81–116. https://doi.org/10.3982/ECTA18709\n\n\nDietvorst, B. J., Simmons, J. P., and Massey, C. (2015). Algorithm\naversion: People erroneously avoid algorithms after seeing them err.\nJournal of Experimental Psychology: General, 144,\n114–126. https://doi.org/10.1037/xge0000033\n\n\nGelman, A., and Carlin, J. (2014). Beyond Power Calculations: Assessing\nType S (Sign) and Type M (Magnitude) Errors. Perspectives on\nPsychological Science, 9(6), 641–651. https://doi.org/10.1177/1745691614551642\n\n\nGelman, A., Hill, J., and Vehtari, A. (2020). Regression and Other\nStories. Cambridge University Press. https://doi.org/10.1017/9781139161879\n\n\nGelman, A., and Loken, E. (2013). The garden of forking paths: Why\nmultiple comparisons can be a problem, even when there is no\n“fishing expedition” or\n“p-hacking” and the research hypothesis was\nposited ahead of time. https://sites.stat.columbia.edu/gelman/research/unpublished/p_hacking.pdf\n\n\nHarrison, G. W., and List, J. A. (2004). Field Experiments. Journal\nof Economic Literature, 47.\n\n\nHaynes, L., Service, O., Goldacre, B., and Torgerson, D. (2012).\nTest, Learn, Adapt: Developing Public Policy with Randomised\nControlled Trials. https://www.gov.uk/government/publications/test-learn-adapt-developing-public-policy-with-randomised-controlled-trials\n\n\nHenrich, J., Boyd, R., Bowles, S., Camerer, C., Fehr, E., Gintis, H.,\nand McElreath, R. (2001). In Search of Homo Economicus: Behavioral\nExperiments in 15 Small-Scale Societies. American Economic\nReview, 91(2), 73–78. https://doi.org/10.1257/aer.91.2.73\n\n\nIyengar, S. S., and Lepper, M. R. (2000). When choice is demotivating:\nCan one desire too much of a good thing? Journal of Personality and\nSocial Psychology, 79(6), 995–1006. https://doi.org/10.1037/0022-3514.79.6.995\n\n\nKahneman, D., and Tversky, A. (1979). Prospect theory: An analysis of\ndecision under risk. Econometrica, 47(2), 263–291. https://doi.org/10.2307/1914185\n\n\nKlein, R. A., Vianello, M., Hasselman, F., Adams, B. G., Adams, R. B.,\nAlper, S., Aveyard, M., Axt, J. R., Babalola, M. T., Bahník, Š., Batra,\nR., Berkics, M., Bernstein, M. J., Berry, D. R., Bialobrzeska, O.,\nBinan, E. D., Bocian, K., Brandt, M. J., Busching, R., … Nosek, B. A.\n(2018). Many Labs 2: Investigating Variation in Replicability Across\nSamples and Settings. Advances in Methods and Practices in\nPsychological Science, 1(4), 443–490. https://doi.org/10.1177/2515245918810225\n\n\nLee, S. M. (2018). Sliced And Diced: The Inside Story Of How An Ivy\nLeague Food Scientist Turned Shoddy Data Into Viral Studies.\nBuzzFeed News. https://www.buzzfeednews.com/article/stephaniemlee/brian-wansink-cornell-p-hacking\n\n\nList, J. A. (2011). Why Economists Should Conduct Field Experiments and\n14 Tips for Pulling One Off. Journal of Economic Perspectives,\n25(3), 3–16. https://doi.org/10.1257/jep.25.3.3\n\n\nList, J., and Al-Ubaydii, O. (2014). The generalisability of\nexperimental results in economics. CEPR. https://cepr.org/voxeu/columns/generalisability-experimental-results-economics\n\n\nManzi, J. (2012). Uncontrolled. Basic Books. https://www.hachettebookgroup.com/titles/jim-manzi/uncontrolled/9780465029310/?lens=basic-books\n\n\nMazar, N., Amir, O., and Ariely, D. (2008). The Dishonesty of Honest\nPeople: A Theory of Self-Concept Maintenance. Journal of Marketing\nResearch, 45(6), 633–644. https://doi.org/10.1509/jmkr.45.6.633\n\n\nMeyer, A., Frederick, S., Burnham, T. C., Guevara Pinto, J. D., Boyer,\nT. W., Ball, L. J., Pennycook, G., Ackerman, R., Thompson, V. A., and\nSchuldt, J. P. (2015). Disfluent fonts don’t help people\nsolve math problems. Journal of Experimental Psychology:\nGeneral, 144(2), e16–e30. https://doi.org/10.1037/xge0000049\n\n\nMeyer, M. N., Heck, P. R., Holtzman, G. S., Anderson, S. M., Cai, W.,\nWatts, D. J., and Chabris, C. F. (2019). Objecting to experiments that\ncompare two unobjectionable policies or treatments. Proceedings of\nthe National Academy of Sciences, 116(22), 10723–10728. https://doi.org/10.1073/pnas.1820701116\n\n\nOpen Science Collaboration. (2015). Estimating the reproducibility of\npsychological science. Science, 349(6251), aac4716. https://doi.org/10.1126/science.aac4716\n\n\nRoth, A. E. (1995). Introduction to experimental economics (J.\nH. Kagel and A. E. Roth, Eds.). Princeton University Press. http://doi.org/10.2307/j.ctvzsmff5.5\n\n\nSalganik, M. (2018). Bit by Bit. Princeton University Press. https://press.princeton.edu/books/paperback/9780691196107/bit-by-bit\n\n\nScheibehenne, B., Greifeneder, R., and Todd, P. M. (2010). Can there\never be too many options? A meta-analytic review of choice overload.\nJournal of Consumer Research, 37(3), 409–425. https://doi.org/10.1086/651235\n\n\nSilberzahn, R., Uhlmann, E. L., Martin, D. P., Anselmi, P., Aust, F.,\nAwtrey, E., Bahník, Š., Bai, F., Bannard, C., Bonnier, E., Carlsson, R.,\nCheung, F., Christensen, G., Clay, R., Craig, M. A., Dalla Rosa, A.,\nDam, L., Evans, M. H., Flores Cervantes, I., … Nosek, B. A. (2018). Many\nAnalysts, One Data Set: Making Transparent How Variations in Analytic\nChoices Affect Results. Advances in Methods and Practices in\nPsychological Science, 1(3), 337–356. https://doi.org/10.1177/2515245917747646\n\n\nSimmons, J. P., Nelson, L. D., and Simonsohn, U. (2011). False-Positive\nPsychology: Undisclosed Flexibility in Data Collection and Analysis\nAllows Presenting Anything as Significant. Psychological\nScience, 22(11), 1359–1366. https://doi.org/10.1177/0956797611417632\n\n\nVerschuere, B., Meijer, E. H., Jim, A., Hoogesteyn, K., Orthey, R.,\nMcCarthy, R. J., Skowronski, J. J., Acar, O. A., Aczel, B., Bakos, B.\nE., Barbosa, F., Baskin, E., Bègue, L., Ben-Shakhar, G., Birt, A. R.,\nBlatz, L., Charman, S. D., Claesen, A., Clay, S. L., … Yıldız, E.\n(2018). Registered Replication Report on Mazar, Amir, and Ariely (2008).\nAdvances in Methods and Practices in Psychological Science,\n1(3), 299–317. https://doi.org/10.1177/2515245918781032\n\n\nVivalt, E. (2020). How much can we generalize from impact evaluations?\nJournal of the European Economic Association, 18(6),\n3045–3089. https://doi.org/10.1093/jeea/jvaa019",
    "crumbs": [
      "References"
    ]
  }
]