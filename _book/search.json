[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Course notes on trial design",
    "section": "",
    "text": "Overview\nIn these notes, I introduce the principles and method of experimental research. You will learn the theory and good practices to design experiments in the field and test hypotheses. We will review confounds which can appear in an experimental study and how to ensure that an experimental project delivers the right insights. We will also examine the generalisability of experiments and how we should review the existing experimental literature.\nThrough the book, I will largely focus on trials for public policy or business applications, in the lab or field, rather than for academic purposes. Many of the principles hold across academic, business and policy settings (in fact, they are often the same as a trial may involve both an academic and a business or policy question), but focusing our attention to the applied behavioural science question will allow us to achieve more depth.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "experimental-foundations/experimental-foundations.html",
    "href": "experimental-foundations/experimental-foundations.html",
    "title": "Experimental foundations",
    "section": "",
    "text": "In this part, I will examine the foundations of the use of experiments to understand behavioural phenomena (and in fact, phenomena across the sciences). We will learn about why we should experiment and some types of experiments. We will also foreshadow some of the considerations in designing an experiment that we will examine through this unit.\nBy the end of this part, you will be able to answer the following questions:\n\nWhat is an experiment? 2.Why do we run experiments?\nWhat are the different types of experiments?\nWhat are some of the elements of experimental design?",
    "crumbs": [
      "Experimental foundations"
    ]
  },
  {
    "objectID": "experimental-foundations/why-experiment.html",
    "href": "experimental-foundations/why-experiment.html",
    "title": "1  Why experiment?",
    "section": "",
    "text": "For several decades, adults with severe head injury were treated using steroid injections. This made perfect sense in principle: steroids reduce swelling, and it was believed that swelling inside the skull killed people with head injuries, crushing their brain. However, these assumptions were not subject to proper tests for some time.\nThen, a decade ago, this assumption was tested in a randomised trial. The study was controversial, and many opposed it, because they thought they already knew that steroids were effective. In fact, when the results were published in 2005, they showed people receiving steroid injections were more likely to die: this routine treatment had been killing people, and in large numbers, because head injuries are so common. These results were so extreme that the trial had to be stopped early, to avoid any additional harm being caused.\nThis is a particularly dramatic example of why fair tests of new and existing interventions are important: without them, we can inflict harm unintentionally, without ever knowing it: and when new interventions become common practice without good evidence, then there can be resistance to testing them in the future.\nHaynes et al. (2012) Test, Learn, Adapt: Developing Public Policy with Randomised Controlled Trials\n\nUnderstanding what works is difficult. The world is complicated. Logic and theory can provide insight, but they do not always lead us to the right answer.\nFurther, we often believe that we want to believe. As Richard Feynman said, “The first principle is that you must not fool yourself – and you are the easiest person to fool.”\nOne approach to understanding the world might be to develop a model (a description of the relationships between the variables you are interested in) and then gather data to test the model. However, it is challenging to make causal statements without a counterfactual as to what would otherwise occur. Would those with head injuries have recovered if they had not been given steroids? There are a variety of approaches to making cause-effect statements in those circumstances (the subject of the other unit this session, Principles of Causal Inference), but these all rest on assumptions of varying robustness.\nOne major problem is what Jim Manzi (2012) calls “causal density”. Study the movement of a planet, and you can assume a single causal factor, gravity. If you examine a new way of disclosing information about your credit card to customers, there are so many possible causes of behaviour that, no matter how sophisticated your tools and models, there is always the possibility of some uncontrolled factor causing what you observe. The high causal density makes inferring causation nearly impossible.\nExperiments provide a more direct way of creating a counterfactual and untangling the causally dense environment by constructing a control group against which to compare outcomes. This provides a foundation for us to move beyond mere statements about correlation and to make causal statements.\nImportantly, experiments don’t provide a definitive answer. But even where there is still room for experts to disagree, the room for reasonable disagreement is often narrowed. They can protect us from being fooled into believing what we want to believe.\n\n\n\n\nHaynes, L., Service, O., Goldacre, B., and Torgerson, D. (2012). Test, Learn, Adapt: Developing Public Policy with Randomised Controlled Trials. https://www.gov.uk/government/publications/test-learn-adapt-developing-public-policy-with-randomised-controlled-trials\n\n\nManzi, J. (2012). Uncontrolled. Basic Books. https://www.hachettebookgroup.com/titles/jim-manzi/uncontrolled/9780465029310/?lens=basic-books",
    "crumbs": [
      "Experimental foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Why experiment?</span>"
    ]
  },
  {
    "objectID": "experimental-foundations/the-experimental-approach-in-economics-and-psychology.html",
    "href": "experimental-foundations/the-experimental-approach-in-economics-and-psychology.html",
    "title": "2  The experimental approach in economics and psychology",
    "section": "",
    "text": "Economists and psychologists use controlled experiments to test what choices people make in specific circumstances.\nMost economics experiments are not pure simulations or role-playing exercises. They involve real people who make real choices to make or lose money.\nResearchers design an experiment that captures the features of some “real world” settings, such as markets. Some participants are assigned the roles of buyers and sellers making trades. Participants have an incentive to think carefully about their decisions since the money they earn from trading is theirs to keep.\nDuring the experiment, researchers can change features of the environment, such as the rules of exchange and the incentives. By observing how the participants’ behaviour changes as the rules or incentives change, they can examine the effects of the intervention. They can then compare the actual results of the experiment with theoretical predictions about how people would respond to the change. \nIn 2002, Vernon Smith won the Nobel Prize in Economics for “having established laboratory experiments as a tool in empirical economic analysis, especially in the study of alternative market mechanisms.”",
    "crumbs": [
      "Experimental foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The experimental approach in economics and psychology</span>"
    ]
  },
  {
    "objectID": "experimental-foundations/the-role-of-experiments.html",
    "href": "experimental-foundations/the-role-of-experiments.html",
    "title": "3  The role of experiments",
    "section": "",
    "text": "Nobel Memorial Prize winner Alvin Roth (1995) categorised the three major uses of experiments in economics as follows:\nSpeaking to theorists: These are experiments designed to test the predictions of theories in a precise, controlled and measured environment that allows the observations to be interpreted in relation to the theory. The experiment is used to fill gaps that are hard to fill with raw empirical data. They are intended to feed back into the theoretical literature, help refine theoretical ideas, and discover irregularities that can help formulate new theories. Experiments can also help to distinguish among theories.\nFor example, game theory was a major driver of the emergence of experimental economics from the 1950s. The body of theory was tested, ultimately leading to fields such as behavioural game theory that seek to explain the gap between the original theoretical predictions and the empirical evidence as to how people play games.\nKahneman and Tversky’s testing of expected utility theory also falls into this category. They tested the theory in controlled experiments to see where it might hold or not. The failure of the theory to predict some of the phenomena fed into new theories, such as Kahneman and Tversky’s own prospect theory.\nSearching for facts: This involves studying the effects of variables about which existing theory may have little to say. It can also be thought of as the search for irregularities or the isolation of the cause of previously observed regularities.\nSome of Kahneman and Tversky’s (1979) early work also falls into this category. They created experiments to test whether people would depart from the rules of logic and probability, identifying a rich set of irregularities that are now the common fodder of behavioural economics.\nWhispering in the ears of princes: Experiments can facilitate a dialogue between experimenters and policymakers or business decision makers. The questions in these experiments are motivated by the types of questions raised by regulatory agencies, government service providers, marketing and pricing teams, or business leaders. Experiments are used to test interventions relating to government policy and programs, and business strategy and tactics.\nThe work of the Behavioural Insights Team is perhaps the best-known example of this, although experiments have been used in both business and policy for almost as long as there has been a discipline of experimental economics. That said, they are rarely used relative to their potential.\n\n\n\n\nKahneman, D., and Tversky, A. (1979). Prospect theory: An analysis of decision under risk. Econometrica, 47(2), 263–291. https://doi.org/10.2307/1914185\n\n\nRoth, A. E. (1995). Introduction to experimental economics (J. H. Kagel and A. E. Roth, Eds.). Princeton University Press. http://doi.org/10.2307/j.ctvzsmff5.5",
    "crumbs": [
      "Experimental foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The role of experiments</span>"
    ]
  },
  {
    "objectID": "experimental-foundations/randomised-trials.html",
    "href": "experimental-foundations/randomised-trials.html",
    "title": "4  Randomised trials",
    "section": "",
    "text": "Our main focus in these notes will be on randomised trials done in the lab or field (e.g. schools, firms, health centres, community) to establish the causal impact of an intervention. The causal impact is the difference in outcomes caused by the intervention. It is the comparison of what happened with the program with what happened without the intervention.\nThe fundamental problem of causal inference: we can never observe the same people at the same time both with and without the intervention. We never observe the counterfactual. So we have to mimic the counterfactual.\nWhat is the best way to mimic the counterfactual?\n\n Compare the outcome of people who took up the program with the outcome of people who did not take up the program Compare the outcome of people before they take up the program with the outcome of people after they take up the program Compare the outcome of people randomly assigned into the program with the outcome of people randomly out of the program\n\n\n\nClick here to see an explanation\n\nWhat is the main issue in comparing people who take up the program with people who do not take up the program?\nSelection matters: those who signed up or are chosen for a program are different from those who do not. They will vary in terms of what we can observe (e.g. education, income) and in terms of what we cannot observe (e.g. motivation, effort).\nIf we compare outcomes for those with and without the program, the difference will have two parts:\nThat are caused by the program That are caused by underlying differences between participants and non-participants Our estimate of impact will not, on average, be equal to the true impact of the program unless (2) is zero and there is no selection bias.\nComparing the outcome of people before and after the program risks the presence of other changes during that time. For example, did the program increase their spending, or did their spending increase as it is approaching Christmas?",
    "crumbs": [
      "Experimental foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Randomised trials</span>"
    ]
  },
  {
    "objectID": "experimental-foundations/how-can-randomisation-help.html",
    "href": "experimental-foundations/how-can-randomisation-help.html",
    "title": "5  How can randomisation help?",
    "section": "",
    "text": "Participants and non-participants are chosen at random.\nThere is no reason, other than chance, that they are selected into the program. On average, participants and non-participants have the same characteristics. They would, on average, have the same outcomes.\nAny difference at the end is due to the program (or chance, to which we will return).\n\nFigure from Haynes et al. (2012)\nRandomisation avoids the need for a detailed understanding of the mechanism underlying the difference in outcomes between groups. James Lind conducted what many considered to be the first clinical trial in 1747 when he gave six scurvy stricken sailors citrus juice, while denying the treatment to another six. He did not need to know that vitamin C was the mechanism, nor anything about human biology to see if it worked.\nQuestion\nA firm provides well-being training to all employees interested. At the end of the program, employees who participated in the program had lower stress levels than those who did not participate in the program.\nCan you conclude from these data that the program was effective in reducing stress?\n\n Yes because those who participated had lower stress level. No, because employees who participated may have had lower stress level before the program already.\n\n\n\nClick here to see an explanation\n\nAs you did not randomise and control access, employees who participated may have had lower stress levels before the program.\n\n\n\n\n\nHaynes, L., Service, O., Goldacre, B., and Torgerson, D. (2012). Test, Learn, Adapt: Developing Public Policy with Randomised Controlled Trials. https://www.gov.uk/government/publications/test-learn-adapt-developing-public-policy-with-randomised-controlled-trials",
    "crumbs": [
      "Experimental foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>How can randomisation help?</span>"
    ]
  },
  {
    "objectID": "experimental-foundations/lab-versus-field.html",
    "href": "experimental-foundations/lab-versus-field.html",
    "title": "6  Lab versus field",
    "section": "",
    "text": "A common claim against laboratory experiments is that they do not enable conclusions to be drawn about the “real world” Those critiques tend to rest on three foundations:\n\nLab experiments are conducted in a “sterile environment” with artificial surroundings and tasks.\nLab experiments often do not use realistic commodities or stakes. They often involve abstract rewards of materially smaller amounts than may be at stake in the outside world.\nLab experiments don’t use “real people”. We are not interested primarily in the behaviour of students.\n\nThese three critiques mean that lab experiments can have limited relevance for predicting field behaviour unless the aspects of behaviour being studied are general across environments, stakes and subject pools.\nField experiment, by contrast:\n\nInvolve the subject pool that is of interest (i.e. not undergraduates)\nCan actually be cheaper than lab experiments in that it is easier to get large samples in the field\nMay be better environment for testing size of change. The sterility of the lab often means that the scale of the observed phenomena is not a reliable indicator of the expected external change.\nUse realistic stakes.\n\nLab experiments are not, however, without their advantages:\n\nLab experiments can give experimenters more control. In the lab, it is easier to give strict instructions that are then followed.\nLab experiments can also give more transparency about the subject pool (even if they are undergraduate students or workers on Amazon Turk). New subject pools may have idiosyncrasies beyond their field relevance.\nLab experiments are typically more replicable. They are easier and cheap(er) to replicate. This can provide comfort against surprising results.\nLab experiments are often simple more feasibile. It is often not possible to experiment in half the market.\n\nThe result of this balance of costs and benefits means that lab and field experiments should be seen as being methodologically complementary. Each can enable insights that the other can’t. Experiments in one can be used to inform the other.\nWe will return to the question of lab versus field experiments in more detail later when we explore the generalisability of experiments.",
    "crumbs": [
      "Experimental foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Lab versus field</span>"
    ]
  },
  {
    "objectID": "experimental-foundations/types-of-experiments.html",
    "href": "experimental-foundations/types-of-experiments.html",
    "title": "7  Types of experiments",
    "section": "",
    "text": "7.1 Criteria that define a field experiment\nOne major experimental consideration is whether to experiment in the lab or the field.\nHarrison and List (2004) described six criteria that can be used to delineate between lab and field experiments:\nThe nature of the subject pool: Is the subject pool made up of standard subjects (e.g. students), a more representative subject pool, or a target population? Decision making may vary across pools.\nInformation the subjects bring to the task: Do participants being with them experience in the commodity or task that forms the basis of the experiment? Decisions may vary with the experience brought to the task.\nThe nature of the commodity: Does the experiment use actual rather than abstractly defined goods? The artificiality of abstract goods can affect behaviour.\nThe nature of the task or trading rules applied: Is the task the same that is naturally undertaken in the field? As field experience can enable the development of task specific heuristics, variation in the task can shift decision making.\nThe nature of the stakes: Are the stakes in commensurate to that in the field? Stakes in the laboratory and field can vary markedly, which might be the difference between indifference and engagement.\nThe nature of the environment that the subject operates in: Is the environment the one in which the decision naturally occurs? Different settings might engender role playing, or suggest particular strategies or heuristics to use in making decisions.",
    "crumbs": [
      "Experimental foundations",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Types of experiments</span>"
    ]
  },
  {
    "objectID": "experimental-foundations/types-of-experiments.html#a-taxonomy-of-experiments",
    "href": "experimental-foundations/types-of-experiments.html#a-taxonomy-of-experiments",
    "title": "7  Types of experiments",
    "section": "7.2 A taxonomy of experiments",
    "text": "7.2 A taxonomy of experiments\nThis criteria can be used to develop a more refined taxonomy of experiments than simply “lab” or “field”, although this is a spectrum rather than a precise taxonomy. Harrison and List developed a spectrum as follows:\nConventional lab experiments: Lab experiments typically use a standard subject pool of students (cheap and convenient), an abstract framing, and an imposed set of rules.\nArtefactual field experiments: Artefactual experiments mimic lab experiments, except that they use non-standard subjects such as people from the market or context of interest.\nAs one example, Michael Haigh and John List got traders from the Chicago Board of Trade to play games to test whether they exhibit myopic loss aversion. (You might recall from 23713 Behavioural economics and corporate decision making that myopic loss aversion was Thaler and Benartzi’s explanation for the equity premium puzzle.) Would professional traders with experience trading make better decisions? Surprisingly, they found that the traders were even more myopically loss averse in a lab environment than the typical student subjects.\nAnother example is work by Joe Henrich and colleagues, who recruited subjects from 15 small-scale societies to play the ultimatum game. (Recall that the ultimatum game involves a proposer offering a split with a respondent. If the respondent accepts, they keep the split. If they reject, they both get nothing.) These subjects came from three foraging societies, six societies that practice slash-and-burn horticulture, four nomadic herding groups, and three sedentary agriculturalist societies.\nAlthough no group exhibited the optimal game theoretic solution (offer the smallest possible non-zero sum, accept), Henrich and colleagues found that there was material behavioural variability across the groups. No group offered, on average, less than 25%. But two groups had average initial offers over 50%. Rejection rates also varied materially. Henrich and colleagues argued that economic organisation and the degree of market integration explained a substantial portion of this variation.\nFramed field experiment: Framed field experiments incorporate elements of the naturally-occurring environment, such as the commodity, tasks, stakes and information sets of the subjects. Subjects understand they are taking part in an experiment and that their behaviour is recorded and scrutinised.\nAs an example framed field experiment, John List tested expected utility and prospect theory at a sportscard show. He endowed experienced traders with a mug or bar of chocolate and give them an opportunity to trade (as per the famous experiments reported by Daniel Kahneman, Jack Knetsch and Richard Thaler). List found that, among inexperienced consumers, prospect theory adequately explained their behaviour. However, those with intense market experience behaved in accordance with neoclassical predictions.\nNatural field experiments: Natural field experiments are similar to framed field experiments except that the environment is one where the subjects naturally undertake these tasks and where the subjects do not know that they are in an experiment.\nNatural field experiments are different from “natural experiments”, a subject you will look at in depth in Principles of Causal Inference. Natural field experiments use treatments constructed by the experimenter to test a hypothesis. Natural experiments use naturally created randomness across treatments to draw conclusions from naturally occurring data. The experimenter has less (usually no) control in natural experiments. Field experiments might be seen as providing a bridge between lab experiments and naturally occurring data, giving mixture of realism and control that you can’t achieve otherwise.\nYou came across a natural field experiment in a previous unit where we saw Marianne Bertrand and Sendhil Mullainathan’s tests of labour market discrimination. They sent manipulated CVs tin response to ads in Boston and Chicago, finding that white names received 50% more callbacks for interviews than African-American soundings names.\n\n\n\n\nHarrison, G. W., and List, J. A. (2004). Field Experiments. Journal of Economic Literature, 47.",
    "crumbs": [
      "Experimental foundations",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Types of experiments</span>"
    ]
  },
  {
    "objectID": "experimental-foundations/control.html",
    "href": "experimental-foundations/control.html",
    "title": "8  Control",
    "section": "",
    "text": "8.1 Direct experimental control\nA primary requirement of a good experiment is that it tests or controls for alternative hypotheses. If you were to test the success of an intervention to increase savings in December without controlling for the possibility that expenditure tends to increase in the lead up to Christmas, you are going to have a poor experiment.\nThere are two ways in which we achieve control: directly and indirectly. These are not either/or options. Rather, we tend to use both direct and indirect control together in an experiment.\nIn a lab experiment, you can directly control many variables. You choose which to keep constant across the experimental participants, such as the rules of the game that they play or their initial endowment. Variables held constant are control variables.\nYou also choose which variables you will vary. Variables that are changed are called treatment variables. If you want to to test the effect of a text message to some people and not others, that controlled variation is your experimental treatment.\nTypically you will test hypotheses or behavioural interventions by changing one variable at a time. You only change variables which are directly relevant to the hypothesis being tested, otherwise holding the environment fixed. This can help to avoid confounds.\nIn the field, you control fewer variables, although you maintain direct control of the treatment effects.",
    "crumbs": [
      "Experimental foundations",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Control</span>"
    ]
  },
  {
    "objectID": "experimental-foundations/control.html#indirect-experimental-control",
    "href": "experimental-foundations/control.html#indirect-experimental-control",
    "title": "8  Control",
    "section": "8.2 Indirect experimental control",
    "text": "8.2 Indirect experimental control\nMany variables are difficult to control directly, particularly in the field. For our advertising example above, it’s hard to cancel Christmas. You might think that you could compare sales year-on-year, but is this Christmas the same as the last (a salient problem this year!). More subtlety, the customers who will see your intervention may have different propensities to save that those who don’t. You can’t directly control that.\nYou can measure variables which you think may affect the propensity to save: gender, age, income, etc, and adjust the analysis after the fact. But there will be more factors than you capture, and likely some important ones that you don’t even realise are relevant. These uncontrolled factors will ultimately undermine your experimental conclusions.\nThere is, however, an indirect way to achieve control: randomisation. By randomly assigning experimental participants to different treatments, we can eliminate differences between the subjects as a cause of differences between treatments.",
    "crumbs": [
      "Experimental foundations",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Control</span>"
    ]
  },
  {
    "objectID": "experimental-foundations/are-randomised-controlled-trials-ethical.html",
    "href": "experimental-foundations/are-randomised-controlled-trials-ethical.html",
    "title": "9  Are randomised controlled trials ethical?",
    "section": "",
    "text": "Randomized experiments—long the gold standard in medicine—are increasingly used throughout the social sciences and professions to evaluate business products and services, government programs, education and health policies, and global aid. We find robust evidence—across 16 studies of 5,873 participants from three populations spanning nine domains—that people often approve of untested policies or treatments (A or B) being universally implemented but disapprove of randomized experiments (A/B tests) to determine which of those policies or treatments is superior. This effect persists even when there is no reason to prefer A to B and even when recipients are treated unequally and randomly in all conditions (A, B, and A/B). This experimentation aversion may be an important barrier to evidence-based practice.\nMeyer et al. (2019)\n\nIs it ethical to withhold an intervention that may benefit someone by assigning them to a control group?\nA common response to this question is that this effectively already occurs in many instances without trials:\n\nInterventions are often piloted, which is equivalent to excluding a group of people who could benefit.\nInterventions are often scaled up, meaning that it takes time for everyone to receive the intervention.\n\nA randomised controlled trial might be considered more ethical than either of those scenarios as it is similarly a phased introduction, but with a mechanism to determine its effectiveness and improve future outcomes. Absent a robust test, we need to be clear about the limits of our knowledge. There are many cases where an intervention assumed to be helpful was later found to be ineffective or harmful.\nFurther, trials often have protocols that in the case of large early effects indicating success or harm, they can be ceased or implemented more rapidly.\nAnother related question is whether it is fair to experiment on people at all. Don’t we risk harm?\nOne response is that every time you roll out a new program, product, communication or tool that may change behaviour or affect their wellbeing, you are running an experiment. It’s just that if you’re doing it absent a control group or some other mechanism to determine effectiveness, your experiment does not even have the benefit of enabling you to know whether it works, or is helping or harming people.\n\n\n\n\nMeyer, M. N., Heck, P. R., Holtzman, G. S., Anderson, S. M., Cai, W., Watts, D. J., and Chabris, C. F. (2019). Objecting to experiments that compare two unobjectionable policies or treatments. Proceedings of the National Academy of Sciences, 116(22), 10723–10728. https://doi.org/10.1073/pnas.1820701116",
    "crumbs": [
      "Experimental foundations",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Are randomised controlled trials ethical?</span>"
    ]
  },
  {
    "objectID": "before-the-trial/before-the-trial.html",
    "href": "before-the-trial/before-the-trial.html",
    "title": "Before the trial",
    "section": "",
    "text": "In this part, I cover some steps you will take before trial design.\n\nWhat is the problem you are trying to solve?\nWhat behaviour is leading to the outcome?\nWhat is our theory of the current behaviour?\nWhat interventions might influence the behaviour?\n\nBy the end of this module, you will be able to answer the following questions:\n\nHow might you get buy-in for your trial?\nHow do you define and diagnose a behavioural problem?\nHow do you design or select an intervention?",
    "crumbs": [
      "Before the trial"
    ]
  },
  {
    "objectID": "before-the-trial/organisational-buy-in.html",
    "href": "before-the-trial/organisational-buy-in.html",
    "title": "10  Organisational buy-in",
    "section": "",
    "text": "Before a trial, there is a need to get the organisation to buy into the trial and execute the experiment. Many proposed experiments have been scuttled by a failure to accept experimentation as a way of determining what works, by running into organisational barriers or by changes in priorities.\nJohn List is a pioneer of the use of field experiments in economics. We have seen some of his experiments in this and previous units. He has run experiments involving airline pilots, traders, fisherman, card collectors and CEOs. He has even started his own company to test ideas around hiring and job performance.\nIn an article containing tips on how to run a field experiment (List, 2011), he offered the following ideas:\nHave a champion within the organization — the higher up the better: Make the experiment a “we” project instead of an “us versus them” pursuit. Senior champions can be the catalyst for others in the organisation to help.\nUnderstand organizational dynamics: If someone is hampering your efforts, turn this person to your side. Insiders can always find a way to stop your field experiment.\nOrganizations that have “skin in the game” are more likely to execute your design and use your results to further organizational objectives: If the organization has invested resources, even sunk costs, the organisation is more likely to complete the project and use the results afterwards.\nRun the field experiment yesterday rather than tomorrow: If you have an open window, take advantage of it as the unexpected will often close it.\nChange the nature of the discussion of the cost of the experiment: Counter claims that the experiment will cost the firm too much money with a response that we are “costing” the firm too much money by not experimenting.\nMake clear that you do not have all the answers: An organization may not welcome an outsider who claims to arrive with all of the answers. Rather, say that you have the tools to discover them.\nBe open to running experiments that might not provide high-powered research findings in the short run: Get your foot in the door by conducting experiments that are not intellectually satisfying so you can run more intellectually interesting experiments in the future.\n\n\n\n\nList, J. A. (2011). Why Economists Should Conduct Field Experiments and 14 Tips for Pulling One Off. Journal of Economic Perspectives, 25(3), 3–16. https://doi.org/10.1257/jep.25.3.3",
    "crumbs": [
      "Before the trial",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Organisational buy-in</span>"
    ]
  },
  {
    "objectID": "before-the-trial/define-and-diagnose-the-problem.html",
    "href": "before-the-trial/define-and-diagnose-the-problem.html",
    "title": "11  Define and diagnose the problem",
    "section": "",
    "text": "Before getting to the trial, you will typically have already done much work. Primary among these is defining and diagnosing the problem.\nDefining and diagnosing the problem is a critical step in good intervention design. This is not the focus of this subject, but you need to clearly define the problem, where you should focus your attention, what particular outcomes you care about the most, and who your target population is. You must also be mindful of your resources and consider what is realistically achievable. We must have a thorough understanding of the context. What is the reality on the ground? What problems and opportunities are the targeted people facing?\nDescriptive methodologies can be very powerful. They are the basis of good intervention design. The understanding of the context that they provide helps ensure that an intervention is designed to solve a problem that exists in a specific context, matters to the targeted people, and fits their constraints and opportunities.\nOne methodology for defining and diagnosing the problem is a needs assessment.\nNeeds assessments aim to generate a description of the problem and existing solutions using interviews and focus groups, existing data, or new surveys. Needs assessments can also help pinpoint weaknesses in existing programs.\nThe types of questions needs assessments can answer include determining what groups should be targeted by the program (for example, what groups have the greatest need or would benefit the most) and what are their problems and opportunities. It is also important to understand the possible reasons for the problems they face and what they are already doing to resolve these problems. Finally, it is essential to understand whether existing programs are already focusing on these underlying needs and what challenges remain unaddressed.\nTo see how this works with a concrete example, let’s look at the questions that Glennerster and Takavarasha (2013) seek to answer with a needs assessment before developing a new educational program for primary school.\n\nIf we are planning an education program (or an evaluation) for primary schools, we need to have a good understanding of the level of learning among primary-age children in the area.\nWho is falling behind, and who is not? What are the possible reasons that learning levels are low? What are the child absenteeism rates? What are the teacher absenteeism rates? (If children who attend regularly are learning well, child absenteeism could be the problem.) Or are children who attend regularly still falling behind? What do parents and children say about why they do not attend and why they find it hard to learn? What do teachers say about why learning levels are low? How are classes conducted? What material is covered? Is it appropriate to the level of learning of most children? Are children sick or hungry? Can they see and hear what is going on? We also need to know if other providers are working in the area. What are they doing?\n\nGlennerster and Takavarasha (2013)\nA range of methodologies can be used to conduct a needs assessment. Focus groups can tell us that people might find a program is located too far away or operates at inconvenient times; absenteeism surveys can show us whether program staff are regularly absent; take-up surveys can assess what fraction of potentially eligible participants are actually using a service, and random supply checks can make sure that supplies are getting through. A combination of both qualitative and quantitative techniques is useful in gaining a good understanding of the context.\nFurther references on defining and diagnosing the problem include:\n\nThe first two steps of BETA’s 4D Framework are Discover and Diagnose. BETA has also published a guidance note on Developing behavioural interventions for randomised controlled trials: Nine guiding questions, which provides more detail on undertaking the first two stages of the 4D framework.\nThe Behavioural Insights Team’s Target, Explore, Solution, Trial, Scale (TESTS) is a guide to running a simple behavioural insights project. The Explore process is particularly relevant. The Behavioural Insights Team’s Barrier Identification Tool allows you to identify the barriers to the desired behaviour. It is based on the COM-B model.\nThe OECD’s BASIC Framework and its first phases, Behaviour and Analysis, provide another lens for identifying and better understanding your problem, and reviewing the evidence to identify the behavioural drivers of the problem.\n\n\n\n\n\nGlennerster, R., and Takavarasha, K. (2013). Asking the right questions. In Running randomized evaluations: A practical guide (pp. 66–97). Princeton University Press. https://www.jstor.org.ezproxy.lib.uts.edu.au/stable/j.ctt4cgd52.7",
    "crumbs": [
      "Before the trial",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Define and diagnose the problem</span>"
    ]
  },
  {
    "objectID": "before-the-trial/partial-compliance.html",
    "href": "before-the-trial/partial-compliance.html",
    "title": "12  Partial compliance",
    "section": "",
    "text": "12.1 References\nPartial compliance occurs when some people in the treatment group do not take up a program or others in the comparison group do.\nPartial compliance can occur when:\nNon-compliance can undermine the trial in the following ways:\nTo limit partial compliance, we can:\nTo account for partial compliance in the analysis, we have to document compliance in the treatment and comparison groups in the same way.\nGlennerster, R., & Takavarasha, K. (2013) “Threats” in Running Randomized Evaluations: A Practical Guide (pp. 298-323). Princeton University Press",
    "crumbs": [
      "Before the trial",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Partial compliance</span>"
    ]
  },
  {
    "objectID": "before-the-trial/attrition.html",
    "href": "before-the-trial/attrition.html",
    "title": "13  Attrition",
    "section": "",
    "text": "13.1 References\nAttrition is the absence of data because the researchers are unable to collect some or all of the outcome measures from some people in the sample. This can occur when participants drop out or refuse to be interviewed. Attrition creates a problem of missing data.\nAttrition can reduce the comparability of treatment and comparison groups. Consider the following example:\nImagine that a program increased the test scores of low-achieving students from an average score of 10 to an average score of 15 (image below). Overall, the average score for the class increased from 15 to 17.5. And because the low-achieving students in the treatment group were given support, they did not drop out of school, as many of their peers in the comparison group did. However, if we had measured the test scores only of those children who stayed in school, we would have concluded that the program worsened test scores, from 18.3 in the comparison group to 17.5 in the treatment group.\nAttrition also reduces the sample size, lowering statistical power.\nWe can limit attrition by:\nGlennerster, R., & Takavarasha, K. (2013) “Threats” in Running Randomized Evaluations: A Practical Guide (pp. 298-323). Princeton University Press",
    "crumbs": [
      "Before the trial",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Attrition</span>"
    ]
  },
  {
    "objectID": "before-the-trial/spillovers.html",
    "href": "before-the-trial/spillovers.html",
    "title": "14  Spillovers",
    "section": "",
    "text": "14.1 References\nSpillovers occur when the effect on those receiving treatment produces a secondary effect on those not treated.\nIf there are positive (negative) spillovers, the estimated impact will be an underestimate (overestimate).\nSome types of spillovers include:\nSpillovers reduce the quality of the counterfactual. The comparison group is no longer as good a counterfactual because their outcomes reflect indirect program effects, not the outcomes in the absence of the program.\nSpillovers become a threat to the evaluation if we do not consider them in the design and/or analysis phase.\nIf the evaluation does not capture or account for positive spillovers from treatment to comparison, the impact of the program will be underestimated. If it does not capture or account for negative spillovers, the impact will be overestimated.\nWe can manage spillovers by:\nGlennerster, R., & Takavarasha, K. (2013) “Threats” in Running Randomized Evaluations: A Practical Guide (pp. 298-323). Princeton University Press",
    "crumbs": [
      "Before the trial",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Spillovers</span>"
    ]
  },
  {
    "objectID": "before-the-trial/evaluation-driven-effects.html",
    "href": "before-the-trial/evaluation-driven-effects.html",
    "title": "15  Evaluation-driven effects",
    "section": "",
    "text": "15.1 References\nBeing part of an evaluation can change the way people behave, independent of any impacts of the program.\nEvaluation-driven effects include:\nEvaluation effects can undermine power and generalizability. Evaluation-driven behaviour can lead to outcome changes that would not occur without the evaluation, which reduces the generalizability of the results.\nEvaluation-driven effects can undermine comparability. If the evaluation-driven behaviour is group specific (affecting only the treatment or only the comparison group), it undermines the comparability of the two groups.\nEvaluation-driven effects can bias impact estimates. Hawthorne effects and social desirability effects can inflate the estimated impact of a program compared to its true impact by artificially boosting outcomes among the treatment group. John Henry effects deflate the estimated impact of the program by artificially boosting outcomes in the comparison group.\nWe can limit evaluation-driven effects by:\nGlennerster, R., & Takavarasha, K. (2013) “Threats” in Running Randomized Evaluations: A Practical Guide (pp. 298-323). Princeton University Press",
    "crumbs": [
      "Before the trial",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Evaluation-driven effects</span>"
    ]
  },
  {
    "objectID": "before-the-trial/develop-the-intervention.html",
    "href": "before-the-trial/develop-the-intervention.html",
    "title": "16  Developing the intervention",
    "section": "",
    "text": "16.1 Program theory\nDesigning the intervention involves:\nA program theory refers to a variety of ways of developing a causal modal linking program inputs and activities to a chain of intended or observed outcomes, and then using this model to guide the evaluation (Rogers et al., 2000). It articulates how an intervention is expected to contribute to a chain of results that produce the intended or actual impacts. It can include positive impacts (which are beneficial) and negative impacts (which are detrimental). It can also show the other factors which contribute to producing impacts, such as context and other projects.\nSpeculating on different possible causal mechanisms through a program theory is useful to provide a conceptual framework to understand to what extent, for whom, and why an intervention does or does not work. It is also helpful to determine which outcomes we should measure. If the program is not successful, having indicators for these intermediate steps helps us understand at which step in the chain the program failed. Although a single evaluation is limited in its scope, program theory makes it easier to combine evidence from a number of studies.\nDifferent types of diagrams can be used to represent a program theory. These are often referred to as logic models, as they show the overall logic of how the intervention is understood to work.",
    "crumbs": [
      "Before the trial",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Developing the intervention</span>"
    ]
  },
  {
    "objectID": "before-the-trial/develop-the-intervention.html#an-apple-a-day-keeps-the-doctor-away-or-does-it",
    "href": "before-the-trial/develop-the-intervention.html#an-apple-a-day-keeps-the-doctor-away-or-does-it",
    "title": "16  Developing the intervention",
    "section": "16.2 An apple a day keeps the doctor away — or does it?",
    "text": "16.2 An apple a day keeps the doctor away — or does it?\nLet’s look at the program example and discussion from Funnell and Rogers (2011). Suppose we have implemented a program, called An Apple A Day, which involves distributing seven apples a week to each participant with the broad objective of improving health.\n\n16.2.1 An evaluation without program theory\nA representation of this program without program theory would simply show the program followed by the intended outcome as shown in this figure. This figure only shows what goes in and what comes out without information about how things are processed in between. This is just a black box evaluation.\n\n\n\n16.2.2 Logic models showing different possible causal mechanisms\nThe following figure shows a program theory for An Apple A Day. It displays four different mechanisms that might plausibly explain why the policy works.\n\n\n\n16.2.3 Using program theory to interpret evaluation findings\nThe table below summarises how an evaluation informed by program theory can distinguish among different types of success and failure. It focuses on the second mechanisms (improved level of Vitamin C) and can be done only if we have collected Vitamin C levels.",
    "crumbs": [
      "Before the trial",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Developing the intervention</span>"
    ]
  },
  {
    "objectID": "before-the-trial/develop-the-intervention.html#design-and-select-an-intervention",
    "href": "before-the-trial/develop-the-intervention.html#design-and-select-an-intervention",
    "title": "16  Developing the intervention",
    "section": "16.3 Design and select an intervention",
    "text": "16.3 Design and select an intervention\nAfter working through your program theory, you will typically have a shortlist of interventions that you could trial. The purpose of the trial is to differentiate between the effectiveness of those behavioural interventions. Are any of the interventions more effective in improving your outcome of interest?\nAn important part of the design and selection of interventions is made through ground and qualitative work.\nYou need to check whether the theory behind the intervention makes sense on the ground. For example, suppose that you want to improve health outcomes in a low-income countries by providing health worker training. When you visit the health centres as part of your groundwork, you realise that the main issue is health worker absenteeism. It does not make sense to improve training if health workers are not at work. You probably need to go back to your program theory and think about ways to reduce absenteeism.\nYou can also use qualitative work to refine the design. For example, if you want to convey complex financial information as part of your intervention, you need to make sure that lay people can understand it without too much effort.\nHere are some other practical questions you need to ask in selecting interventions for trial.\nFirst, you want the choice of interventions to help answer an interesting question. You want to learn something. For example, has the intervention been found effective or ineffective in similar contexts before? Will you learn something new from trying it again? If text message reminders have been found consistently effective in similar scenarios, a test of text messages versus control of no message may not be informative. However, a trial varying the content of the message and the theoretical underpinning of that message might be.\nSecond, you want to select interventions that you can deliver consistently at scale. For the experiment, you want everyone within each group (control and treatments) to receive the same interventions as others in the group. But more importantly, what would happen if your trial was successful? What will it practically look like at scale relative to your perfect world conception of the intervention? If you have found an intervention to be highly effective, but it is not feasible to roll out at scale, your experiment is not useful.\nThe process of selecting interventions for an applied behavioural science trial is usually conceptually simpler than developing interventions for an academic experiment designed to test a theory. In that case, it is important that there is not a confounding theory that gives an equally plausible rationale for the behaviour observed in the experiment. The experimenter needs to understand the most plausible hypotheses that should be controlled for, which might depend on recent developments in theory. An experiment may only appear good at the time, as subsequent work may expose its theoretical flaws.\nBut that is not to say that you shouldn’t think about the conceptual and theoretical underpinnings of your interventions in an applied behavioural science trial. There will typically be a theoretical basis for your choice of interventions. That basis will help determine what you should measure. It will inform the interpretation of your results. It can provide the foundation for you to take those interventions into other contexts.\n\n\n\n\nFunnell, S. C., and Rogers, P. J. (2011). The essence of program theory. Jossey-Bass. https://www.wiley.com/en-au/Purposeful+Program+Theory%3A+Effective+Use+of+Theories+of+Change+and+Logic+Models-p-9780470478578\n\n\nRogers, P. J., Petrosino, A., Huebner, T. A., and Hacsi, T. A. (2000). Program theory evaluation: Practice, promise, and problems. New Directions for Evaluation, 2000(87), 5–13. https://doi.org/10.1002/ev.1177",
    "crumbs": [
      "Before the trial",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Developing the intervention</span>"
    ]
  },
  {
    "objectID": "before-the-trial/program-theory.html",
    "href": "before-the-trial/program-theory.html",
    "title": "17  Program theory",
    "section": "",
    "text": "17.1 An apple a day keeps the doctor away — or does it?\nA program theory refers to a variety of ways of developing a causal modal linking program inputs and activities to a chain of intended or observed outcomes, and then using this model to guide the evaluation (Rogers et al., 2000). It articulates how an intervention is expected to contribute to a chain of results that produce the intended or actual impacts. It can include positive impacts (which are beneficial) and negative impacts (which are detrimental). It can also show the other factors which contribute to producing impacts, such as context and other projects.\nSpeculating on different possible causal mechanisms through a program theory is useful to provide a conceptual framework to understand to what extent, for whom, and why an intervention does or does not work. It is also helpful to determine which outcomes we should measure. If the program is not successful, having indicators for these intermediate steps helps us understand at which step in the chain the program failed. Although a single evaluation is limited in its scope, program theory makes it easier to combine evidence from a number of studies.\nDifferent types of diagrams can be used to represent a program theory. These are often referred to as logic models, as they show the overall logic of how the intervention is understood to work.\nLet’s look at the program example and discussion from Funnell and Rogers (2011). Suppose we have implemented a program, called An Apple A Day, which involves distributing seven apples a week to each participant with the broad objective of improving health.",
    "crumbs": [
      "Before the trial",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Program theory</span>"
    ]
  },
  {
    "objectID": "before-the-trial/program-theory.html#an-apple-a-day-keeps-the-doctor-away-or-does-it",
    "href": "before-the-trial/program-theory.html#an-apple-a-day-keeps-the-doctor-away-or-does-it",
    "title": "17  Program theory",
    "section": "",
    "text": "17.1.1 An evaluation without program theory\nA representation of this program without program theory would simply show the program followed by the intended outcome as shown in the figure below. This figure only shows what goes in and what comes out without information about how things are processed in between. This is just a black box evaluation.\n\n\nBlack box evaluation showing program leading to outcomes\n\n\n\n\n\n17.1.2 Logic models showing different possible causal mechanisms\nThe following figure shows a program theory for An Apple A Day. It displays four different mechanisms that might plausibly explain why the policy works.\n\n\nFour different possible causal mechanisms for An Apple A Day\n\n\n\nThe table below summarises how an evaluation informed by program theory can distinguish among different types of success and failure. It focuses on the second mechanisms (improved level of Vitamin C) and can be done only if we have collected Vitamin C levels.\n\n\n17.1.3 Using program theory to interpret evaluation findings\n\n\n\n\n\n\n\n\n\n\napples delivered\napples eaten\nvitamin C levels raised\nhealth outcomes improved\ninterpretation\n\n\n\n\nno\nno\nno\nno\nimplementation failure\n\n\nyes\nno\nno\nno\nengagement or adherence failure (first causal link)\n\n\nyes\nyes\nno\nno\ntheory failure (early causal link)\n\n\nyes\nyes\nyes\nno\ntheory failure (later causal link)\n\n\nyes\nyes\nyes\nyes\nconsistent with theory\n\n\nyes\nyes\nmaybe\nmaybe\npartial theory failure (works in some contexts)\n\n\nyes\nyes\nno\nyes\ntheory failure (different causal path)\n\n\n\nDiagrams and table adapted from: Funnell, S. C., & Rogers, P. J. (2011). The essence of program theory. Purposeful program theory: effective use of theories of change and logic models (1st ed., pp. 3–13). Jossey-Bass.\n\n\n\n\n\n\nTipActivity\n\n\n\nCan you think about other mechanisms through which providing an apple a day to people will improve their health to augment the program theory?",
    "crumbs": [
      "Before the trial",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Program theory</span>"
    ]
  },
  {
    "objectID": "before-the-trial/program-theory.html#references",
    "href": "before-the-trial/program-theory.html#references",
    "title": "17  Program theory",
    "section": "17.2 References",
    "text": "17.2 References\nGlennerster, R., & Takavarasha, K. (2013). Outcomes and Instruments. Running Randomized Evaluations: A Practical Guide (pp. 180–240). Princeton University Press.\nRogers, P. J., Petrosino, A., Huebner, T. A., & Hacsi, T. A. (2000). Program theory evaluation: Practice, promise, and problems. New Directions for Evaluation, 2000(87), 5–13.\nFunnell, S. C., & Rogers, P. J. (2011). The essence of program theory. Purposeful program theory: effective use of theories of change and logic models (1st ed., pp. 3–13). Jossey-Bass.",
    "crumbs": [
      "Before the trial",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Program theory</span>"
    ]
  },
  {
    "objectID": "before-the-trial/design-and-select-an-intervention.html",
    "href": "before-the-trial/design-and-select-an-intervention.html",
    "title": "18  Design and select an intervention",
    "section": "",
    "text": "After working through your program theory, you will typically have a shortlist of interventions that you could trial. The purpose of the trial is to differentiate between the effectiveness of those behavioural interventions. Are any of the interventions more effective in improving your outcome of interest?\nAn important part of the design and selection of interventions is made through ground and qualitative work.\nYou need to check whether the theory behind the intervention makes sense on the ground. For example, suppose that you want to improve health outcomes in a low-income countries by providing health worker training. When you visit the health centres as part of your groundwork, you realise that the main issue is health worker absenteeism. It does not make sense to improve training if health workers are not at work. You probably need to go back to your program theory and think about ways to reduce absenteeism.\nYou can also use qualitative work to refine the design. For example, if you want to convey complex financial information as part of your intervention, you need to make sure that lay people can understand it without too much effort.\nHere are some other practical questions you need to ask in selecting interventions for trial.\nFirst, you want the choice of interventions to help answer an interesting question. You want to learn something. For example, has the intervention been found effective or ineffective in similar contexts before? Will you learn something new from trying it again? If text message reminders have been found consistently effective in similar scenarios, a test of text messages versus control of no message may not be informative. However, a trial varying the content of the message and the theoretical underpinning of that message might be.\nSecond, you want to select interventions that you can deliver consistently at scale. For the experiment, you want everyone within each group (control and treatments) to receive the same interventions as others in the group. But more importantly, what would happen if your trial was successful? What will it practically look like at scale relative to your perfect world conception of the intervention? If you have found an intervention to be highly effective, but it is not feasible to roll out at scale, your experiment is not useful.\nThe process of selecting interventions for an applied behavioural science trial is usually conceptually simpler than developing interventions for an academic experiment designed to test a theory. In that case, it is important that there is not a confounding theory that gives an equally plausible rationale for the behaviour observed in the experiment. The experimenter needs to understand the most plausible hypotheses that should be controlled for, which might depend on recent developments in theory. An experiment may only appear good at the time, as subsequent work may expose its theoretical flaws.\nBut that is not to say that you shouldn’t think about the conceptual and theoretical underpinnings of your interventions in an applied behavioural science trial. There will typically be a theoretical basis for your choice of interventions. That basis will help determine what you should measure. It will inform the interpretation of your results. It can provide the foundation for you to take those interventions into other contexts.",
    "crumbs": [
      "Before the trial",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Design and select an intervention</span>"
    ]
  },
  {
    "objectID": "running-a-trial/running-a-trial.html",
    "href": "running-a-trial/running-a-trial.html",
    "title": "Running a trial",
    "section": "",
    "text": "In this part, I will cover the steps to design and run a trial. I will largely going to consider trials for public policy or business applications rather than for academic purposes. Many of the principles hold across academic, business and policy settings (in fact, they are often the same, as a trial may involve both an academic and a business or policy question).\nApplied behavioural science projects are typically designed to change behaviour (or improve an outcome caused by a certain behaviour). The trial is one step of that project.\nBy the end of this module, you will be able to answer the following questions:\n\nWhat are the steps in a randomised trial?\nHow can you articulate the causal links between a program and the outcomes?\nHow do you choose outcomes and indicators to measure the success of a trial?",
    "crumbs": [
      "Running a trial"
    ]
  },
  {
    "objectID": "running-a-trial/steps-in-a-randomised-trial.html",
    "href": "running-a-trial/steps-in-a-randomised-trial.html",
    "title": "19  Steps in a randomised trial",
    "section": "",
    "text": "You have designed a behavioural intervention. Here are the steps to test it in a randomised trial:\n\nDefine the outcome measure: What outcomes will be used to measure impact?\nRandom assignment: You need to allocate participants into a treatment and a control group.\nData collection plan: Collecting data is complex and you need a detailed plan on how this will take place.\nPilot: Although not essential, it is good practice to pilot your intervention and survey instrument before rolling them at scale.\nTrial Implementation: It can be useful to put in place a process evaluation to know whether the program is being implemented as planned and how well it is functioning.\nData analysis: After you have implemented the intervention and completed your data collection, it is time to analyse the data to assess whether it achieved its core objectives.\n\nWe’ll discuss these steps in more detail over the following pages.",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Steps in a randomised trial</span>"
    ]
  },
  {
    "objectID": "running-a-trial/define-the-outcome-measure.html",
    "href": "running-a-trial/define-the-outcome-measure.html",
    "title": "20  Define the outcome measure",
    "section": "",
    "text": "20.1 Observable signals of changes\nAn important part of the trial design is to define observable signals of changes.\nImpact is the changes our program aims to achieve. Often these changes are conceptual, such as “financial wellbeing.”\nOutcomes are observable signals of these changes, such as “credit card balance” or “ability to read a simple paragraph.” Indicators measure whether the impact is in fact happening. If an evaluation is to be influential in broader policy debates, it is useful to include outcome measures and indicators that are commonly used in the literature for assessing programs with similar objectives.\nAn outcome measure must be:\nJargon used in data collection",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Define the outcome measure</span>"
    ]
  },
  {
    "objectID": "running-a-trial/define-the-outcome-measure.html#observable-signals-of-changes",
    "href": "running-a-trial/define-the-outcome-measure.html#observable-signals-of-changes",
    "title": "20  Define the outcome measure",
    "section": "",
    "text": "Observable: it must be a behavior or state that can be observed in the real world. Happiness is not an observable indicator, but self-reported happiness is.\nFeasible: it must be feasible to measure politically, ethically and financially.\nDetectable: it must be detectable with your instruments and statistical power (to which we will come back in Module 3) of your experiments.\n\n\n\n\n\n\n\n\n\n\nTerm\nDefinition\nExamples\n\n\n\n\nImpact\nA change caused by the program we are evaluating\nIncrease or decrease in women’s empowerment, child health, corruption\n\n\nOutcome\nAn observable signal used to measure impact\nThe number of women who spoke at a meeting, child’s arm circumference\n\n\nInstrument\nThe tool we use to measure the outcomes\nA survey question, achievement test, direct observation record\n\n\nVariable\nThe numerical values of the outcomes\n\n\n\n\n\n20.1.1 When designing instruments don’t forget social-desirability bias\nWhen designing your instrument, you need to think about the possibility of social-desirability bias. It is a form of response bias in which participants prefer to answer questions in a way that will be viewed favourably by others instead of reporting the truth. It can take the form of over-reporting “good behaviour” or under-reporting “bad”, or undesirable behaviour. Social-desirability bias is more likely to occur when the topic of the question is a sensitive one. Potential ways to reduce social-desirability bias includes (i) using computer-assisted interviews instead of face-to-face or phone interviews, (ii) providing financial incentives for reporting the truth, (iii) using proxy indicators (e.g. childbearing instead of unprotected sex) or (iv) conducting a list experiment.\nVoting is typically a sensitive topic. During the U.S. 2016 presidential campaign featuring Hillary Clinton (Democratic candidate) against Donald J. Trump (Republican candidate), pre-election polls showed Clinton leading Trump in the battleground states that decided the presidency. There are several reasons that may explain why they got it wrong. One of them may be social-desirability bias. Brownback and Novotny (2018) conducted three list experiments to test this and found evidence that explicit polling overstates agreement with Clinton relative to Trump.\n\n20.1.1.1 What is a list experiment?\nSuppose you are trying to gauge support for a political position, but responses are clouded by social desirability bias.\nOne approach to overcoming social desirability bias is to use a list experiment. In a list experiment, the sample is randomly divided into two groups. The first group is presented with a list of neutral and non-sensitive items (length=N). The second group is presented with an identical list plus the sensitive item (length=N+1). They are then asked how many of the items they agree with.\nFor example, the first group might be asked: How many of the following statements do you agree with?\n\nIce cream is better than bacon.\nSummer is the best season.\nBicycle riders should not have to wear helmets.\nAFL is the most Australian sport.\n\nThe second group would be shown the same list, plus an additional item, such as:\n\nI will vote for Donald Trump in the next Presidential election.\n\nNote that the group members are not asked which items they agree with. They are only asked how many. This gives the respondent a degree of privacy. A researcher could not conclusively determine whether a person would vote for Donald Trump unless the person answered 0 or N+1.\nWith a large enough sample, researchers can estimate the proportion of people to whom the sensitive item pertains, in this case, voting for Trump, by subtracting the average number of agreements of the first group from the average number of agreements of the second group. For example, if the average number of agreements is 2.6 in the first and 3.2 in the second, the average number of agreements with the Trump statement is 0.6. This can be done as the responses to the non-sensitive questions across the two groups should, with large enough sample, be equal.\n\n\n\n20.1.2 Case study: An HIV education program in Kenya\nThis case study taken from Glennerster and Takavarasha (2013)\nillustrate a two-step process for selecting outcomes and instruments during program evaluation: (1) mapping the theory of change and (2) determining indicators that are the logical consequences of each step in the theory of change.\n\nA program in Kenya provided in-service training for teachers to improve their delivery of HIV prevention education in primary schools.\nThe training focused on how to teach HIV/AIDS prevention best practices while teaching other subjects and how to start and run after school student health clubs devoted to HIV/AIDS education.\nA simple theory of change for the program says that teacher training (1) increased HIV education, which (2) increased knowledge of prevention best practices, which (3) reduced unsafe sexual behavior, which (4) led to reduced HIV infection rates.\nHIV status is the ultimate outcome of interest. If we can measure HIV status in both our treatment and our comparison groups, we can know whether our program changed HIV infection rates. Although testing HIV status is common in the literature, it may be too expensive as well as ethically, politically, or logistically infeasible in our context. If this is the case, we can use other indicators as proxies for HIV rates, such as self-reported sexual behavior or childbearing.\nWe may also be interested in more proximate outcomes, such as changes in knowledge, that are not captured by HIV status. Indeed, HIV status is not a valid measure of knowledge of best practices, because someone may know the best prevention methods but choose to not practice them. Measuring these intermediate outputs and outcomes would allow us to learn about the underlying mechanisms that can translate program inputs into impacts. For example, if the program failed to achieve the desired impact, did it fail because it did not change knowledge or because changing knowledge did not change behaviour?\n\nThe following table shows the program theory. For each of the concepts in the causal chain, it also suggests potential indicator in the real world that we can observe and measure.\nLogical framework for the HIV education program.\n\n\n\n\n\n\n\n\n\nInput, Outputs/Outcome\nObjectives hierarchy\nIndicators\nAssumptions or threats\n\n\n\n\nInputs (activities)\nTeachers are trained to provide HIV education\nHours of training implemented\nTeachers engage with the training and learn new concepts\n\n\nOutputs\nTeachers increase and improve HIV education\nHours of HIV education given; application of the program’s teaching methods\nTeachers are receptive to changing their HIV education approach, and outside pressures do not prevent them from implementing a new curriculum\n\n\nOutcome (project objective)\nStudents learn prevention best practices\nTest scores on HIV education exam\nStudents engage with the new curriculum, and understand and retain the new concepts\n\n\nImpact (goal, overall objective)\nStudents reduce their unprotected sexual behaviour; incidence of HIV infection decreases\nSelf-reported sexual behaviour; number of girls who have started child-bearing; HIV status\nThe information students learn changes their beliefs and their behaviour. Students can make decisions and act on their preferences for engaging in sexual behaviour",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Define the outcome measure</span>"
    ]
  },
  {
    "objectID": "running-a-trial/define-the-outcome-measure.html#level-of-outcome",
    "href": "running-a-trial/define-the-outcome-measure.html#level-of-outcome",
    "title": "20  Define the outcome measure",
    "section": "20.2 Level of outcome",
    "text": "20.2 Level of outcome\nWhat outcome do you want for you, your customers, your employees or citizens?\nConsider a trial to test whether text messages containing one of several behaviourally-informed messages, such as a social norm and a loss frame, can increase the level of repayment of credit card debt.\nThe level of repayment of the credit card debt is one outcome. But what of:\n\nThe change in credit card balance over time?\nThe change in all debt balances over time (credit cards or otherwise)?\nThe allocation of those debt balances over different financial instruments (such as shifts to payday lenders)?\nThe change in their net financial position (considering both debt and savings)?\nTheir subjective financial wellbeing?\n\nWhich of these outcomes are most interested in? This likely would have been asked in the “define” stage of the project that led to the trial, but it is easy to forget broader objectives once a trial is designed. They need to be kept in mind, particularly for the purposes of measurement.\n\n20.2.1 Measurement\nWhen designing a trial, whether you can measure an outcome is almost as important as what outcome you are interested in. The outcome variable for the experiment needs to be measurable.\nConsider the credit card example above. If you are the financial institution that issued the credit card you can likely measure both the change in repayments and the change in the credit card balance over time.\nIf you offer a suite of financial products to customers, you may be able to monitor changes in balances of other savings and debt products for a subset of the customers. You may also be able to infer use of financial products from other providers by observing transactions. Together these can help you understand the broader distribution of debt balances and net financial position. However, this will be a partial picture and could be biased, particularly if those who hold many products with you have different characteristics than those who hold just a credit card.\nSubjective financial wellbeing, although being a broader measure, may actually be easier to obtain. You could add a survey component to your trial and obtain a direct measurement. The challenge there, however, is whether a minor intervention such as a text message can shift subjective financial wellbeing enough for you to detect it in a trial.\nBeyond outcomes, you might also want to measure process pieces. For example, if looking at debt on credit card, you might collect data on payment patterns. What day and time do they pay? How much each payment? What method of payment? Where did the payment come from? This could enable you to better understand how or why your intervention works and provide further ideas for testing.\nDue to practical requirements, measurement may simply involve analysis of the data you already collect. Many measures of interest are already collected, and if your experiment can capitalise on that, it may improve feasibility and reduce cost. However, this may also create an arbitrary break to the extent of the outcomes that you want to examine.\n\n\n20.2.2 Example: reducing power consumption\nPower companies often want to limit their customers’ electricity demand. This might be for environmental reasons or to reduce peak demand.\nOne method to achieve this is to give that person or household a comparison of their power consumption with that of their neighbours. People have a desire to conform, and look to cues to inform their decisions. If shown that their power usage is above their neighbours, they tend to reduce their use.\n\nThat is one possible outcome: reduced electricity usage. A related direct outcome is the financial saving through reduced power usage. Do they pay a smaller power bill?\nBut these are narrow outcomes. Broader questions could be asked.\nWhat was the net change in energy usage by the household? Was there substitution into gas or other energy sources? What is the emissions profile of these changes? What is the cost?\nMore broadly, what was the total change in household expenditure due to the intervention? Did they pay for energy savings appliances or fitouts to their house? What did they spend any energy bill savings on? How much time did they expend changing their energy usage?\nEven those questions might be seen as narrow. If the individual or household’s objectives were purely financial, there may be a success. They have saved on their power bill. Their reduction in use also aligns with the environmental or peak demand reduction objectives of the electricity provider. But what if their objective is satisfaction in life? Or comfort? Did they freeze during winter and swelter during summer to maintain their self-image? You have just compared them negatively with their neighbour. Did their happiness change when they saw that they compared poorly? Did it increase mental stress?\nMeasuring many of those outcomes are difficult. But that does not mean that they don’t matter.\nAllcott and Kessler (2019) sought a broader measure of the benefits of the comparisons by asking customers their “willingness to pay” for the home energy reports that contained comparisons of their energy use. This willingness to pay measure supported the argument that there was a net welfare gain from the the comparison. However, because the willingness to pay measures capture the broader costs and benefits of the energy reports beyond simple changes in energy usage, they found that the welfare gains were much smaller than assessed using narrow measures.\nPerhaps most interestingly, one third of the recipients would be willing to pay to not receive the report. For those customers, the psychological or other costs outweighed any information benefit.\n\n\n\n\nAllcott, H., and Kessler, J. B. (2019). The Welfare Effects of Nudges: A Case Study of Energy Use Social Comparisons. American Economic Journal: Applied Economics, 11(1), 236–276. https://doi.org/10.1257/app.20170328\n\n\nBrownback, A., and Novotny, A. (2018). Social desirability bias and polling errors in the 2016 presidential election. Journal of Behavioral and Experimental Economics, 74, 38–56. https://doi.org/10.1016/j.socec.2018.03.001\n\n\nGlennerster, R., and Takavarasha, K. (2013). Outcomes and instruments. In Running randomized evaluations: A practical guide (pp. 180–240). Princeton University Press. https://www.jstor.org/stable/j.ctt4cgd52.9",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Define the outcome measure</span>"
    ]
  },
  {
    "objectID": "running-a-trial/level-of-outcome.html",
    "href": "running-a-trial/level-of-outcome.html",
    "title": "21  Level of outcome",
    "section": "",
    "text": "21.1 Measurement\nConsider a trial to test whether text messages containing one of several behaviourally-informed messages, such as a social norm and a loss frame, can increase the level of repayment of credit card debt.\nThe level of repayment of the credit card debt is one outcome. But what of:\nWhich of these are you most interested in? This likely would have been asked in the “define” stage of the project that led to the trial, but it is easy to forget broader objectives once a trial is designed. They need to be kept in mind, particularly for the purposes of measurement.\nWhen designing a trial, whether you can measure an outcome is almost as important as what outcome you are interested in. The outcome variable for the experiment needs to be measurable.\nConsider the credit card example above. If you are the financial institution that issued the credit card you can likely measure both the change in repayments and the change in the credit card balance over time.\nIf you offer a suite of financial products to customers, you may be able to monitor changes in balances of other savings and debt products for a subset of the customers. You may also be able to infer use of financial products from other providers by observing transactions. Together these can help you understand the broader distribution of debt balances and net financial position. However, this will be a partial picture and could be biased, particularly if those who hold many products with you have different characteristics than those who hold just a credit card.\nSubjective financial wellbeing, although being a broader measure, may actually be easier to obtain. You could add a survey component to your trial and obtain a direct measurement. The challenge there, however, is whether a minor intervention such as a text message can shift subjective financial wellbeing enough for you to detect it in a trial.\nBeyond outcomes, you might also want to measure process pieces. For example, if looking at debt on credit card, you might collect data on payment patterns. What day and time do they pay? How much each payment? What method of payment? Where did the payment come from? This could enable you to better understand how or why your intervention works and provide further ideas for testing.\nDue to practical requirements, measurement may simply involve analysis of the data you already collect. Many measures of interest are already collected, and if your experiment can capitalise on that, it may improve feasibility and reduce cost. However, this may also create an arbitrary break to the extent of the outcomes that you want to examine.",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Level of outcome</span>"
    ]
  },
  {
    "objectID": "running-a-trial/level-of-outcome.html#measurement",
    "href": "running-a-trial/level-of-outcome.html#measurement",
    "title": "21  Level of outcome",
    "section": "",
    "text": "NoteExample: Reducing power consumption\n\n\n\nPower companies often want to limit their customers’ electricity demand. This might be for environmental reasons or to reduce peak demand.\nOne method to achieve this is to give that person or household a comparison of their power consumption with that of their neighbours. People have a desire to conform, and look to cues to inform their decisions. If shown that their power usage is above their neighbours, they tend to reduce their use.\n\n\nReducing power consumption example\n\n\n\nThat is one possible outcome: reduced electricity usage. A related direct outcome is the financial saving through reduced power usage. Do they pay a smaller power bill?\nBut these are narrow outcomes. Broader questions could be asked.\nWhat was the net change in energy usage by the household? Was there substitution into gas or other energy sources? What is the emissions profile of these changes? What is the cost?\nMore broadly, what was the total change in household expenditure due to the intervention? Did they pay for energy savings appliances or fitouts to their house? What did they spend any energy bill savings on? How much time did they expend changing their energy usage?\nEven those questions might be seen as narrow. If the individual or household’s objectives were purely financial, there may be a success. They have saved on their power bill. Their reduction in use also aligns with the environmental or peak demand reduction objectives of the electricity provider. But what if their objective is satisfaction in life? Or comfort? Did they freeze during winter and swelter during summer to maintain their self-image? You have just compared them negatively with their neighbour. Did their happiness change when they saw that they compared poorly? Did it increase mental stress?\nMeasuring many of those outcomes are difficult. But that does not mean that they don’t matter.\nHunt Allcott and Judd Kessler sought a broader measure of the benefits of the comparisons by asking customers their “willingness to pay” for the home energy reports that contained comparisons of their energy use. This willingness to pay measure supported the argument that there was a net welfare gain from the the comparison. However, because the willingness to pay measures capture the broader costs and benefits of the energy reports beyond simple changes in energy usage, they found that the welfare gains were much smaller than assessed using narrow measures.\nPerhaps most interestingly, one third of the recipients would be willing to pay to not receive the report. For those customers, the psychological or other costs outweighed any information benefit.",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Level of outcome</span>"
    ]
  },
  {
    "objectID": "running-a-trial/level-of-outcome.html#optional-reading",
    "href": "running-a-trial/level-of-outcome.html#optional-reading",
    "title": "21  Level of outcome",
    "section": "21.2 Optional reading",
    "text": "21.2 Optional reading\nAllcott and Kessler (2019) “The Welfare Effects of Nudges: A Case Study of Energy Use Social Comparisons”, American Economic Journal: Applied Economics, 11(1), 236-276, https://doi.org/10.1257/app.20170328",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Level of outcome</span>"
    ]
  },
  {
    "objectID": "running-a-trial/random-assignment.html",
    "href": "running-a-trial/random-assignment.html",
    "title": "22  Random assignment",
    "section": "",
    "text": "22.1 What is involved in practice?\nIn its simplest form, randomised trials work by splitting trial participants into two or more groups by random lot. Each group is then given a different intervention, with one of those groups typically a “control group” or “test group” that receives no intervention or the status quo.\nGroups might be allocated randomly by drawing numbers out of a hat, flipping a coin or, more commonly in experimental work, using a pseudo-random number generator. It is not the experimenter that decides who gets an intervention or not, but rather chance.\nRandomisation works as a control technique because it enables experimenters to hold approximately equal the sources of experimental bias, such as uncontrolled variables, between the control and treatment groups. These might even be variables of which we are ignorant.\nIn Uncontrolled: The Surprising Payoff of Trial-and-Error for Business Politics and Society, Jim Manzi (2012) gives the following example:\nRandomisation relies on the law of large numbers, the idea that as sample size increases the sample average converges to the expected value. In the case of the experiment to reduce blood pressure, as the size of the groups increased, we would expect the proportion of people with the hypertension genetic variant to converge to around 10% in each group.\nTo think of what that means intuitively, if you had only ten in each group, there is a material chance the groups could have zero, one, or more people with the hypertension variant, although it would rarely be three or more. Therefore, with a small sample, the relative proportions of the variant vary markedly between groups. It is only be collecting large samples that we can expect the groups to approximately equal proportions.\nAfter randomisation and application of the treatment, outcomes are then measured for each group. Within certain statistical parameters (also to be covered later in this unit), we can then take the differences between the two groups to be as a result of the different interventions we received. The treatment “caused” the differences, although as noted above, we may not understand the mechanism.\nYou can read more about why we randomise in Glennerster and Takavarasha (2013b).\nIn practice, random assignment includes the following steps:\nWe will cover some of these steps, such as power analysis, in more detail later in these notes.",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Random assignment</span>"
    ]
  },
  {
    "objectID": "running-a-trial/random-assignment.html#what-is-involved-in-practice",
    "href": "running-a-trial/random-assignment.html#what-is-involved-in-practice",
    "title": "22  Random assignment",
    "section": "",
    "text": "Define those eligible for a program.\nDecide the level of randomisation (such as individuals, communities, schools, call centres, bank branches). When you design a randomized evaluation, you need to decide whether to randomise individuals in and out of the program or to randomise the whole group in and out of the program.\nPower Analysis: Decide the sample size.\nRandomly assign which units (individuals or groups) are in the treatment, and the control group. Groups might be allocated randomly by drawing numbers out of a hat, flipping a coin or, more commonly in experimental work, using a pseudo-random number generator. It is not the experimenter that decides who gets an intervention or not, but rather chance.",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Random assignment</span>"
    ]
  },
  {
    "objectID": "running-a-trial/random-assignment.html#random-sample-versus-random-assignment",
    "href": "running-a-trial/random-assignment.html#random-sample-versus-random-assignment",
    "title": "22  Random assignment",
    "section": "22.2 Random sample versus random assignment",
    "text": "22.2 Random sample versus random assignment\nIn both random assignment and random sampling, we use a random process to select units (individuals, households, schools, etc.). But there is a crucial difference.\nIn random sampling, we take a population and use a random process to select units and create a group that is representative of the entire population. We can then measure the characteristics of this group and infer from them the characteristics of the entire population. (Most surveys use this approach.)\nIn random assignment, we take a study population—a pool of eligible units—and use a random process to assign units to different groups, such as treatment and comparison groups (i.e., we use it to determine access to a program).\nRandom assignment:\n\nUnits (people, schools, etc.) are randomly assigned to different groups (e.g. treatment and comparison).\nCreates two or more comparable groups.\nBasis of randomised evaluation.\n\nRandom sampling:\n\nWant to measure the characteristics of a group (e.g. average height).\nMeasure a random sample of the group.\nOften used during randomised evaluations, especially group level randomisation.",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Random assignment</span>"
    ]
  },
  {
    "objectID": "running-a-trial/random-assignment.html#what-can-be-randomised",
    "href": "running-a-trial/random-assignment.html#what-can-be-randomised",
    "title": "22  Random assignment",
    "section": "22.3 What can be randomised?",
    "text": "22.3 What can be randomised?\nThere are three basic ways in which a program can be randomised.\nAccess\nWe can choose which people are offered access to a program. This is the most common way to randomise.\nFor example: “Draw a list of 200 eligible branches and then randomly select 100 to receive the new product.”\nTiming\nWe can choose when people are offered access.\nSometimes, everyone needs access to the program (e.g. by law, fairness). We can randomly assign the time when people get access to the program.\nFor example, suppose a new medical treatment is proposed. Group A could get the treatment in Year 1, Group B in Year 2 and Group C in Year 3. Groups B and C are control groups in Year 1. Group C is a control group in Year 2.\nPhase-in designs may result in “anticipatory effects”. The anticipation of treatment may affect the behaviour of the control group, leading to an overestimate or underestimate of the impact.\nFor example, consider a program that provides a laptop to each student at schools. Suppose some parents of children in the control group that planned to purchase a home computer before the evaluation decided not to purchase it and wait for their child to receive a laptop. This behavioural change is important: some parents in control group behaved differently than they would have if the program did not exist. They are no longer the best representation of the counterfactual.\nAnother implication of phase-in designs is that, since the control group receives treatment after a fixed time frame, there is a limited time over which impact can be measured. The evaluation of the program’s impact will be of short-term outcomes.\nEncouragement\nWe can choose which people are encouraged to participate\nEncouragement might be a small incentive, a letter, or a phone call that reminds people of their eligibility and details the steps to enrol in the program. Messaging is a form of encouragement design.\nEncouragement is useful to evaluate a program already open to all eligible recipients, but only some are currently using it. Effective encouragement leads to higher take-up of the program in the treatment group than in the control group.\nIt is important to note that the impact of receiving encouragement to take up the program is evaluated (and its indirect effect on program take-up), rather than the direct impact of the program itself.\nWhen studying the program’s impact, comparing the entire treatment group to the entire control group is important. When analysing the results, individuals in the treatment group who receive encouragement but do not apply for the program must still be considered a part of the treatment group. Similarly, individuals in the control group who apply for the program without special encouragement must remain in the control group for analysis.\nEncouragement designs attract the following considerations:\n\nThe program to be evaluated must be undersubscribed.\nTo generate impact estimates, the encouragement must induce significantly higher take-up rates in the treatment group compared to the control group.\nThe encouragement should not have a direct effect on the outcome.\nEveryone must be affected by the encouragement incentive in the same direction. If the encouragement itself increases the take-up of some groups and reduces the take-up of others, impact estimates will likely be biased.\n\nYou can read more about how to randomise in Glennerster and Takavarasha (2013a).",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Random assignment</span>"
    ]
  },
  {
    "objectID": "running-a-trial/random-assignment.html#opportunities-to-randomise",
    "href": "running-a-trial/random-assignment.html#opportunities-to-randomise",
    "title": "22  Random assignment",
    "section": "22.4 Opportunities to randomise",
    "text": "22.4 Opportunities to randomise\nGlennerster and Takavarasha (2013a) listed ten of the most common opportunities to randomise:\n\nNew program design: A problem has been identified, and a new solution is required. This might provide an opportunity to contribute to the program design and then test.\nNew programs: When a program is new, and its effects are unknown, a randomised evaluation is the best way to estimate its impact.\nNew service: When an existing program offers a new service, rollout of the new service can be randomised.\nNew people: Programs often expand by adding new people. When there are not enough resources for everyone, randomising may be the fairest way to decide who will be served first. For example, Oregon had limited funding to extend Medicaid (health insurance for low-income people) to more people. They held a lottery to decide who would be offered the service.\nNew locations: Programs often expand to new locations. People in the new location may be added progressively through randomisation.\nOversubscription: When demand outstrips supply, random assignment may be the fairest way to choose participants. The government in Colombia uses a lottery to decide who receives secondary school tuition vouchers. The US government uses a lottery to decide who receives vouchers for housing in more affluent neighbourhoods.\nUndersubscription: When the take-up of a program is low, we can increase demand by encouraging a random group. For example, a large American university encourages some employees to attend an information session on their retirement savings program.\nRotation: If people take turns accessing the program, the order in which program resources rotate can be decided randomly, with the group on rotation serving as the treatment group. If we do this, we need to consider effects lingering after the program has rotated to another group.\nAdmission cutoffs: Some programs have admission cutoffs. Those just below the cutoff could be randomly admitted. For example. if a bank approves loans to applicants with a credit score above 60, a random selection of applicants with scores between 55 and 59 could be approved. Those with scores in that range but not approved would form the control group.\nAdmission in phases: If resources are going to grow, people may be admitted to the program as resources become available.",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Random assignment</span>"
    ]
  },
  {
    "objectID": "running-a-trial/random-assignment.html#level-of-randomisation",
    "href": "running-a-trial/random-assignment.html#level-of-randomisation",
    "title": "22  Random assignment",
    "section": "22.5 Level of randomisation",
    "text": "22.5 Level of randomisation\nA question that you might need to consider is the level of randomisation. Do you randomise individuals, or do you cluster? In cluster randomisation, groups of subjects are randomised. For example, if you have 20 call centres, randomise those 20 into the treatment and control rather than individual call centre staff.\nCluster randomisation can introduce greater complexity into the analysis, including potential intracluster correlation that should be accounted for. Glennerster and Takavarasha (2013a) identify considerations as to the appropriate level of randomisation as including:\n\nUnit of measurement: What is the unit at which your outcomes will be measured? Randomisation cannot occur at a lower level than our measured outcome. For example, we need to randomise at the firm (not the worker) level to study the effect of worker training on firm profits.\nSpillovers: Are there spillovers in that the intervention changes outcomes for those not directly participating? Do we want the impact estimate to capture them? Do we want to measure them? Cluster randomisation is used to control for contamination across individuals, as a change in behaviour in one might change the behaviour of others. For example, suppose you are implementing a trial to improve on-time tax return submissions by phoning taxpayers. You want to test what scripts and tools for call centre staff most effectively increase on-time submission. If you randomise at the staff–member level, staff in the control group may become aware of other approaches in their centre and change their behaviour.\nAttrition: Attrition is when we cannot collect data on all our sample. Which level is best to keep participants from dropping out of the sample? For example, sometimes providing benefits to a person but not their neighbour is considered unfair. If people feel a study is unfair, they can refuse to cooperate, leading to attrition. Randomising at a higher level can help.\nCompliance: compliance is when all those in the treatment group get the program and all those in the comparison don’t. Which level is best to limit departures from the study protocol? For example, if iron supplements are provided to one family member, what if they share with others? They would receive less than intended and the other family member more.\nStatistical power: Statistical power is the ability to detect an effect of a given size. Which level gives us the greatest probability of detecting a treatment? Cluster randomisation reduces power by effectively reducing the sample size.\nFeasibility: Which level is ethically, financially, politically and logistically feasible?\n\nEthics: Randomising at an individual level may create tensions that lead to harm if some are seen as unfairly chosen. Randomising at the group level raises issues about seeking informed consent.\nPolitics: Allocation may seem arbitrary when people with equal needs interacting regularly are assigned to different groups.\nLogistics: Sometimes, it is not practicable to randomise experimental subjects individually. In the staff training example above, training in the tools is provided at staff briefings at the beginning of each shift, making it impracticable to randomise across call centre staff within the centre.\nCost: If you randomise at the suburb level, we usually have to treat everyone in the suburb.",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Random assignment</span>"
    ]
  },
  {
    "objectID": "running-a-trial/random-assignment.html#three-research-designs",
    "href": "running-a-trial/random-assignment.html#three-research-designs",
    "title": "22  Random assignment",
    "section": "22.6 Three research designs",
    "text": "22.6 Three research designs\nWe have discussed three aspects of the intervention that we can randomise: access, timing and encouragement.\nAnother dimension to consider is how we can create variation in exposure to the program. Here I discuss three: the basic lottery, a lottery around the cutoff and the phase-in design.\nThe basic lottery\nThe basic lottery randomises access to the intervention and leaves the treatment status unchanged throughout. We compare those with and without access to the intervention.\nThe basic lottery is most workable when a program is oversubscribed, resources are constant for the evaluation period, and it is acceptable that some receive no intervention.\nAdvantages are that it is familiar, understood, generally seems fair, easy to implement and allows for estimation of long-term impacts.\nA disadvantage is differential attrition, as units in the control group may have little reason to cooperate with the survey.\nLottery around the cutoff\nA lottery around the cutoff randomises the intervention among those close to an eligibility cutoff. This strategy is workable when eligibility is determined by a scoring system, and there are many participants.\nA disadvantage of this strategy is that it measures the impact of the intervention only on those close to the eligibility cutoff.\nPhase-in\nThe phase-in strategy randomises the timing of access and switches groups from control to treatment over time. It compares those with access to those who have not yet received access. This strategy is most workable when everyone must eventually receive the program and resources are growing over time.\nAn advantage of this approach is that phased roll-outs of interventions are common, and the control group may be more willing to cooperate in anticipation of future benefits.\nA disadvantage is that the control group is of limited duration, meaning there is a limited time over which impact can be measured. Anticipation of the treatment may also affect the behaviour of the control group.\nQuestion\nOnly 5% of people in your company are taking advantage of a new mindfulness program provided by your employer. You would like to evaluate if this program improves employee productivity.\nWhat type of design would be suitable for this evaluation?\n\n A phase-in design an encouragement design A lottery that would give access to the program to some employees\n\n\n\nClick here to see an explanation\n\nAn encouragement design would be suitable as the program is available to everyone but not taken up.\n\n\n\n\n\nGlennerster, R., and Takavarasha, K. (2013a). Randomizing. In Running randomized evaluations: A practical guide (pp. 98–179). Princeton University Press. https://www.jstor.org/stable/j.ctt4cgd52.8\n\n\nGlennerster, R., and Takavarasha, K. (2013b). Why randomize? In Running randomized evaluations: A practical guide (pp. 24–65). Princeton University Press. https://www.jstor.org/stable/j.ctt4cgd52.6\n\n\nHaynes, L., Service, O., Goldacre, B., and Torgerson, D. (2012). Test, Learn, Adapt: Developing Public Policy with Randomised Controlled Trials. https://www.gov.uk/government/publications/test-learn-adapt-developing-public-policy-with-randomised-controlled-trials\n\n\nManzi, J. (2012). Uncontrolled. Basic Books. https://www.hachettebookgroup.com/titles/jim-manzi/uncontrolled/9780465029310/?lens=basic-books",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Random assignment</span>"
    ]
  },
  {
    "objectID": "running-a-trial/random-sample-versus-random-assignment.html",
    "href": "running-a-trial/random-sample-versus-random-assignment.html",
    "title": "23  Random sample versus random assignment",
    "section": "",
    "text": "23.1 References\nIn both random assignment and random sampling, we use a random process to select units (individuals, households, schools, etc.). But there is a crucial difference.\nIn random sampling, we take a population and use a random process to select units and create a group that is representative of the entire population. We can then measure the characteristics of this group and infer from them the characteristics of the entire population. (Most surveys use this approach.)\nIn random assignment, we take a study population—a pool of eligible units—and use a random process to assign units to different groups, such as treatment and comparison groups (i.e., we use it to determine access to a program).\nGlennerster, R., & Takavarasha, K. (2013). Why Randomize?. Running Randomized Evaluations: A Practical Guide (pp. 24–65). Princeton University Press.",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Random sample versus random assignment</span>"
    ]
  },
  {
    "objectID": "running-a-trial/what-can-be-randomised.html",
    "href": "running-a-trial/what-can-be-randomised.html",
    "title": "24  What can be randomised?",
    "section": "",
    "text": "24.1 References\nThere are three basic ways in which a program can be randomised.\nGlennerster and Takavarasha (2013) “Randomizing” in Running Randomized Evaluations: A Practical Guide (pp. 98–179). Princeton University Press.",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>What can be randomised?</span>"
    ]
  },
  {
    "objectID": "running-a-trial/opportunities-to-randomise.html",
    "href": "running-a-trial/opportunities-to-randomise.html",
    "title": "25  Opportunities to randomise",
    "section": "",
    "text": "25.1 References\nTen of the most common opportunities to randomise are listed below:\nGlennerster, R., & Takavarasha, K. (2013) “Randomizing” in Running Randomized Evaluations: A Practical Guide (pp. 66–97). Princeton University Press",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Opportunities to randomise</span>"
    ]
  },
  {
    "objectID": "running-a-trial/level-of-randomisation.html",
    "href": "running-a-trial/level-of-randomisation.html",
    "title": "26  Level of randomisation",
    "section": "",
    "text": "26.1 References\nA question that you might need to consider is the level of randomisation. Do you randomise individuals, or do you cluster? In cluster randomisation, groups of subjects are randomised. For example, if you have 20 call centres, randomise those 20 into the treatment and control rather than individual call centre staff.\nCluster randomisation can introduce greater complexity into the analysis, including potential intracluster correlation that should be accounted for. Considerations as to the appropriate level of randomisation include:\nGlennerster, R., & Takavarasha, K. (2013) “Randomizing” in Running Randomized Evaluations: A Practical Guide (pp. 66–97). Princeton University Press",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Level of randomisation</span>"
    ]
  },
  {
    "objectID": "running-a-trial/three-research-designs.html",
    "href": "running-a-trial/three-research-designs.html",
    "title": "27  Three research designs",
    "section": "",
    "text": "27.1 The basic lottery\nWe have discussed three aspects of the intervention that we can randomise: access, timing and encouragement.\nAnother dimension to consider is how we can create variation in exposure to the program. Here I discuss three: the basic lottery, a lottery around the cutoff and the phase-in design.\nThe basic lottery randomises access to the intervention and leaves the treatment status unchanged throughout. We compare those with and without access to the intervention.\nThe basic lottery is most workable when a program is oversubscribed, resources are constant for the evaluation period, and it is acceptable that some receive no intervention.\nAdvantages are that it is familiar, understood, generally seems fair, easy to implement and allows for estimation of long-term impacts.\nA disadvantage is differential attrition, as units in the control group may have little reason to cooperate with the survey.",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Three research designs</span>"
    ]
  },
  {
    "objectID": "running-a-trial/three-research-designs.html#lottery-around-the-cutoff",
    "href": "running-a-trial/three-research-designs.html#lottery-around-the-cutoff",
    "title": "27  Three research designs",
    "section": "27.2 Lottery around the cutoff",
    "text": "27.2 Lottery around the cutoff\nA lottery around the cutoff randomises the intervention among those close to an eligibility cutoff. This strategy is workable when eligibility is determined by a scoring system, and there are many participants.\nA disadvantage of this strategy is that it measures the impact of the intervention only on those close to the eligibility cutoff.",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Three research designs</span>"
    ]
  },
  {
    "objectID": "running-a-trial/three-research-designs.html#phase-in",
    "href": "running-a-trial/three-research-designs.html#phase-in",
    "title": "27  Three research designs",
    "section": "27.3 Phase-in",
    "text": "27.3 Phase-in\nThe phase-in strategy randomises the timing of access and switches groups from control to treatment over time. It compares those with access to those who have not yet received access. This strategy is most workable when everyone must eventually receive the program and resources are growing over time.\nAn advantage of this approach is that phased roll-outs of interventions are common, and the control group may be more willing to cooperate in anticipation of future benefits.\nA disadvantage is that the control group is of limited duration, meaning there is a limited time over which impact can be measured. Anticipation of the treatment may also affect the behaviour of the control group.",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Three research designs</span>"
    ]
  },
  {
    "objectID": "running-a-trial/three-research-designs.html#references",
    "href": "running-a-trial/three-research-designs.html#references",
    "title": "27  Three research designs",
    "section": "27.4 References",
    "text": "27.4 References\nGlennerster, R., & Takavarasha, K. (2013) “Randomizing” in Running Randomized Evaluations: A Practical Guide (pp. 66–97). Princeton University Press",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Three research designs</span>"
    ]
  },
  {
    "objectID": "running-a-trial/randomisation-techniques.html",
    "href": "running-a-trial/randomisation-techniques.html",
    "title": "28  Randomisation techniques",
    "section": "",
    "text": "28.1 References\nAs we discussed earlier, randomisation provides indirect control of uncontrolled variables. It provides us with a way to infer that differences in outcomes are due to the treatments and not due to the individual characteristics of the experimental participants.\nHowever, randomisation is not a panacea, nor is it always practical to undertake a pure randomisation. The below discusses two complications that can be involved in randomisation.\nKirgios, Mandel, Park, Milkman, Gromet, Kay, and Duckworth (2020) “Teaching temptation bundling to boost exercise: A field experiment” Organizational Behavior and Human Decision Processes, 161, 20–35, https://doi.org/10.1016/j.obhdp.2020.09.003\nKristal et al. (2020) “When We’re Wrong, It’s Our Responsibility as Scientists to Say So”, Scientific American, https://blogs.scientificamerican.com/observations/when-were-wrong-its-our-responsibility-as-scientists-to-say-so/\nList, Sadoff and Magner (2010) “So you want to run an experiment, now what? Some simple rules of thumb for optimal experimental design”, Experimental Economics, https://doi.org/10.1007/s10683-011-9275-7",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Randomisation techniques</span>"
    ]
  },
  {
    "objectID": "running-a-trial/data-collection-plan.html",
    "href": "running-a-trial/data-collection-plan.html",
    "title": "29  Data collection plan",
    "section": "",
    "text": "29.1 Finding existing data sources or collecting new data\nOnce you have specified your outcomes of interest and selected the indicators you will use to measure outcomes, you need to specify how the data will be collected.\nYou must decide between using existing administrative data sources and collecting your own data through surveys or non-survey instruments. Administrative data are records collected by governments or civil society organisations, usually in the context of program administration. Collecting your own data allows creating a custom data set on exactly the outcomes you are interested in. The most common method is to administer surveys. In some cases, you can rely on non-survey instruments such as random spot checks (for example to check the attendance and participation of program participants) or mystery clients.",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Data collection plan</span>"
    ]
  },
  {
    "objectID": "running-a-trial/data-collection-plan.html#finding-existing-data-sources-or-collecting-new-data",
    "href": "running-a-trial/data-collection-plan.html#finding-existing-data-sources-or-collecting-new-data",
    "title": "29  Data collection plan",
    "section": "",
    "text": "29.1.1 Specifying the time and frequency of data collection\nYou must decide when and how often to collect data. Should you conduct a baseline survey before the program is rolled out? Should you collect data throughout program implementation? How long do you wait before doing an endline survey?\nA baseline survey might be worthwhile when we want to show that our treatment and comparison group look similar before the program. This is called a balance check and if we want to analyse the data by subgroup or conduct regression analysis in the final analysis.\nThe more frequent our data collection, the more likely we are to capture intermediate outcomes. Of course, a downside is the cost. Also, sometimes, too-frequent surveying can itself become an intervention and change outcomes. To find out more, look at the link below and read the influential article published in 2011 in the Proceedings of the National Academy of Sciences of the United States of America.\nThe decision of when to collect endline data is usually a trade-off between getting long term outcomes and getting results on time for the findings to inform decision-making.\n\n\n29.1.2 Ensuring identical data collection between the groups\nThe data must be collected in an identical manner between the treatment and comparison groups. Everything about the data collection (who does it, how it is done, how frequently, and with what tools) must be identical between the treatment and comparison groups. This ensures that you are not creating any differences between the treatment and comparison groups.\n\n\n\n\n\n\nNoteThe effects of being surveyed\n\n\n\nRead the following abstract (Zwane et al, 2011) to learn more about their findings on the effects of surveys on later behaviours of the household members. If you want to learn more in detail about the study, read the full article.\n\nAbstract\nDoes completing a household survey change the later behaviour of those surveyed? In three field studies of health and two of micro-lending, we randomly assigned subjects to be surveyed about health and/or household finances and then measured subsequent use of a related product with data that does not rely on subjects’ self-reports. In the three health experiments, we find that being surveyed increases use of water treatment products and take-up of medical insurance. Frequent surveys on reported diarrhoea also led to biased estimates of the impact of improved source water quality. In two micro-lending studies, we do not find an effect of being surveyed on borrowing behaviour. The results suggest that limited attention could play an important but context-dependent role in consumer choice, with the implication that researchers should reconsider whether, how, and how much to survey their subjects.\n\nZwane, A. P., Zinman, J., Van Dusen, E., Pariente, W., Null, C., Miguel, E., Kremer, M., Karlan, D. S., Hornbeck, R., Giné, X., Duflo, E., Devoto, F., Crepon, B., & Banerjee, A. (2011). Being surveyed can change later behavior and related parameter estimates. Proceedings of the National Academy of Sciences, 108(5), 1821–1826.\n\n\n\n\n\n\n\n\nTipActivity\n\n\n\nWhat are the advantages and disadvantages of using survey data over administrative records?",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Data collection plan</span>"
    ]
  },
  {
    "objectID": "running-a-trial/data-collection-plan.html#optional-reading",
    "href": "running-a-trial/data-collection-plan.html#optional-reading",
    "title": "29  Data collection plan",
    "section": "29.2 Optional reading",
    "text": "29.2 Optional reading\nZwane, A. P., Zinman, J., Van Dusen, E., Pariente, W., Null, C., Miguel, E., Kremer, M., Karlan, D. S., Hornbeck, R., Giné, X., Duflo, E., Devoto, F., Crepon, B., & Banerjee, A. (2011). Being surveyed can change later behavior and related parameter estimates. Proceedings of the National Academy of Sciences - PNAS, 108(5), 1821–1826.",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Data collection plan</span>"
    ]
  },
  {
    "objectID": "running-a-trial/pilot.html",
    "href": "running-a-trial/pilot.html",
    "title": "30  Pilot",
    "section": "",
    "text": "30.1 References\nAlthough not essential, it is often a good idea to conduct a formal pilot of the intervention. Pilot results can inform feasibility and identify modifications needed in the design of the larger study. Note that the results of a pilot cannot inform about the effectiveness of the intervention per se due to small sample sizes. However, it can serve several purposes:\nIf you would like to know more, read The role and interpretation of pilot studies in impact evaluation research by Shagun Sabarwa. The case study underlines the importance of having broad pilot studies in informing the causal hypothesis, study design, key variables of interest measurement and finally the decision of whether or not to proceed with a randomised evaluation.\nGlennerster, R., & Takavarasha, K. (2013). The Experimental Approach. Running Randomized Evaluations: A Practical Guide (pp. 1–23). Princeton University Press.",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Pilot</span>"
    ]
  },
  {
    "objectID": "running-a-trial/trial-implementation.html",
    "href": "running-a-trial/trial-implementation.html",
    "title": "31  Trial implementation",
    "section": "",
    "text": "31.1 What is involved",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Trial implementation</span>"
    ]
  },
  {
    "objectID": "running-a-trial/trial-implementation.html#what-is-involved",
    "href": "running-a-trial/trial-implementation.html#what-is-involved",
    "title": "31  Trial implementation",
    "section": "",
    "text": "Implement intervention as designed\nEnsure no violation of random assignment\nPut in place a process evaluation to assess operations on paper and in the field\n\n\n31.1.1 What methodologies are used for a process evaluation\nGlennerster and Takavarasha (2013) describe their methods of assessing operations on paper, following paper trails and assessing operations in the field.\n\nAssessing operations on paper. For each step in the theory of change, an operations plan should articulate the tasks that must be performed to achieve it, by whom, and when. If there isn’t one already, we must write one so that we will know how the program is supposed to be carried out. For example, imagine that we are conducting an anti-malaria program that distributes mosquito nets at prenatal clinics. We have to buy the nets, recruit the clinics into the program, deliver and store the nets, inform the target population, give the nets to the targeted people only, and so on. The operations plan outlines all the program tasks, and an analysis of this plan may reveal potential problems.\nFollowing paper trails. We can then examine paper records to see if the operations plan has been followed. We would want to trace the flow of money, determine the delivery dates of inputs, ensure that time targets were met, and, where possible, assess the rates of use of the new facility or program.\nAssessing operations in the field. A paper review should ideally be accompanied by some on-the-ground checking. The records may say that the attendance rate at a new school is high, but a good process evaluation will check the records against reality. A small number of random visits will quickly tell us whether the paper records are accurate and, if not, how great the gap is between the records and reality. We should also interview beneficiaries to hear their side of the story. Are they using the new school? How do they find it?\nGlennerster and Takavarasha (2013)",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Trial implementation</span>"
    ]
  },
  {
    "objectID": "running-a-trial/trial-implementation.html#references",
    "href": "running-a-trial/trial-implementation.html#references",
    "title": "31  Trial implementation",
    "section": "31.2 References",
    "text": "31.2 References\nGlennerster and Takavarasha (2013) “Asking the Right Questions” in Running Randomized Evaluations: A Practical Guide (pp. 66–97). Princeton University Press.",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Trial implementation</span>"
    ]
  },
  {
    "objectID": "running-a-trial/data-analysis.html",
    "href": "running-a-trial/data-analysis.html",
    "title": "32  Data analysis",
    "section": "",
    "text": "32.1 What is involved\nOnce the program has been implemented and you have completed the data collection, you need to analyse these data.\nCompare outcomes for treatment group and control group\nThis is typically straightforward because you have randomised people into a treatment and a control group.\nConduct possible heterogeneity analysis\nIt is sometimes interesting to see if the treatment effect differs in different subgroups (heterogeneous treatment effects). For example, is the effect larger for females than for males? To do this, you can compare outcomes for the treatment group and control group among the sample of females, and repeat this comparison for the sample of males. The group must share an observable characteristic that was determined before the start of the program.\nWe estimate the impact of the program on a subgroup by dropping all the individuals who do not belong to the subgroup and then running the same estimation procedure we do for the full sample.\nBe aware of data mining when there are many outcomes\nIf you have many outcomes, you may find a treatment effect only for a few but not consistently for all of them. One needs to be careful not to search too much for an effect that does not exist. Researchers and practitioners now often register a pre-analysis plan for their trial.",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Data analysis</span>"
    ]
  },
  {
    "objectID": "running-a-trial/data-analysis.html#preparing-the-data",
    "href": "running-a-trial/data-analysis.html#preparing-the-data",
    "title": "32  Data analysis",
    "section": "32.2 Preparing the data",
    "text": "32.2 Preparing the data\nBefore we start any analysis, we prepare (or “clean”) the data to ensure we understand their properties. We might:\n\nCorrect any errors in the data. Surveys should have consistency checks built in. However, we should not “overclean” the data.\nCheck for outliers. Are the results sensitive to whether certain observations are included or excluded from the analyses? Usually, we do not drop these outliers from the data unless we have prespecified exclusion criterion in the pre-analysis plan.\nCalculate attrition rates. A high attrition rate will tell us that we will need to undertake some analysis to indicate how sensitive our results are to different assumptions about what happened to those for whom we do not have data.\nCalculate compliance rates.\nPlot and describe the data. Exploring the data before formal statistical analysis is useful to uncover problems in the data and understand better the results.\n\nOne illustration of the importance of examining the data visually and descriptively relates to Shu et al. (2012). They reported that “signing before—rather than after—the opportunity to cheat makes ethics salient when they are needed most and significantly reduces dishonesty.” The paper contained the results of a field experiment whereby applicants for vehicle insurance reported higher mileage and incurred higher premiums when they signed at the top.\nAnonymous authors (2021) on Data Colada examined the data and wrote:\n\nLet’s first think about what the distribution of miles driven should look like. If there were about a year separating the Time 1 and Time 2 mileages, we might expect something like the figure below, taken from the UK Department of Transportation (.pdf) based on similar data (two consecutive odometer readings) collected in 2010:\n\n\n\nAs we might expect, we see that some people drive a whole lot, some people drive very little, and most people drive a moderate amount.\nAs noted by the authors of the 2012 paper, it is unknown how much time elapsed between the baseline period (Time 1) and their experiment (Time 2), and it was reportedly different for different customers [3]. For some customers the “miles driven” measure may reflect a 2-year period, while for others it may be considerably more or less than that [4]. It is therefore hard to know what the distribution of miles driven should look like in those data. It is not hard, however, to know what it should not look like. It should not look like this:\n\n\nThe data had been fraudulently generated. However, during the original analysis by Shu et al. (2012) and a review of the data by Kristal et al. (2020), nobody had picked this up as (likely) no one had done a simple plot.",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Data analysis</span>"
    ]
  },
  {
    "objectID": "running-a-trial/data-analysis.html#references",
    "href": "running-a-trial/data-analysis.html#references",
    "title": "32  Data analysis",
    "section": "32.3 References",
    "text": "32.3 References\nAnonymous (2021) “[98] Evidence of Fraud in an Influential Field Experiment About Dishonesty.” Data Colada. August 17, 2021. http://datacolada.org/98\nKristal et al. (2020) “Signing at the beginning versus at the end does not decrease dishonesty”, PNAS, 117(13), 7103-7107, https://doi.org/10.1073/pnas.1911695117\nShu et al. (2012) “Signing at the beginning makes ethics salient and decreases dishonest self-reports in comparison to signing at the end”, PNAS, 109(38), https://doi.org/10.1073/pnas.1209746109",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Data analysis</span>"
    ]
  },
  {
    "objectID": "running-a-trial/an-example-data-analysis.html",
    "href": "running-a-trial/an-example-data-analysis.html",
    "title": "33  An example data analysis",
    "section": "",
    "text": "33.1 The experimental design\nIn this part, I will illustrate the data analysis process by reproducing the analysis in Study 1 of Dietvorst, Simmons and Massey’s (2015) paper on algorithm aversion. The experiment tested the hypothesis that “seeing the model perform, and therefore err, would decrease participants’ tendency to bet on it rather than the human forecaster, despite the fact that the model was more accurate than the human.”\nExperimental participants were given a judgment task, with one group allowed to see the algorithm in action before undertaking the task. Participants were given the option of using the algorithm’s predictions or their own. Those who had seen the algorithm perform were less likely to use it in the task.\nParticipants were informed that they would play the part of an MBA admissions officer with a task to forecast the success of each applicant (as a percentile). Success was defined as an equal weighting of GPA, respect, and employer prestige.\nParticipants were also told that “thoughtful analysts” had built a statistical model to forecast performance using data that the participants would receive.\nAfter consent and information provision, participants were allocated into one of four conditions: three treatment conditions and a control.\nParticipants in each of the three treatment conditions saw and/or made 15 forecasts on which they received feedback on performance.\nAll participants, including the control, were asked whether they would like to use their own or the model’s forecasts for all 10 incentivised forecasts. After their choice, participants would make the forecasts (even if they had chosen the model for awarding incentives). After making their forecasts, they were asked questions about their confidence in their own forecasts and that of the model. They then learned the bonus earned.\nThere were four experimental conditions, with the difference in conditions occurring in the first stage of the experiment. For each of the 15 MBA applicants, they would gain experience as follows.\nBesides illustrating the analysis process for this subject, reproducing the analysis of a paper has other benefits.\nFirst, it allows you to check that the original analysis is correct. This is often not the case. Most times the errors are minor, but sometimes the original finding may not even hold. In that case, there may be limited value to the replication. You may also determine that an inappropriate analysis methodology has been used.\nSecond, reproducing the analysis is another way to understand better the experimental methodology and what the analysis shows. You can see what variables were recorded and how they are used.\nIt is normally only possible to reproduce the analysis if the original data is available on a public repository or from the authors.",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>An example data analysis</span>"
    ]
  },
  {
    "objectID": "running-a-trial/an-example-data-analysis.html#the-experimental-design",
    "href": "running-a-trial/an-example-data-analysis.html#the-experimental-design",
    "title": "33  An example data analysis",
    "section": "",
    "text": "Model: Participants would get feedback showing the model’s prediction and the applicant’s true percentile\nHuman: Participants would make a forecast and then get feedback showing their own prediction and the applicant’s true percentile\nModel and human: Participants would make a forecast and then get feedback showing their own prediction, the model’s prediction, and the applicant’s true percentile.\nControl: No experience.\n\n\n\nExperimental flow diagram",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>An example data analysis</span>"
    ]
  },
  {
    "objectID": "running-a-trial/an-example-data-analysis.html#reproducing-the-algorithm-aversion-paper",
    "href": "running-a-trial/an-example-data-analysis.html#reproducing-the-algorithm-aversion-paper",
    "title": "33  An example data analysis",
    "section": "33.2 Reproducing the algorithm aversion paper",
    "text": "33.2 Reproducing the algorithm aversion paper\nThe data for Dietvorst et al. (2015) is available from ResearchBox and the Journal of Experimental Psychology: General website.\nThe download includes the Stata code used by Dietvorst et al. (2015) to analyse the data. I do not know Stata, so I asked ChatGPT to translate the code into R. While that translation contained some errors, I made it operational with only minor changes.\nIn what follows I provide the R code that I used to reproduce the analysis. You can expand each “Code” item to see the code or copy it to replicate the results yourself.",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>An example data analysis</span>"
    ]
  },
  {
    "objectID": "running-a-trial/an-example-data-analysis.html#analysis",
    "href": "running-a-trial/an-example-data-analysis.html#analysis",
    "title": "33  An example data analysis",
    "section": "33.3 Analysis",
    "text": "33.3 Analysis\nI start by downloading and unzipping the data file and loading the CSV for Study 1 into the R environment. I then made some amendments to the variable names to make them more readable (e.g. replacing the numbers for each condition with the words they represent: 1=control, etc.).\n\n\nCode\n# Data downloaded from https://researchbox.org/379&PEER_REVIEW_passcode=MOQTEQ\n# Open Zip file from research box\n# unzip(\"ResearchBox_379.zip\")\n\n#Load data from csv\ndata &lt;- read.csv(\"ResearchBox 379/Data/Study 1 Data.csv\", header = TRUE, sep = \",\", na.strings = \".\")\n\n# Change to camel case\ncolnames(data) &lt;- paste(tolower(substring(colnames(data), 1, 1)), substring(colnames(data), 2), sep = \"\")\nnames(data)[1] &lt;- \"ID\"\n\n# Define label mappings\ncondition &lt;- c(\"1\" = \"control\", \"2\" = \"human\", \"3\" = \"model\", \"4\" = \"model&human\")\nmodelBonus &lt;- c(\"0\" = \"choseHuman\", \"1\" = \"choseModel\")\nbinary &lt;- c(\"0\" = \"no\", \"1\" = \"yes\")\nbetterStage2Bonus &lt;- c(\"1\" = \"model\", \"2\" = \"equal\", \"0\" = \"human\")\nconfidence &lt;- c(\"1\" = \"none\", \"2\" = \"little\", \"3\" = \"some\", \"4\" = \"fairAmount\", \"5\" = \"aLot\")\n\n# Apply label mappings to variables - have done more here than required for analysis, but might be useful later\ndata$condition &lt;- factor(data$condition, labels = condition)\ndata$modelBonus &lt;- factor(data$modelBonus, labels = modelBonus)\ndata$humanAsGoodStage1 &lt;- factor(data$humanAsGoodStage1, labels = binary)\ndata$humanBetterStage1 &lt;- factor(data$humanBetterStage1, labels = binary)\ndata$humanBetterStage2 &lt;- factor(data$humanBetterStage2, labels = binary)\ndata$betterStage2Bonus &lt;- factor(data$betterStage2Bonus, labels = betterStage2Bonus)\ndata$model &lt;- factor(data$model, labels = binary)\ndata$human &lt;- factor(data$human, labels = binary)\ndata$modelConfidence &lt;- factor(data$modelConfidence, labels = confidence)\ndata$humanConfidence &lt;- factor(data$humanConfidence, labels = confidence)\n\n\nI then run a quick check on the number of observations. I note from the paper that I should find 369 participants at the start, but 8 of these participants did not complete enough questions to get to the dependent variable. That leaves 361 participants across each of the conditions.\n\n\nCode\n# Total participants\nlength(data$ID)\n\n# Number of participants who chose between model and human\ntable(data$modelBonus)\n\n\nAs expected, I find 369 participants and 201+160=361 dependent variable observations.\nI then broke down the dependent variable findings in more detail to see what each participant chose (human or model) in each condition.\n\n\nCode\n# Choice of participants in each condition\n\nconditions &lt;- table(data$modelBonus, data$condition)\nconditions &lt;- rbind(conditions, total=colSums(conditions))\nconditions\n\n\nTwo chi-squared tests were then run to see if there was a significant difference between the conditions. The results are shown in the Study 1 diagram in Figure 3.\nThe first was a test of whether there was a difference between the two conditions without and the two conditions with the model. This is a test of the core hypothesis.\n\n\nCode\n# chisq.test between conditions 1 and 2 (control and human) and 3 and 4 (model and model&human)\n\nchisq.test(table(data$modelBonus, data$model), correct=FALSE)\n\n\nThe second was a test of whether there was a difference between the two conditions without and the two conditions with the human experience. This test was run to see whether the experience in seeing their own errors changed the participants’ choices.\n\n\nCode\n# chisq.test between conditions 1 and 3 (control and model) and 2 and 4 (human and model&human)\n\nchisq.test(table(data$modelBonus, data$human), correct=FALSE)\n\n\nI then plotted that data in a bar chart to illustrate the results. This matches Figure 3 in the paper.\n\n\nCode\n## load ggplot2 for chart and tidyverse packages for data manipulation\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# convert table to data frame for future operations\nconditions &lt;- as.data.frame.matrix(conditions)\n\n#percentage choosing model in each condition\n\nconditions &lt;- rbind(conditions, percent=conditions[2,]/conditions[3,])\nconditions &lt;- tibble::rownames_to_column(conditions, \"variable\")\nconditions_long &lt;- pivot_longer(conditions, cols = -variable, names_to = \"treatment\", values_to = \"value\", cols_vary = \"slowest\")\nconditions_long &lt;- conditions_long[c(2, 1, 3)]\n\n# ggplot2 bar plot of percent rows of conditions data frame\nplot_data &lt;- filter(conditions_long, variable==\"percent\")\n\nggplot(plot_data, aes(x=treatment, y=value)) +\n  geom_col()+\n  labs(title=\"Percent choosing model in each condition\", x=\"Condition\", y=\"Percent choosing model\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n\n  # Set the theme\n  theme_minimal()+\n  geom_hline(yintercept = 0, linewidth=0.25)\n\n\nFinally, I replicate the analysis related to Study 1 as shown in Table 3.\n\n\nCode\n# Calculate human and model mean rewards\nm&lt;-round(mean(data$bonusFromModel, na.rm=TRUE), 2)\nh&lt;-round(mean(data$bonusFromHuman, na.rm=TRUE), 2)\n\n# Bonus if chose model vs. human\nbonusModel &lt;- t.test(data$bonusFromModel, data$bonusFromHuman, paired=TRUE)\np &lt;- signif(bonusModel$p.value, 2)\nt &lt;- round(bonusModel$statistic, 2)\nd &lt;- round(bonusModel$estimate, 2)\n\n# Participants' AAE compared to model's AAE for stage 1 forecasts\nstage1AAE &lt;- t.test(data$humanAAE1, data$modelAAE1, paired=TRUE)\n\np1 &lt;- signif(stage1AAE$p.value, 2)\nt1 &lt;- round(stage1AAE$statistic, 2)\nd1 &lt;- round(stage1AAE$estimate, 2)\n\nAAEm&lt;-round(mean(data[data$condition=='model&human', 'modelAAE1'], na.rm=TRUE), 2)\nAAEh&lt;-round(mean(data[data$condition=='model&human', 'humanAAE1'], na.rm=TRUE), 2)\n\n# Participants' AAE compared to model's AAE for stage 2 forecasts\nstage2AAE &lt;- t.test(data$humanAAE2, data$modelAAE2, paired=TRUE)\n\np2 &lt;- signif(stage2AAE$p.value, 2)\nt2 &lt;- round(stage2AAE$statistic, 2)\nd2 &lt;- round(stage2AAE$estimate, 2)\n\nAAEm2&lt;-round(mean(data$modelAAE2, na.rm=TRUE), 2)\nAAEh2&lt;-round(mean(data$humanAAE2, na.rm=TRUE), 2)\n\n\nTable 3 results\nNote: To see the actual results, download the data from the links provided above and run this code yourself with eval: true.\n\n\n\n\nModel\nHuman\nDifference\nt score\np value\n\n\n\n\nBonus\n$2.50\n$2.22\n$0.28\n8.57\n&lt; 0.001\n\n\nStage 1 error\n12.00\n13.81\n-1.81\n-3.64\n&lt; 0.001\n\n\nStage 2 error\n11.76\n13.19\n-1.43\n-5.65\n&lt; 0.001\n\n\n\nThe numbers match those provided in the paper.",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>An example data analysis</span>"
    ]
  },
  {
    "objectID": "running-a-trial/an-example-data-analysis.html#regression",
    "href": "running-a-trial/an-example-data-analysis.html#regression",
    "title": "33  An example data analysis",
    "section": "33.4 Regression",
    "text": "33.4 Regression\nAn alternative approach is to do the comparison between treatment and comparison groups within a regression framework. This will give us the same results but in a different format.\nTo determine whether there is a treatment effect, we use the regression:\n\nY_i=c+\\beta T_i+\\epsilon_i\n\nWhere Y_i is the number of participants that chose the model, c is a constant, and T_i is a treatment dummy. T_i=0 if participant i is in the control group and 1 if participant i is in the treatment group. \\beta is the coefficient of the treatment dummy.\nLogistic regression is used here as we are considering the probability of an event taking place (choosing the model). If we were considering a continuous variable (e.g. amount saved due to an intervention), we would use linear regression, which is typically easier to interpret.\n\n\nCode\n# Regression to determine whether there is a treatment effect\nreg &lt;- glm(modelBonus ~ model, data=data, family=binomial)\n\n# Show results\nsummary(reg)\n\n\nThe p-value of 2.09e-13 is not identical to that from the chi-squared test above (3.419e-14) as there are slight differences in the underlying tests, but either way, the result is significant. We can see the equivalence of the underlying result by examining the parameter estimates.\nThe intercept of 0.5792 implies that the odds of choosing the model are e^{0.5792} for those in the control condition, which is equal to the observed proportion of 116 choosing the model compared to 65 choosing the human (116/65=1.784). The estimate of \\beta of -1.7077 implies that the ratio of the odds of those choosing the model in the control treatment condition relative to those in the control condition is e^{-1.7077}=0.181, which is equal to the observed proportion of 44 choosing the model and 136 choosing the model in the treatment condition, compared to the 116 choosing the model and 65 choosing the human in the control condition ((44/136)/(116/65)=0.181).\nThe benefit of using a regression approach is that other covariates can be added to the analysis. For example, if there was a difference between male and female respondents, we could add a dummy for gender to the analysis.\nIncluding covariates in the regression can give us a more precise estimate of the impact of the intervention as they can reduce unexplained variance. However, including covariates can reduce the precision of our estimate of the treatment effect if they don’t explain enough error variance.\nOften, the best covariate to include is a baseline measure of the outcome variable. With stratified randomisation, it is often recommended to add an indicator for each different stratum.",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>An example data analysis</span>"
    ]
  },
  {
    "objectID": "running-a-trial/an-example-data-analysis.html#references",
    "href": "running-a-trial/an-example-data-analysis.html#references",
    "title": "33  An example data analysis",
    "section": "33.5 References",
    "text": "33.5 References\nDietvorst, Simmons and Massey (2015) “Algorithm aversion: People erroneously avoid algorithms after seeing them err”, Journal of Experimental Psychology: General, 144, 114–126, https://doi.org/10.1037/xge0000033",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>An example data analysis</span>"
    ]
  },
  {
    "objectID": "running-a-trial/threats-to-the-integrity-of-the-experiment.html",
    "href": "running-a-trial/threats-to-the-integrity-of-the-experiment.html",
    "title": "34  Threats to the integrity of the experiment",
    "section": "",
    "text": "34.1 References\nThe randomisation design and data collection plan describe how the study is meant to proceed. But rarely does everything go to plan.\nIn this section, I discuss four threats to the integrity of an experiment: partial compliance; attrition; spillovers; and evaluation-driven effects.\nGlennerster, R., & Takavarasha, K. (2013) “Threats” in Running Randomized Evaluations: A Practical Guide (pp. 298-323). Princeton University Press\nGlennerster, R., and Takavarasha, K. (2013) “Analysis” In Running Randomized Evaluations: A Practical Guide (pp. 324–385). Princeton University Press\nJohnson and Wang-Ly (2020) “Using Tax Refunds for Debt Repayment: A Study with Commonwealth Bank of Australia”, Financial Health Network. https://finhealthnetwork.org/research/financial-health-solutions-using-tax-refunds-for-debt-repayment/",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Threats to the integrity of the experiment</span>"
    ]
  },
  {
    "objectID": "running-a-trial/alternative-trial-design-processes.html",
    "href": "running-a-trial/alternative-trial-design-processes.html",
    "title": "35  Alternative trial design processes",
    "section": "",
    "text": "35.1 Test, Lean, Adapt\nBeyond the set of steps for randomised trials that we have just explored, there are many other conceptions of the steps in a randomised controlled trial.\nOne of the best known in applied behavioural science is described by Haynes et al. (2012) in Test, Learn, Adapt: Developing Public Policy with Randomised Controlled Trials. This paper is written in relation to public policy problems, but the process can be applied equally to business problems. They describe nine steps:",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Alternative trial design processes</span>"
    ]
  },
  {
    "objectID": "running-a-trial/alternative-trial-design-processes.html#test-lean-adapt",
    "href": "running-a-trial/alternative-trial-design-processes.html#test-lean-adapt",
    "title": "35  Alternative trial design processes",
    "section": "",
    "text": "35.1.1 Test\nStep 1: Identify two or more interventions to compare. The purpose of a trial is to differentiate between different interventions when we do not know which is most effective.\nStep 2: Define the outcome that the intervention is intended to influence. We will discuss this on the next page.\nStep 3: Decide on the randomisation unit.\nStep 4: Determine how many units are required for robust results.\nStep 5: Assign each unit to one of the interventions using a robustly random method. By “robust”, we need to protect against bias creeping into the trial. We cannot allow those with vested interests to lean on which units are allocated to which treatments, or which units might be excluded altogether outside of those identified to be excluded as part of the original plan.\nStep 6: Introduce the interventions to the assigned groups. This may sound easy, but monitoring is required to ensure that the interventions are being introduced in the way intended to the right units. There also needs to be a consistency of treatment between the trial and what would be done at scale. For example, is a test of gold-plated interventions delivered by enthusiastic, recently trained practitioners going to give a fair measure of the effectiveness of an intervention delivered month-after-month by those not involved in the experiment? If specific effort is applied during the trial but cannot be applied when scaled, the trial is likely to be a poor guide as to effectiveness when scaled.\n\n\n35.1.2 Learn\nStep 7: Measure the results and determine the impact of the interventions. Your trial may show that none of the interventions are successful, but that’s a success. You have learnt something. The method of measurement should have been decided before randomisation and captured in the pre-analysis plan. We will cover pre-analysis plans in week 4.\n\n\n35.1.3 Adapt\nStep 8: Adapt your policy intervention to reflect your findings.\nStep 9: Return to step 1. This process worth is iterative. The trial gives you information that you may be able to implement at scale to improve outcomes, but it does not provide the perfect solution. You can continue to iterate toward better outcomes. Further, for many interventions, what is most effective may also change over time.",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Alternative trial design processes</span>"
    ]
  },
  {
    "objectID": "running-a-trial/alternative-trial-design-processes.html#other-processes",
    "href": "running-a-trial/alternative-trial-design-processes.html#other-processes",
    "title": "35  Alternative trial design processes",
    "section": "35.2 Other processes",
    "text": "35.2 Other processes\nA selection of processes in around the development of a trial include:\nideas42\nideas42’s behavioural design process\nThey describe a five-stage process:\n\nDefine the problem and try to remove any embedded assumptions about why it may be occurring.\nDiagnose what behavioral bottlenecks may be driving the problem.\nDesign interventions that directly address the key bottlenecks we’ve diagnosed.\nTest the intervention to see whether the design successfully addresses the problem, typically using a randomised controlled trial.\nScale the solution to a larger population or adapt it to other contexts if an intervention proves effective.\n\nBETA 4D\nBETA’s 4D framework, a four stage process:\n\nDiscover: Define the policy, program or service delivery issue and develop a behavioural problem statement.\nDiagnose: Understand the current behaviour and its drivers and develop a clearly defined hypothesis of behaviour change.\nDesign: Design an intervention to address the behavioural problem. Design an evaluation to test the intervention.\nDeliver: Implement the intervention and evaluation and share the results.\n\nBETA has also published a guidance note on Developing behavioural interventions for randomised controlled trials: Nine guiding questions, which provides more detail on undertaking the first two stages of the 4D framework.\n\n\n\n\nHaynes, L., Service, O., Goldacre, B., and Torgerson, D. (2012). Test, Learn, Adapt: Developing Public Policy with Randomised Controlled Trials. https://www.gov.uk/government/publications/test-learn-adapt-developing-public-policy-with-randomised-controlled-trials",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Alternative trial design processes</span>"
    ]
  },
  {
    "objectID": "running-a-trial/scaling.html",
    "href": "running-a-trial/scaling.html",
    "title": "36  Scaling",
    "section": "",
    "text": "36.1 1. False positives: The inference problem\nScaling is the process of expanding a successfully trialled intervention to the broader population of interest.\nIn his book The Voltage Effect, John List describes five barriers to scaling. These are:\nA false positive is an erroneous sign that success will continue (or even increase) as your enterprise grows. … [I]nitial feedback might simply be a false positive. Think of a false fire alarm or a false signal that you have COVID after a personal test. … One simple way to weed out false positives is to replicate ideas that show early promise.",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Scaling</span>"
    ]
  },
  {
    "objectID": "running-a-trial/scaling.html#representativeness-of-the-population",
    "href": "running-a-trial/scaling.html#representativeness-of-the-population",
    "title": "36  Scaling",
    "section": "36.2 2. Representativeness of the population",
    "text": "36.2 2. Representativeness of the population\nNo product is designed for everyone, everywhere, at all times. Knowing who your idea will work for, where, and when will help you figure out how far it can go. … Make sure your test groups at smaller scales reflect the larger population you’re aiming to reach.",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Scaling</span>"
    ]
  },
  {
    "objectID": "running-a-trial/scaling.html#representativeness-of-the-situation",
    "href": "running-a-trial/scaling.html#representativeness-of-the-situation",
    "title": "36  Scaling",
    "section": "36.3 3. Representativeness of the situation",
    "text": "36.3 3. Representativeness of the situation\nIf the driver is something unique or specific to a time and place, it may not be easily scalable. … [H]iring 30 excellent teachers might be easy, but hiring 30,000 excellent teachers is a whole different kettle of fish.",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Scaling</span>"
    ]
  },
  {
    "objectID": "running-a-trial/scaling.html#unintended-consequences-and-negative-spillovers",
    "href": "running-a-trial/scaling.html#unintended-consequences-and-negative-spillovers",
    "title": "36  Scaling",
    "section": "36.4 4. Unintended consequences and negative spillovers",
    "text": "36.4 4. Unintended consequences and negative spillovers\nWhen designing your idea early on, you must anticipate unintended consequences and negative spillovers and look for ways to engineer positive spillovers. For instance, in our preschool, we discovered that children who weren’t in our program were nonetheless performing significantly better. It turned out that just from playing with the students who were in the program aided their development—a wonderfully scalable spillover.",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Scaling</span>"
    ]
  },
  {
    "objectID": "running-a-trial/scaling.html#the-supply-side-of-scaling",
    "href": "running-a-trial/scaling.html#the-supply-side-of-scaling",
    "title": "36  Scaling",
    "section": "36.5 5. The supply side of scaling",
    "text": "36.5 5. The supply side of scaling\n[D]etermine how much the trajectory of costs change as you scale your idea, and if you or the market can bear this change. To ensure you don’t fall into the voltage drop of running out of money, you must account for two types of costs: 1) upfront fixed costs, like the one-time investment for the research and development to create a new product; and 2) your ongoing operating expenses.",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Scaling</span>"
    ]
  },
  {
    "objectID": "running-a-trial/scaling.html#barriers-to-adoption",
    "href": "running-a-trial/scaling.html#barriers-to-adoption",
    "title": "36  Scaling",
    "section": "36.6 Barriers to adoption",
    "text": "36.6 Barriers to adoption\nOther evidence on scaling comes from DallaVigna, Kim and Linos (2022), who examined 73 randomised controlled trials run across 30 cities in the United States. They wrote:\n\nCompared to most contexts, the barriers to adoption are low. Yet, city departments adopt a nudge treatment in follow-on communication in 27% of cases. As potential determinants of adoption we consider (i) the strength of the evidence, as determined by the RCT itself, (ii) features of the organization, such as “organizational capacity” of the city and whether the city staff member working on the RCT has been retained, and (iii) the experimental design, such as whether the RCT was implemented as part of pre-existing communication. We find (i) a limited impact of strength of the evidence and (ii) some impact of city features, especially the retention of the original staff member. By far, the largest predictor of adoption is (iii) whether the communication was pre-existing, as opposed to a new communication. We consider two main interpretations of this finding: organizational inertia, in that changes to pre-existing communications are more naturally folded into year-to-year city processes, and costs, since new communications may require additional funding. We find the same pattern for electronic communications, with zero marginal costs, supporting the organizational inertia explanation. The pattern of results differs from the predictions of both experts and practitioners, who over-estimate the extent of evidence-based adoption. Our results underline the importance of considering the barriers to evidence adoption, beginning at the stage of experimental design and continuing after the RCT completion.",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Scaling</span>"
    ]
  },
  {
    "objectID": "running-a-trial/scaling.html#references",
    "href": "running-a-trial/scaling.html#references",
    "title": "36  Scaling",
    "section": "36.7 References",
    "text": "36.7 References\nDellaVigna, Kim and Linos (2022) “Bottlenecks for Evidence Adoption” National Bureau of Economic Research, Working Paper 30144), https://doi.org/10.3386/w30144\nList (2022) “The Five Vital Signs of a Scalable Idea and How to Avoid a Voltage Drop”, Behavioral Scientist. https://behavioralscientist.org/the-five-vital-signs-of-a-scalable-idea-and-how-to-avoid-a-voltage-drop/\nList (2022) The Voltage Effect: How to Make Good Ideas Great and Great Ideas Scale. Currency, New York",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Scaling</span>"
    ]
  },
  {
    "objectID": "running-a-trial/ethical-principles-for-trial-design.html",
    "href": "running-a-trial/ethical-principles-for-trial-design.html",
    "title": "37  Ethical principles for trial design",
    "section": "",
    "text": "37.1 Respect for Persons\nExperimental research is full of examples of ethical failures. One of the most notorious is the Tuskegee Syphilis Study, in which almost 400 hundred African American men were denied safe and effective treatment for almost 40 years.\nToday there are many frameworks for ethical conduct of trials. One landmark framework comes from the Belmont Report (1979), a US Congress initiated national commission into ethical guidelines for research involving human subjects. It proposed three central principles for the conduct of trials: respect for persons, beneficence, and justice.\nThe Menlo Report (2012) was later developed in the light of increased digitisation of research practices and the difficulty in applying the ideas in the Belmont report in the digital age. It affirmed the three principles from the Menlo report and proposed a fourth: respect for law and public interest.\nThese principles form the basis of many Institutional Review Board ethical approvals in university and government. Each principle is described below:\nThe Menlo report described respect for persons as having the following components:\nPractically, respect for persons has tended to revolve around the principle of informed consent. People should be given information about the experiment in a comprehensible format and then voluntarily agree to participate.\nIt is easy to come up with scenarios where informed consent does not appear appropriate or would undermine the very purpose of the experiment. For example, Marianne Bertrand and Sendhil Mullainathan (2004) sent fictitious CVs in response to ads with randomly assigned African American or White sounding names. White names received 50% more callbacks for interviews. Obtaining informed consent from employers would be impractical and make the experiment pointless.\nThere have been hundreds of discrimination studies of this nature. The lack of consent has been justified for reasons including the limited harm to employers and the social benefit of an accurate measure of discrimination. These justifications have enabled experiments of this nature to pass Institution Review Board processes.",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Ethical principles for trial design</span>"
    ]
  },
  {
    "objectID": "running-a-trial/ethical-principles-for-trial-design.html#respect-for-persons",
    "href": "running-a-trial/ethical-principles-for-trial-design.html#respect-for-persons",
    "title": "37  Ethical principles for trial design",
    "section": "",
    "text": "Participation as a research subject is voluntary, and follows from informed consent\nTreat individuals as autonomous agents and respect their right to determine their own best interests\nRespect individuals who are not targets of research yet are impacted - Individuals with diminished autonomy, who are incapable of deciding for themselves, are entitled to protection.",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Ethical principles for trial design</span>"
    ]
  },
  {
    "objectID": "running-a-trial/ethical-principles-for-trial-design.html#beneficence",
    "href": "running-a-trial/ethical-principles-for-trial-design.html#beneficence",
    "title": "37  Ethical principles for trial design",
    "section": "37.2 Beneficence",
    "text": "37.2 Beneficence\nThe Menlo report described beneficence as having the following components:\n\nDo not harm\nMaximize probable benefits and minimize probable harms\nSystematically assess both risk of harm and benefit.\n\nOne major potential source of harm is “informational risk”, the risk that information gathered in a trial may be disclosed. This is often dealt with through anonymisation, although there are ample examples of failures or re-identification of data after anonymisation processes. This leads to requirements to develop data protection plans and sharing protocols.",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Ethical principles for trial design</span>"
    ]
  },
  {
    "objectID": "running-a-trial/ethical-principles-for-trial-design.html#justice",
    "href": "running-a-trial/ethical-principles-for-trial-design.html#justice",
    "title": "37  Ethical principles for trial design",
    "section": "37.3 Justice",
    "text": "37.3 Justice\nThe Menlo report described justice as having the following components:\n\nEach person deserves equal consideration in how to be treated, and the benefits of research should be fairly distributed according to individual need, effort, societal contribution, and merit\nSelection of subjects should be fair, and burdens should be allocated equitably across impacted subjects.\n\nEarly conceptions of this concept focused on protection. More focus today, however, is on access, with groups such as women and minority groups needing to be explicitly included in trials so that they can benefit from the knowledge gained.",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Ethical principles for trial design</span>"
    ]
  },
  {
    "objectID": "running-a-trial/ethical-principles-for-trial-design.html#respect-for-law-and-public-interest",
    "href": "running-a-trial/ethical-principles-for-trial-design.html#respect-for-law-and-public-interest",
    "title": "37  Ethical principles for trial design",
    "section": "37.4 Respect for Law and Public Interest",
    "text": "37.4 Respect for Law and Public Interest\nThe Menlo report described respect for law and public interest as having the following components:\n\nEngage in legal due diligence\nBe transparent in methods and results\nBe accountable for actions.\n\nThe Belmont report took respect for law and public interest to be part of beneficence, but the Menlo report argues it deserves explicit consideration. It extends beyond the participants to society and law more generally.\n\n37.4.1 An example\n\nFor one week in January 2012, approximately 700,000 Facebook users were placed in an experiment to study “emotional contagion,” the extent to which a person’s emotions are impacted by the emotions of the people with whom they interact. … Participants in the Emotional Contagion experiment were put into four groups: a “negativity-reduced” group, for whom posts with negative words (e.g., sad) were randomly blocked from appearing in the News Feed; a “positivity-reduced” group, for whom posts with positive words (e.g., happy) were randomly blocked; and two control groups, one for the positivity-reduced group and one for the negativity-reduced group. The researchers found that people in the positivity-reduced group used slightly fewer positive words and slightly more negative words relative to the control group. Likewise, they found that people in the negativity-reduced group used slightly more positive words and slightly fewer negative words. Thus, the researchers found evidence of emotional contagion …\nSalganik (2018) Bit by Bit: Social Research in the Digital Age\n\nUsing the principles above, what ethical questions arise in that experiment?\n\n\n\n\nBertrand, M., and Mullainathan, S. (2004). Are Emily and Greg More Employable Than Lakisha and Jamal? A Field Experiment on Labor Market Discrimination. American Economic Review, 94(4), 991–1013. https://doi.org/10.1257/0002828042002561\n\n\nKenneally, E., and Dittrich, D. (2012). The Menlo Report: Ethical Principles Guiding Information and Communication Technology Research.\n\n\nSalganik, M. (2018). Bit by Bit. Princeton University Press. https://press.princeton.edu/books/paperback/9780691196107/bit-by-bit\n\n\nThe National Commission for the Protection of Human Subjects of Biomedical and Behavioral Research. (1979). The Belmont Report. U.S. Department of Health, Education, & Welfare, USA. https://www.hhs.gov/ohrp/regulations-and-policy/belmont-report/read-the-belmont-report/index.html",
    "crumbs": [
      "Running a trial",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Ethical principles for trial design</span>"
    ]
  },
  {
    "objectID": "statistical-power/statistical-power.html",
    "href": "statistical-power/statistical-power.html",
    "title": "Statistical power, error control, and effect sizes",
    "section": "",
    "text": "In this topic’s content, we will explore some elements related to statistical power. The concept of the statistical power of a trial refers to the probability of detecting a difference between study groups when such a difference exists. The power is determined by several factors, including the magnitude of the effect, the study design, and the sample size.\nIn the next few pages we will discuss:\n\nType I and Type II errors\nStatistical power\nPower analysis\nEffect sizes",
    "crumbs": [
      "Statistical power, error control, and effect sizes"
    ]
  },
  {
    "objectID": "statistical-power/type-i-and-type-ii-errors.html",
    "href": "statistical-power/type-i-and-type-ii-errors.html",
    "title": "38  Type I and Type II errors",
    "section": "",
    "text": "Statistical tests are often thought of in terms of the errors they can generate.\nThe first error is where the test rejects a null hypothesis that is true. You find an effect where none exists. This is known as a Type I error, or false positive.\nWe set the rate at which Type I errors occur. The significance level \\alpha is the rate of Type I errors. If we use a significance level of 0.05, we have a 5% probability of rejecting the null hypothesis when it is true, generating a Type I error.\nThe second error is when we fail to reject a false null hypothesis. We do not find an effect where one exists. This is known as a Type II error, or false negative.\nThe Type II error rate is unknown, but can be calculated if we make a number of assumptions. We will examine this in the following pages. The type II error rate is denoted by \\beta.\nThe relationship between these errors and correct inference is shown in the following table.\n\n\n\n\n\n\n\n\n\nNull hypothesis (H_0) is true\nNull hypothesis (H_1) is false\n\n\n\n\nDon’t reject H_0\nTrue negative. Probability = 1-\\alpha\nType II error (false negative). Probability = \\beta\n\n\nReject H_0\nType I error (false positive). Probability = \\alpha\nTrue positive. Probability = 1-\\beta\n\n\n\nThe following diagrams provide another view on these errors.\nAs per our running example, suppose we are estimating two sample means for how many people submit their tax return on time. We want to know whether the difference between them represents a true effect of the intervention. Let us suppose that the null hypothesis is true and there is no effect.\nAs we have discussed, the estimate of the effect is with error. Our estimate may vary from the true value. That estimate will fall within a probability distribution of mean \\mu and standard deviation \\frac{\\sigma}{\\sqrt{n}}. The curve below represents that probability distribution.\nWhen we set \\alpha=0.05, we are setting a critical value such that there is a 0.05 chance that the estimate will be above the critical value, despite the null hypothesis being true. The red shaded area is the probability of type I error.\n[For this version of the diagram, only show the bell curve on the left. Remove the curve on the right and the shaded green area. Change “Any mean” to “Critical value”.]\n\nLet us now assume there is an effect of our intervention. The alternative hypothesis is true.\nOur estimate of this effect will again be with error, falling within a probability distribution of mean \\mu and standard deviation \\frac{\\sigma}{\\sqrt{n}}, but this time with \\mu representing a positive effect. This is represented by the curve on the right.\nAs you can see in the diagram, there is a probability that even if the effect is true, the measured value of the effect will fall below the critical value. You will fail to reject the null even though the alternative hypothesis is true. The green shaded area represents this probability of a Type II error.\n[Tweaks to make to diagram: change “Any mean” to “Critical value”; delete “Null” and “Theoretical non-null value” with \\bar{x}_1]\n\nVersions of diagram: 1. As in week 3.4 - normal distribution 2. As in week 3.5 - critical value 3. As here - version just null hypothesis curve 4. As here - version both curves 5. (for 4.3) Shifting H1 curve to the left or right - illustrate more/less chance of type II error if small/large effect size 6. (for 4.3) Larger sample size - reduce chance of type II error - bell curves getting taller and narrower, so green area shrinks 7. Trade-off between type 1 and 2 errors - changing significance level - slider??",
    "crumbs": [
      "Statistical power, error control, and effect sizes",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Type I and Type II errors</span>"
    ]
  },
  {
    "objectID": "statistical-power/type-i-error-control.html",
    "href": "statistical-power/type-i-error-control.html",
    "title": "39  Type I error control",
    "section": "",
    "text": "39.1 Multiple comparisons\nWe set the rate of Type 1 errors through the significance level. A standard significance level of \\alpha=0.05 gives a 5% false positive rate if the null hypothesis is true.\nThere are many recent arguments that \\alpha should be smaller than 0.05, such as Benjamin et al’s (2018) argument that \\alpha should be set at 0.005. Many of these relate to the replication crisis that we will cover this later in this course. We simply want to generate less false positives.\nOne scenario where there is a longer history of using a smaller alpha is where you are testing multiple hypotheses. This could arise because you have multiple treatment groups or because you are measuring many potential outcomes.\nIf you are testing many hypotheses, it becomes increasingly likely that at least one of them will meet the statistical threshold merely by chance. If you conduct 20 tests and all of null hypothesis for each test is true, you expect one false positive.\nA common correction applied to \\alpha in instances of multiple comparisons is the Bonferroni correction. If you are testing m hypotheses, set the significance level for each hypothesis at \\frac{\\alpha}{m}. The Bonferroni correction is conservative in that it decreases the family-wise error rate - which is the probability of making one or more false discoveries - to below 0.05.\nA Bonferroni type correction is typically applied where there is large-scale multiple testing. In genomic association studies, where they are testing of the order of a million genetic variants, they will normally set the significance level at 5x10^{-8}\nA smaller significance level and associated higher critical value means, however, that we will get a higher rate of Type II errors. We will discuss this in the following pages.",
    "crumbs": [
      "Statistical power, error control, and effect sizes",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Type I error control</span>"
    ]
  },
  {
    "objectID": "statistical-power/type-i-error-control.html#replication-data-sets",
    "href": "statistical-power/type-i-error-control.html#replication-data-sets",
    "title": "39  Type I error control",
    "section": "39.2 Replication data sets",
    "text": "39.2 Replication data sets\nAn alternative method of Type I error control is use of a replication sample, which is a subset of the data that is excluded from the initial analysis. If there are any significant results in the first analysis, testing for those hypotheses that were significant in the first is conducted on the replication sample.\nThis approach is common in machine learning applications, but is starting to become more common in statistical analysis. However, it is rarely used in experimental analysis.\nWe will cover the concept of replication more broadly in week 6.",
    "crumbs": [
      "Statistical power, error control, and effect sizes",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Type I error control</span>"
    ]
  },
  {
    "objectID": "statistical-power/statistical-power-content.html",
    "href": "statistical-power/statistical-power-content.html",
    "title": "40  Statistical power",
    "section": "",
    "text": "40.1 Calculating power\nThe probability of a Type II error is \\beta, the probability that you will not reject the null hypothesis when it is false.\nA related concept is power, which is the probability that you will reject the null hypothesis when it is false. Power is equal to 1 - \\beta.\nQuestions related to the Type II error rate for a trial are typically framed in terms of power. How can I design my trial so that it has enough power to reject the null hypothesis in circumstances where it is not true? Alternatively, how can I increase the probability that my trial will achieve statistical significance when the null hypothesis is not true?\nStatistical power can be calculated with the following variables:\nThe significance level: The stricter the significance level that you use in your experiment, the lower the power of the experiment. There is a trade-off between Type I and Type II errors. As you decrease the probability of false positives by using a stricter significance level, you also decrease the probability of true positives. You should not relax the significance level to achieve power, but should be aware of the consequences of making it more strict.\nThe effect size: What is the magnitude of the effect of your intervention? The larger the effect size, the more power the experiment has to detect an effect and generate a statistically significant outcome.\nUnfortunately, as you have not yet run the experiment, you do not know what the effect size is. If there are other studies with which parallels can be drawn, you can make an educated guess as to its likely size. You might generate your estimate through a literature review, pilot studies or other related experiments.\nEstimates of the effect size should be conservative. As we will see later in this unit, the experimental literature is full of over-exaggerated effect sizes. An overestimate of the effect size will overestimate the power of the experiment.\nYou also need to understand the variability of the effect size (such as its standard deviation in the population of interest) to calculate power. A highly-variable effect will have a higher standard error in its measurement, so more probability of getting an extreme result that generates an error.\nAs a result, when an effect size estimate is made, it it typically made as a standardised effect size. This is the magnitude of the effect divided by the standard deviation. How many standard deviations is the effect?\nYou don’t have the ability to increase the effect size. However, variation in the effect size is affected by things such as measurement error. More precise measurement can increase power.\nThe sample size: Power is a function of the sample size of the experiment. It is the factor over which you have the most control.\nIncreasing the sample size reduces the standard error of your estimated effect size. This means that, if the alternative hypothesis is true, you will have a greater probability of detecting an effect of any given size. This can be seen in the following diagram: a larger sample size results in lower standard error, which is reflected in a narrower probability distribution for your estimate.\n[ADD GRAPHIC OF NORMAL DISTRIBUTION GETTING NARROWER]\nBeyond increasing power, larger samples have other benefits such as reducing the extent to which we overestimate the effect size. We will discuss this more in coming material.\nPower is calculated by asking, given the critical value implied by the chosen significance level, what effect size is required such that 80% of the probability distribution for the estimated effect size will be above the critical value.",
    "crumbs": [
      "Statistical power, error control, and effect sizes",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Statistical power</span>"
    ]
  },
  {
    "objectID": "statistical-power/statistical-power-content.html#optional-reading",
    "href": "statistical-power/statistical-power-content.html#optional-reading",
    "title": "40  Statistical power",
    "section": "40.2 Optional reading",
    "text": "40.2 Optional reading\nSchimmack (2017) “Reconstruction of a Train Wreck: How Priming Research Went Off the Rails”, Replicability-Index, https://replicationindex.com/2017/02/02/reconstruction-of-a-train-wreck-how-priming-research-went-of-the-rails/ (Note Daniel Kahneman’s response in the comments.)",
    "crumbs": [
      "Statistical power, error control, and effect sizes",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Statistical power</span>"
    ]
  },
  {
    "objectID": "statistical-power/power-analysis.html",
    "href": "statistical-power/power-analysis.html",
    "title": "41  Power analysis",
    "section": "",
    "text": "41.1 Power determination approach\nOnce you have all the ingredients for a power calculation, how do you actually conduct it? Many statistical packages have sample size function. Free tools are also available, such as G*Power.\nThere are two common approaches for conducting power analysis.\nThe first approach begins with an assumption about the effect size the intervention produces, and the aim is to compute the power they will have to detect that effect with given sample size.\nFor example, suppose that a team of researchers is planning a study to detect the effect of a school-level intervention aimed at improving math achievement for third graders. They plan to randomise schools to receive either the treatment or continue with the current protocol. Pilot studies and available theory suggest that a practically significant effect would entail a standardized effect size of 0.20; that is, a mean difference equivalent to 0.20 in units of the population standard deviation of the outcome (more about this on a following page). Thus the researchers want to plan the study to be able to detect an effect of at least 0.20 standard deviation units. In this case, the effect size is already determined, and the researchers are interested in calculating the sample size necessary to achieve the power of 0.80. Of course, this process can be repeated for a range of effect sizes.",
    "crumbs": [
      "Statistical power, error control, and effect sizes",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Power analysis</span>"
    ]
  },
  {
    "objectID": "statistical-power/power-analysis.html#effect-size-approach",
    "href": "statistical-power/power-analysis.html#effect-size-approach",
    "title": "41  Power analysis",
    "section": "41.2 Effect size approach",
    "text": "41.2 Effect size approach\nThe second approach begins with a desired level of power and the aim is to compute the minimum effect size that can be detected at that level of power for any given sample size. This approach can, of course, be replicated at any given level of power. The minimum detectable effect size is the smallest true effect that can be detected for a specified level of power and significance level for any given sample size.\nFor example, suppose that another team of researchers is studying a whole-school reform model. They plan to randomise schools to either the new reform model or current conditions. Because of financial considerations, the team can only recruit 50 schools and 100 students within each school. The sample size is set, thus the researchers are trying to determine the smallest effect size they can detect with the pre-specified sample size.\nThe power determination and effect size approaches represent two ways to conduct a power analysis. However, both approaches yield the same conclusions. That is, a power analysis could be conducted using either approach, and ultimately the same conclusions would be reached.\nThe two graphs below graphs, drawn with the G*Power software, show how power and the minimum detectable effect size change with sample size. Both graphs assume a significance level of 0.05. The first graph uses the power determination approach and plots power against sample size for an effect size of 0.2. It shows that power increases with sample size. It also shows that we have more power for the same sample size when the effect size is larger. The second graph uses the effect size approach and plots the effect size against the sample size for 90% power. It shows that for a given level of power, we can detect a smaller effect size when the sample size is larger.",
    "crumbs": [
      "Statistical power, error control, and effect sizes",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Power analysis</span>"
    ]
  },
  {
    "objectID": "statistical-power/power-analysis.html#pre--and-post-experiment-power-analysis",
    "href": "statistical-power/power-analysis.html#pre--and-post-experiment-power-analysis",
    "title": "41  Power analysis",
    "section": "41.3 Pre- and post-experiment power analysis",
    "text": "41.3 Pre- and post-experiment power analysis\nPower calculations should be done before an experiment. It is used to determine what sample size is required to obtain the requisite power. A common practice is obtaining a sample size sufficient to achieve 80% power, although there is little reason to limit yourself to that level of power if you can increase power at a modest cost.\nPower calculations are also often done after an experiment. You will see in the literature that post-experiment power calculations are regularly used to justify that the experiment had sufficient power, with the effect size found in the experiment as the basis for the power calculation. This is poor practice, as a significant result in an underpowered experiment will tend to exaggerate the effect size. If you then use this exaggerated effect to calculate power, it gives the impression that the experiment was adequately powered. We will discuss this exaggeration of effect size later in the unit.\nIf a power calculation is done after the experiment, it should only be done using a well-grounded assumed effect size, not the effect size observed in the experiment.\nAn alternative use of post-experiment power calculations is to examine whether published experiments should show the proportion of significant results that they do. If a set of experiments has an average power of 80%, only 80% should find a statistically significant effect. A higher proportion suggests publication bias (a topic later in the unit).",
    "crumbs": [
      "Statistical power, error control, and effect sizes",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Power analysis</span>"
    ]
  },
  {
    "objectID": "statistical-power/power-analysis.html#optional-reading",
    "href": "statistical-power/power-analysis.html#optional-reading",
    "title": "41  Power analysis",
    "section": "41.4 Optional reading",
    "text": "41.4 Optional reading\nGlennerster, R., & Takavarasha, K. (2013). Statistical Power. Running Randomized Evaluations: A Practical Guide (pp. 241–297). Princeton University Press.",
    "crumbs": [
      "Statistical power, error control, and effect sizes",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Power analysis</span>"
    ]
  },
  {
    "objectID": "statistical-power/an-example-power-analysis.html",
    "href": "statistical-power/an-example-power-analysis.html",
    "title": "42  An example power analysis",
    "section": "",
    "text": "42.1 References\nBelow, I run a power analysis for Study 1 in Dietvorst, Simmons and Massey’s (2015) paper on algorithm aversion. The experiment tested the hypothesis that “seeing the model perform, and therefore err, would decrease participants’ tendency to bet on it rather than the human forecaster, despite the fact that the model was more accurate than the human.”\nExperimental participants were given a judgment task, with one group allowed to see the algorithm in action before undertaking the task. Participants were given the option of using the algorithm’s predictions or their own. Those who had seen the algorithm perform were less likely to use it in the task.\nFirst I run a power analysis using data from the original experiment. Post-experiment power analysis should not be done to justify the sample size in the original study, as any significant effect in an underpowered study is likely to be exaggerated in magnitude. If you then use this exaggerated effect to calculate power, it may give the impression that the experiment was adequately powered.\nHowever, this post-experiment analysis may still provide some information about the study’s robustness. It also provides a starting point for further analysis where we adjust the effect size within a plausible range to test how sensitive the assessment of power is to the effect size.\nTo calculate power, we need to know the sample size (per group) and the proportion choosing the model in each group.\nIn Dietvorst et al (2015) Study 1, the smallest group had 90 members. In those groups, 59 of 91 in the control group chose the model, and 23 of 90 who had seen the model perform chose the model.\nThe power of the test was over 99.9%.\nHowever, this effect size may not be representative. The other studies in Dietvorst et al (2015) had smaller effect sizes, as did the replication of Study 3b by Jung and Seiter (2021).\nThe effect size of Dietvorst et al (2015) Study 3a was 16%, with 57% in the control and 41% in the model group choosing the model. Assuming that is the true effect size in Study 1, we would have the following power.\nIf that effect size was the true effect size, that gives only 57% power.\nSimilarly, Jung and Seiter (2021) found that 67% of the control group and 47% of the treatment group chose the model. This is a smaller effect size than Study 1, but larger than that is Study 3b of Dietvorst et al. Again, if we assumed that is the true effect size in Study 1, we would have the following power.\nThis results in a power of 77%, which is below the common “rule of thumb” of 80% power.\nFor caution, let us assume that the smallest effect size, that in Dietvorst et al (2015) Study 3a is representative of the true effect size for Study 1. We can then calculate the sample size required to achieve 90% power as:\nNinety per cent power would require a sample of 200.\nDietvorst, Simmons and Massey (2015) “Algorithm aversion: People erroneously avoid algorithms after seeing them err”, Journal of Experimental Psychology: General, 144, 114–126, https://doi.org/10.1037/xge0000033\nJung and Seiter (2021) “Towards a better understanding on mitigating algorithm aversion in forecasting: An experimental study”, Journal of Management Control, 32(4), 495–516, https://doi.org/10.1007/s00187-021-00326-3",
    "crumbs": [
      "Statistical power, error control, and effect sizes",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>An example power analysis</span>"
    ]
  },
  {
    "objectID": "statistical-power/effect-sizes.html",
    "href": "statistical-power/effect-sizes.html",
    "title": "43  Effect sizes",
    "section": "",
    "text": "43.1 Cohen’s d\nStatistical significance is not the same as practical importance. A statistically significant result may not be large enough to matter in practice. You are interested not just in whether treatment affects people, but also in how much.\nFor example, which is the more interesting result? A statistically significant experimental treatment that could boost the financial wellbeing of all Australians by $2 (p=0.04). Or a non-significant experimental treatment that could boost the financial wellbeing of all Australians by $1000 (p=0.06)?\nEffect sizes from experiments should be interpreted and reported with caution. While often reported as a point estimate, we can provide a confidence interval around the estimated effect size. Effect size with a p-value marginally below 0.05 will have a confidence interval with a lower end only marginally above 0.\nOne common way in which effect sizes are talked about is “Cohen’s d”. Cohen’s d is defined as the difference of two means divided by the standard deviation of the data. It is calculated as:\nd=\\frac{\\bar{x}_1-\\bar{x}_0}{s}=\\frac{\\mu_1-\\mu_0}{s}\nwhere s is the pooled standard deviation of the data (you don’t have to know how to calculate that.)\nCohen’s d has the benefit of translating effect sizes in different experiments onto a common scale. You can speak of how many standard deviations an effect size is.\nIt is common to label different sizes of Cohen’s d. A Cohen’s d of 0.2 is typically called a small effect, 0.5 is medium, and 0.8 is large. When people are calculating power for an experiment, they will often think in terms of whether the effect is small, medium or large, and use the associated number. One way this can mislead, however, is that many effect sizes in the social sciences are far less than 0.2.",
    "crumbs": [
      "Statistical power, error control, and effect sizes",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Effect sizes</span>"
    ]
  },
  {
    "objectID": "statistical-power/effect-sizes.html#summarising-effect-sizes",
    "href": "statistical-power/effect-sizes.html#summarising-effect-sizes",
    "title": "43  Effect sizes",
    "section": "43.2 Summarising effect sizes",
    "text": "43.2 Summarising effect sizes\nOne place you often see wonderfully transparent communication of effect sizes are in meta-analyses (summaries of the literature) or multi-lab replications. Below is one example. (Note that the effect sizes are standardised as Cohen’s d.)\n\n43.2.1 The jam experiment\nOn two Saturdays in a California supermarket, Mark Lepper and Sheena Iyengar (2000) set up tasting displays of either six or 24 jars of jam. Consumers could taste as many jams as they wished, and if they approached the tasting table they received a $1 discount coupon to buy the jam.\nFor attracting initial interest, the large display of 24 jams did a better job, with 60 per cent of people who passed the display stopping. Forty per cent of people stopped at the six jam display. But only three per cent of those who stopped at the 24 jam display purchased any of the jam, compared with almost 30 per cent who stopped at the six jam display.\nThis result has become the classical example of the “paradox of choice”. More choice can lead us to fail to make a choice.\nLater, Benjamin Scheibehenne and friends (2010) surveyed the literature on the choice overload hypothesis. This chart is a plot of the effect sizes across the literature. Of those that are significant - that is, those for which the 95% confidence interval does not contain zero - they have a large point estimate of effect size, yet a 95% confidence interval barely excluding zero.\nLooking across these experiments, in some cases, choice increases purchases. In others it reduces them. Scheibehenne and friends determined that the mean effect size of changing the number of choices across the studies was effectively zero.",
    "crumbs": [
      "Statistical power, error control, and effect sizes",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Effect sizes</span>"
    ]
  },
  {
    "objectID": "statistical-power/effect-sizes.html#references",
    "href": "statistical-power/effect-sizes.html#references",
    "title": "43  Effect sizes",
    "section": "43.3 References",
    "text": "43.3 References\nBenjamin et al (2018) “Redefine statistical significance”, Nature Human Behaviour, 2, 6-10, https://doi.org/10.1038/s41562-017-0189-z\nIyengar, S. S., & Lepper, M. R. (2000). When choice is demotivating: Can one desire too much of a good thing?. Journal of Personality and Social Psychology, 79(6), 995–1006.\nScheibehenne, B., Greifeneder, R., & Todd, P. M. (2010). Can There Ever Be Too Many Options? A Meta-Analytic Review of Choice Overload. The Journal of Consumer Research, 37(3), 409–425.",
    "crumbs": [
      "Statistical power, error control, and effect sizes",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Effect sizes</span>"
    ]
  },
  {
    "objectID": "interpreting-trial-results/interpreting-trial-results.html",
    "href": "interpreting-trial-results/interpreting-trial-results.html",
    "title": "Interpreting trial results",
    "section": "",
    "text": "In this part, I will examine the challenges of interpreting and scaling trial results. How do we read and interpret published trial results? Can we trust them? What happens when we scale our or other’s interventions, move from lab to field, or change the context?\nBy the end of this part, you will be able to answer the following questions:\n\nWhat is the replication crisis and its causes?\nHow generalisable are trials to new contexts?",
    "crumbs": [
      "Interpreting trial results"
    ]
  },
  {
    "objectID": "interpreting-trial-results/navigating-the-literature.html",
    "href": "interpreting-trial-results/navigating-the-literature.html",
    "title": "44  Navigating the literature",
    "section": "",
    "text": "It may be useful in your journey of running a trial to conduct a literature review of existing articles. Literature refers here to a collection of published materials on a particular area of research or topic, such as books and journal articles. A literature review seeks to summarize the information that existing studies have gathered on these issues. It will help you answer the following questions:\n\nWhat studies have already been done in this area?\nAre there gaps in the evidence?\nWhat are some of the general lessons that emerge from the many different studies?\n\nYou can search for articles using existing databases through your institutional library or with sources such as Google Scholar.\nOnce you have selected the studies, it is important to be able to read or skim-read them in an educated way. Over the following pages, we will discuss this more in-depth and provide you with examples.",
    "crumbs": [
      "Interpreting trial results",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Navigating the literature</span>"
    ]
  },
  {
    "objectID": "interpreting-trial-results/reading-research.html",
    "href": "interpreting-trial-results/reading-research.html",
    "title": "45  Reading research",
    "section": "",
    "text": "45.1 How to read the tables\nAs part of your review of the literature, it is important to be able to understand the research articles that describe the motivation and results of trials.\nThey will often be organised as follows:\nLet’s work through a concrete example. Please download and start reading the following article:\nThe following video will help you understand how to read the tables in research papers.",
    "crumbs": [
      "Interpreting trial results",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Reading research</span>"
    ]
  },
  {
    "objectID": "interpreting-trial-results/reading-research.html#how-to-read-the-tables",
    "href": "interpreting-trial-results/reading-research.html#how-to-read-the-tables",
    "title": "45  Reading research",
    "section": "",
    "text": "NoteActivity\n\n\n\nUsing the comment box below, discuss whether, based on this study you think high schools in Australia should allow laptops in the classroom. Please explain your answer.\nNote: This was originally an interactive discussion in Canvas. Consider the following when forming your response:\n\nWhat were the specific findings of the Carter et al. study?\nWhat was the context of the study (United States Military Academy)?\nHow might this context differ from Australian high schools?\nWhat are the limitations of generalizing from this specific trial?",
    "crumbs": [
      "Interpreting trial results",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Reading research</span>"
    ]
  },
  {
    "objectID": "interpreting-trial-results/the-replication-crisis.html",
    "href": "interpreting-trial-results/the-replication-crisis.html",
    "title": "46  The replication crisis",
    "section": "",
    "text": "46.1 Example 1: Disfluency\nThere is a large experimental literature in behavioural science. It is the product of over 50-years of experimentation, initially conducted in the lab, but increasingly today in the field.\nHowever, over the last decade, there have been increased questions about the reliability of the published results of randomised controlled trials. Replication is the ability to re-perform the experiment and collect new data. An experiment is said to replicate if it returns a similar result.\nMany results have failed to replicate, and they have failed at a rate that suggests this is a systemic issue.\nBelow are two examples of replication failures, together with large scale replication projects conducted in psychology and economics.\nAdam Alter and friends (Alter et al., 2007) exposed 40 students to two versions of the cognitive reflection task. One question in the cognitive reflection task is the following:\nThe two versions differed in that one used small light grey font that made the questions hard to read. Those exposed to the harder to read questions achieved higher scores. Slowing people down made them do better.\nEight years later Frederick Meyer and friends (Meyer et al., 2015) conducted a replication across many labs involving thousands of people. Whereas the original study found a large effect, the replication found nothing. You can see the original small sample outlier on the bottom left of the chart.",
    "crumbs": [
      "Interpreting trial results",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>The replication crisis</span>"
    ]
  },
  {
    "objectID": "interpreting-trial-results/the-replication-crisis.html#example-1-disfluency",
    "href": "interpreting-trial-results/the-replication-crisis.html#example-1-disfluency",
    "title": "46  The replication crisis",
    "section": "",
    "text": "A bat and a ball cost $1.10 in total. The bat costs $1.00 more than the ball. How much does the ball cost?",
    "crumbs": [
      "Interpreting trial results",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>The replication crisis</span>"
    ]
  },
  {
    "objectID": "interpreting-trial-results/the-replication-crisis.html#example-2-honest-priming",
    "href": "interpreting-trial-results/the-replication-crisis.html#example-2-honest-priming",
    "title": "46  The replication crisis",
    "section": "46.2 Example 2: Honest priming",
    "text": "46.2 Example 2: Honest priming\nVerschuere and friends (Verschuere et al., 2018) replicated a highly cited experiment by Nina Mazar, On Amir and Dan Ariely (Mazar et al., 2008) across multiple labs. The abstract of the paper reads as follows:\n\nThe self-concept maintenance theory holds that many people will cheat in order to maximize self-profit, but only to the extent that they can do so while maintaining a positive self-concept. Mazar, Amir, and Ariely (2008, Experiment 1) gave participants an opportunity and incentive to cheat on a problem-solving task. Prior to that task, participants either recalled the Ten Commandments (a moral reminder) or recalled 10 books they had read in high school (a neutral task). Results were consistent with the self-concept maintenance theory. When given the opportunity to cheat, participants given the moral-reminder priming task reported solving 1.45 fewer matrices than did those given a neutral prime (Cohen’s d = 0.48); moral reminders reduced cheating. Mazar et al.’s article is among the most cited in deception research, but their Experiment 1 has not been replicated directly. This Registered Replication Report describes the aggregated result of 25 direct replications (total N = 5,786), all of which followed the same preregistered protocol. In the primary meta-analysis (19 replications, total n = 4,674), participants who were given an opportunity to cheat reported solving 0.11 more matrices if they were given a moral reminder than if they were given a neutral reminder (95% confidence interval = [−0.09, 0.31]). This small effect was numerically in the opposite direction of the effect observed in the original study (Cohen’s d = −0.04).\n\nFigure 2 from the paper demonstrates the result:",
    "crumbs": [
      "Interpreting trial results",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>The replication crisis</span>"
    ]
  },
  {
    "objectID": "interpreting-trial-results/the-replication-crisis.html#estimating-the-reproducibility-of-psychological-science",
    "href": "interpreting-trial-results/the-replication-crisis.html#estimating-the-reproducibility-of-psychological-science",
    "title": "46  The replication crisis",
    "section": "46.3 Estimating the reproducibility of psychological science",
    "text": "46.3 Estimating the reproducibility of psychological science\nThese examples are illustrative of a wider problem. The Open Science Collaboration (Open Science Collaboration, 2015) ran replications of 100 experiments previously reported in three top psychology journals. The abstract captures the disappointing result:\n\nReproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47% of original effect sizes were in the 95% confidence interval of the replication effect size; 39% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.\n\nThe results are summarised in this diagram. If the original studies were representative of the results we should expect, the balls should be clustered around the diagonal line. Instead, most are below it, representing the decline in the effect size, with many not significantly different from zero.",
    "crumbs": [
      "Interpreting trial results",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>The replication crisis</span>"
    ]
  },
  {
    "objectID": "interpreting-trial-results/the-replication-crisis.html#replication-in-economics",
    "href": "interpreting-trial-results/the-replication-crisis.html#replication-in-economics",
    "title": "46  The replication crisis",
    "section": "46.4 Replication in economics",
    "text": "46.4 Replication in economics\nThe replication problem is not just in psychology, although economics has not had such high profile failures to date.\nColin Camerer and friends (Camerer et al., 2016) replicated 18 studies from two top economics journals. They found:\n\nThe replicability of some scientific findings has recently been called into question. To contribute data about replicability in economics, we replicated 18 studies published in the American Economic Review and the Quarterly Journal of Economics between 2011 and 2014. All of these replications followed predefined analysis plans that were made publicly available beforehand, and they all have a statistical power of at least 90% to detect the original effect size at the 5% significance level. We found a significant effect in the same direction as in the original study for 11 replications (61%); on average, the replicated effect size is 66% of the original. The replicability rate varies between 67% and 78% for four additional replicability indicators, including a prediction market measure of peer beliefs.\n\nAn interesting aspect to these replications was that a prediction market and survey were run in advance to enable academics to predict whether each of the studies would replicate. As can be seen in the diagram, those studies that replicated were predicted to be more likely to replicate than those that did not, although both the survey and prediction market were slightly optimistic in the probability of replication.\n\n\n\n\n\nAlter, A. L., Oppenheimer, D. M., Epley, N., and Eyre, R. N. (2007). Overcoming intuition: Metacognitive difficulty activates analytic reasoning. Journal of Experimental Psychology: General, 136(4), 569–576. https://doi.org/10.1037/0096-3445.136.4.569\n\n\nCamerer, C. F., Dreber, A., Forsell, E., Ho, T.-H., Huber, J., Johannesson, M., Kirchler, M., Almenberg, J., Altmejd, A., Chan, T., Heikensten, E., Holzmeister, F., Imai, T., Isaksson, S., Nave, G., Pfeiffer, T., Razen, M., and Wu, H. (2016). Evaluating replicability of laboratory experiments in economics. Science, 351(6280), 1433–1436. https://doi.org/10.1126/science.aaf0918\n\n\nMazar, N., Amir, O., and Ariely, D. (2008). The Dishonesty of Honest People: A Theory of Self-Concept Maintenance. Journal of Marketing Research, 45(6), 633–644. https://doi.org/10.1509/jmkr.45.6.633\n\n\nMeyer, A., Frederick, S., Burnham, T. C., Guevara Pinto, J. D., Boyer, T. W., Ball, L. J., Pennycook, G., Ackerman, R., Thompson, V. A., and Schuldt, J. P. (2015). Disfluent fonts don’t help people solve math problems. Journal of Experimental Psychology: General, 144(2), e16–e30. https://doi.org/10.1037/xge0000049\n\n\nOpen Science Collaboration. (2015). Estimating the reproducibility of psychological science. Science, 349(6251), aac4716. https://doi.org/10.1126/science.aac4716\n\n\nVerschuere, B., Meijer, E. H., Jim, A., Hoogesteyn, K., Orthey, R., McCarthy, R. J., Skowronski, J. J., Acar, O. A., Aczel, B., Bakos, B. E., Barbosa, F., Baskin, E., Bègue, L., Ben-Shakhar, G., Birt, A. R., Blatz, L., Charman, S. D., Claesen, A., Clay, S. L., … Yıldız, E. (2018). Registered Replication Report on Mazar, Amir, and Ariely (2008). Advances in Methods and Practices in Psychological Science, 1(3), 299–317. https://doi.org/10.1177/2515245918781032",
    "crumbs": [
      "Interpreting trial results",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>The replication crisis</span>"
    ]
  },
  {
    "objectID": "interpreting-trial-results/replication-and-reproducibility.html",
    "href": "interpreting-trial-results/replication-and-reproducibility.html",
    "title": "47  Replication and reproducibility",
    "section": "",
    "text": "47.1 Reproducibility\nReproducibility is the ability of a different analyst to re-perform the same analysis of experimental data.\nReproducible research is research that provides the full materials require to reproduce academic research. It includes features such as the code and data that was used and details on the computational environment in which it was examined.\nReplication is the ability to re-perform the experiment and collect new data. An experiment is said to replicate if it returns a similar result.\nThe terms reproduction and replication are sometimes used interchangeably, so their precise names don’t matter, but the distinct concepts do.\nEven with the same data, people can come to different conclusions about the hypotheses.\nThey can transform the data in different ways. They can use different tests for the hypotheses. There are many possible analytic approaches. The result is that experimental data does not in itself provide the conclusion. Absent reproducible research, it is not clear what choices were made and the effect of those choices on the conclusion cannot be examined.\nAs one dramatic illustration involved an analysis of data to see whether soccer referees are more likely to give red cards to dark-skinned players than to light-skinned players (Silberzahn et al., 2018). Twenty-nine analyst teams were tasked with answering this question. They found effect sizes ranging from 0.89 (dark-skinned players receive fewer) to 2.93 (dark-skinned players receive almost three times as many). Twenty of the 29 teams found a statistically significant positive effect, with the other nine failing to find a statistically significant relationship.",
    "crumbs": [
      "Interpreting trial results",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Replication and reproducibility</span>"
    ]
  },
  {
    "objectID": "interpreting-trial-results/replication-and-reproducibility.html#statcheck",
    "href": "interpreting-trial-results/replication-and-reproducibility.html#statcheck",
    "title": "47  Replication and reproducibility",
    "section": "47.2 Statcheck",
    "text": "47.2 Statcheck\nOne way to perform a quick robustness check across a paper is by using statcheck. To use statcheck, you upload a pdf, HTML or docx of the paper. Statcheck then extracts details of the tests reported in the paper and checks that the reported numbers are consistent. (An R package for statcheck allows you to run your own statcheck implementation.)\nAs an example, I uploaded the Dietvorst et al. (2015) pdf, but the analysis did not work. I then accessed a HTML download of the paper using the UTS library website. Uploading the HTML version of the paper resulted in the following.\n\nThe results appear consistent with the reported tests.\nStatcheck does not work on all papers and only checks that the tests are consistent with the reported numbers, but it is a quick way to look for red flags.\n\n\n\n\nDietvorst, B. J., Simmons, J. P., and Massey, C. (2015). Algorithm aversion: People erroneously avoid algorithms after seeing them err. Journal of Experimental Psychology: General, 144, 114–126. https://doi.org/10.1037/xge0000033\n\n\nSilberzahn, R., Uhlmann, E. L., Martin, D. P., Anselmi, P., Aust, F., Awtrey, E., Bahník, Š., Bai, F., Bannard, C., Bonnier, E., Carlsson, R., Cheung, F., Christensen, G., Clay, R., Craig, M. A., Dalla Rosa, A., Dam, L., Evans, M. H., Flores Cervantes, I., … Nosek, B. A. (2018). Many Analysts, One Data Set: Making Transparent How Variations in Analytic Choices Affect Results. Advances in Methods and Practices in Psychological Science, 1(3), 337–356. https://doi.org/10.1177/2515245917747646",
    "crumbs": [
      "Interpreting trial results",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Replication and reproducibility</span>"
    ]
  },
  {
    "objectID": "interpreting-trial-results/publication-bias.html",
    "href": "interpreting-trial-results/publication-bias.html",
    "title": "48  Publication bias",
    "section": "",
    "text": "48.1 Analysis to identify publication bias\nIn 2011, the Journal of Personality and Social Psychology (a big, flagship journal) published a paper by Daryl Bem (2011) entitled “Feeling the Future: Experimental Evidence of Anomalous Retroactive Influences on Cognition and Affect.”\nIn the paper, Bem described nine experiments. In the first experiment, participants were shown pictures of two curtains side-by-side on a screen. One had a picture behind it, the other a blank wall. The participants were asked to click on the curtain they felt had the picture behind it. They were then shown if they had selected the correct curtain.\nSome of the pictures shown to the participants were “erotic”. Where there was an erotic picture, participants selected the pictures more often than expected by chance: 53.1% of the time. For non-erotic pictures, the probability of success did not vary significantly from chance. The p-value for selecting the erotic pictures was 0.01, a significant result.\nWould Bem’s research have been published in a top psychology journal if he had not obtained a statistically significant result? Would the journal waste its resources publishing a paper suggesting people do not have ESP?\nThis point is the essence of publication bias. Publication bias occurs where the outcome of an experiment influences whether it is published or not.\nStudies that find significant effects are more likely to be published. This incentivises those conducting experiments to only write up their positive results. Studies that do not generate statistically significant results end up in the file drawer. Ultimately, the published literature ceases to be a representative sample of the evidence. Instead, it is biased.\nWhile it is easy to see the incentives that might generate publication bias, measuring publication bias is more difficult and sometimes controversial.\nOne common way is through the use of a funnel plot, which is used to analyse for publication bias across a literature. The effect sizes of all experiments examining a particular intervention are plotted against the precision of the studies. Precision is usually proxied by study size or the standard error.\nA literature in which all experimental results are published should see a spread of results around the effect size, with smaller or less precise studies having more variation around that point. This results in a funnel shape of results, as in the first diagram.\nWhere there is publication bias, there is often an asymmetry in that the results on one side of the funnel plot (typically the small sample studies that delivered results in the unintended direction) are missing.\nImage from Cressey (2017)\nAsymmetric funnel plots are not definitive of publication bias, and rest on several assumptions such as a lack of systematic link between size of effect and size of study (which there may be if people use a larger sample because they believe the effect is small).",
    "crumbs": [
      "Interpreting trial results",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Publication bias</span>"
    ]
  },
  {
    "objectID": "interpreting-trial-results/publication-bias.html#remedying-publication-bias",
    "href": "interpreting-trial-results/publication-bias.html#remedying-publication-bias",
    "title": "48  Publication bias",
    "section": "48.2 Remedying publication bias",
    "text": "48.2 Remedying publication bias\nA primary method proposed for reducing publication bias is the use of pre-registration. This provides a basis for understanding the full scope of the studies that have been undertaken.\nHowever, this is not a complete remedy as many pre-registered studies are not published and their results not available. That prevents us from obtaining a complete view of the experiments that have been conducted.\n\n\n\n\nBem, D. J. (2011). Feeling the future: Experimental evidence for anomalous retroactive influences on cognition and affect. Journal of Personality and Social Psychology, 100(3), 407–425. https://doi.org/10.1037/a0021524\n\n\nCressey, D. (2017). Tool for detecting publication bias goes under spotlight. Nature. https://doi.org/10.1038/nature.2017.21728",
    "crumbs": [
      "Interpreting trial results",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Publication bias</span>"
    ]
  },
  {
    "objectID": "interpreting-trial-results/p-hacking-and-the-garden-of-forking-paths.html",
    "href": "interpreting-trial-results/p-hacking-and-the-garden-of-forking-paths.html",
    "title": "49  P-hacking and the garden of forking paths",
    "section": "",
    "text": "49.1 P-hacking\nIn conducting their analysis, researchers have many decisions to make. Should they collect more data? How should they process the data? Should they transform it in any way? Should they exclude outliers? What statistical tests will they use? What outcomes will they test? And so on.\nP-hacking occurs where researchers use this flexibility in their data collection, analysis and reporting to inflate their rate of significant findings. Result not significant yet? Collect some more data. Result not significant with that extreme data point? Exclude it. No significant change after 6 months? Try 12. No significant effect for people predicting what picture is behind the curtain? Let’s restrict the analysis to the erotic ones.\nA p-value of 0.05 sets an effective false positive rate of 0.05. You will only see data that extreme 5% of the time if the null hypothesis is true. But if you look at the data in many different ways and select only those arrangements that result in low p-values, the p-value ceases to be informative about the false positive rate. The false positive rate will be much higher (Simmons et al., 2011).\nP-hacking is considered to be a major cause of the replication crisis. Even without publication bias, p-hacking can dramatically inflate the rate of false positives in the literature that will fail to replicate if tested.",
    "crumbs": [
      "Interpreting trial results",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>P-hacking and the garden of forking paths</span>"
    ]
  },
  {
    "objectID": "interpreting-trial-results/p-hacking-and-the-garden-of-forking-paths.html#p-hacking",
    "href": "interpreting-trial-results/p-hacking-and-the-garden-of-forking-paths.html#p-hacking",
    "title": "49  P-hacking and the garden of forking paths",
    "section": "",
    "text": "49.1.1 P-hacking without intent\nP-hacking should not be taken to infer nefarious research practices (Brian Wansink and the Cornell Food Lab excepted (Lee, 2018)). Nor is p-hacking necessarily a case of the researcher making many comparisons and throwing out those results that don’t meet their needs.\nRather, researchers might do a single analysis given their assumptions and the data. However, that single analysis is contingent on the data they see. If the data had been different they might have done a different analysis. There is effectively a large number of comparisons they could have done, and they pick the a more prospective approach. Andrew Gelman and Eric Loken (2013) call this the garden of forking paths. Even though seemingly benign, the selection of the most prospective path effectively inflates the false positive rate.\n\n\n49.1.2 An example\nAndrew Gelman Jennifer Hill and Aki Vehtari (2020) write:\n\nWe demonstrate the last two problems mentioned above — multiple potential comparisons and the statistical significance filter — using the example of a research article published in a leading journal of psychology. The article begins:\n\nEach month many women experience an ovulatory cycle that regulates fertility. Whereas research finds that this cycle influences women’s mating preferences, we propose that it might also change women’s political and religious views. Building on theory suggesting that political and religious orientation are linked to reproductive goals, we tested how fertility influenced women’s politics, religiosity, and voting in the 2012 U.S. presidential election. In two studies with large and diverse samples, ovulation had drastically different effects on single versus married women. Ovulation led single women to become more liberal, less religious, and more likely to vote for Barack Obama. In contrast, ovulation led married women to become more conservative, more religious, and more likely to vote for Mitt Romney. In addition, ovulatory-induced changes in political orientation mediated women’s voting behavior. Overall, the ovulatory cycle not only influences women’s politics, but appears to do so differently for single versus married women.\n\nOne problem here is that there are so many different things that could be compared, but all we see is some subset of the comparisons. Some of the choices available in this analysis include the days of the month characterized as peak fertility, the dividing line between single and married (in this particular study, unmarried but partnered women were counted as married), data exclusion rules based on reports of menstrual cycle length and timing, and the decision of which interactions to study. Given all these possibilities, it is no surprise at all that statistically significant comparisons turned up; this would be expected even were the data generated purely by noise.\n\n\n\n49.1.3 Preventing p-hacking\nThere are many proposed solutions for p-hacking. The major solution is pre-registration of analysis plans. These transparently commit researchers to an approach, meaning that we can then take the p-value as a prima facie indication of seeing data that extreme under the assumption that the null hypothesis is true. These are discussed further on the next page.\n\n\n\n\nGelman, A., Hill, J., and Vehtari, A. (2020). Regression and Other Stories. Cambridge University Press. https://doi.org/10.1017/9781139161879\n\n\nGelman, A., and Loken, E. (2013). The garden of forking paths: Why multiple comparisons can be a problem, even when there is no “fishing expedition” or “p-hacking” and the research hypothesis was posited ahead of time. https://sites.stat.columbia.edu/gelman/research/unpublished/p_hacking.pdf\n\n\nLee, S. M. (2018). Sliced And Diced: The Inside Story Of How An Ivy League Food Scientist Turned Shoddy Data Into Viral Studies. BuzzFeed News. https://www.buzzfeednews.com/article/stephaniemlee/brian-wansink-cornell-p-hacking\n\n\nSimmons, J. P., Nelson, L. D., and Simonsohn, U. (2011). False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant. Psychological Science, 22(11), 1359–1366. https://doi.org/10.1177/0956797611417632",
    "crumbs": [
      "Interpreting trial results",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>P-hacking and the garden of forking paths</span>"
    ]
  },
  {
    "objectID": "interpreting-trial-results/pre-analysis-plans-and-preregistration.html",
    "href": "interpreting-trial-results/pre-analysis-plans-and-preregistration.html",
    "title": "50  Pre-analysis plans and pre-registration",
    "section": "",
    "text": "A pre-analysis plan is a step-by-step plan setting out how you will analyse the data from your experiment. It is written before data collection, or at least before seeing the data.\nComponents of a pre-analysis plan typically include:\n\nA description of the sample that will be used, including proposed sample size and randomisation techniques\nThe hypotheses to be tested\nHow any variables will be constructed (i.e. you may be planning to manipulate data in the analysis, such as by excluding outlier values or transforming it to log levels)\nWhat statistical tests will be used\nHow you will deal with multiple outcomes\n\nCommitment to a pre-analysis plan is required to be able to take p-values at their face value.Why? A p-value is the probability of the data on the assumption that the null hypotheses is true. As we noted before, our measurement of the data is a test statistic such as a z-score.But your measurement of the data relies on the analysis you choose. If you change that analysis based on the data you see (e.g. by excluding outliers or measuring at a different point in time or using a different outcome measure) your p-value is no longer the probability of the data given the null hypothesis is true. The p-value is now the probability of the data given the null hypothesis is true and given you made a series of choices.\nTo put this into more practical terms, all of the choices that are specified in a pre-analysis plan affect the chance of “success” of a trial. The more degrees of freedom you allow yourself in your analysis, the more likely you are to stumble upon a p-value lower than 0.05 by chance. Don’t find anything at 6 months, try 12. Maybe exclude the outlier. And so on.\nThese researcher degrees of freedom are one reason behind the “replication crisis” that we will explore in the third module.\n\n50.0.1 Preregistration\nPreregistration is publication of the pre-analysis plan in a public archive. This acts as a form of commitment to the pre-analysis plan and enables others to verify that you have done the analysis as you stated.\nAs an example, BETA at PM&C pre-registers many of it studies. You can see some of the pre-registered studies here.",
    "crumbs": [
      "Interpreting trial results",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Pre-analysis plans and pre-registration</span>"
    ]
  },
  {
    "objectID": "interpreting-trial-results/type-m-and-type-s-errors.html",
    "href": "interpreting-trial-results/type-m-and-type-s-errors.html",
    "title": "51  Type S and M errors",
    "section": "",
    "text": "51.1 An example\nWhen a null hypothesis is rejected, people tend to report and make decisions based on the point estimate of the sign and magnitude of the effect. We have already highlighted that effect sizes should be treated with caution, but there are alternative ways to examine the properties of this effect size.\nOne of these is the concept of Type S (Sign) and Type M (Magnitude) errors (Gelman and Carlin, 2014).\nA Type S error occurs when the sign of the estimated effect is in the opposite direction to the true effect. The Type S error rate is the probability of the sign being in the opposite direction.\nA Type M error occurs when the magnitude of the estimated effect is much different from (larger than) the true effect. Type M errors are expressed in terms of the expected exaggeration factor, the expected ratio of the size of the estimated effect divided by the size of the underlying effect.\nThese errors tend to occur in low powered studies. As one example, suppose we have an effect size that cannot realistically be more than 2 percentage points, and a standard error of 8 percentage points. The below diagram shows the distribution of estimated effect sizes that would occur with this underlying data.\nFor a statistically significant result, the effect size needs to be ~16 percentage points (around 2 standard deviations greater than zero). This is an 8-fold exaggeration of the true effect size. There is also a 24% probability that, in the case of a significant result, it is in the wrong direction.\nThis shows that the problem with a low-powered study is not just the high probability of a Type II error. The problem is that even if the researcher gets a statistical significant result, the effect size can have a high probability of being massively exaggerated or even in the opposite direction. This is not just a case of bad luck: in the case of a low powered study, the effect size can only be significant if it is exaggerated.\nThe net result of this framework is that low-powered studies will always generate at least one form of error, be that Type II or Type M.\nContinuing the example of the ovulatory cycle and voting, Andrew Gelman, Jennifer Hill and Aki Vehtari (2020) write:",
    "crumbs": [
      "Interpreting trial results",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Type S and M errors</span>"
    ]
  },
  {
    "objectID": "interpreting-trial-results/type-m-and-type-s-errors.html#an-example",
    "href": "interpreting-trial-results/type-m-and-type-s-errors.html#an-example",
    "title": "51  Type S and M errors",
    "section": "",
    "text": "In addition, relative to our understanding of the vast literature on voting behavior, the claimed effects seem implausibly large — a type M error. For example, the paper reports that, among women in relationships, 40% in the ovulation period supported Romney, compared to 23% in the non-fertile part of their cycle. Given that opinion polls find very few people switching their vote preferences during the campaign for any reason, these numbers seem unrealistic. The authors might respond that they don’t care about the magnitude of the difference, just the sign, but (a) with a magnitude of this size, we are talking noise (not just sampling error but also errors in measurement), and (b) one could just as easily explain this as a differential nonresponse pattern: maybe liberal or conservative women in different parts of their cycle are more or less likely to participate in a survey. It would be easy enough to come up with a story about that.\n\n\n\n\n\nGelman, A., and Carlin, J. (2014). Beyond Power Calculations: Assessing Type S (Sign) and Type M (Magnitude) Errors. Perspectives on Psychological Science, 9(6), 641–651. https://doi.org/10.1177/1745691614551642\n\n\nGelman, A., Hill, J., and Vehtari, A. (2020). Regression and Other Stories. Cambridge University Press. https://doi.org/10.1017/9781139161879",
    "crumbs": [
      "Interpreting trial results",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Type S and M errors</span>"
    ]
  },
  {
    "objectID": "interpreting-trial-results/validity.html",
    "href": "interpreting-trial-results/validity.html",
    "title": "52  Validity",
    "section": "",
    "text": "Validity is the extent to which the results of an experiment support a more general conclusion.\nThere are many forms of validity. Here we briefly define four (Salganik, 2018).\nStatistical conclusion validity is the extent to which the experimental statistical analysis was done directly. For example, did the experimenter calculate the p-values correctly?\nInternal validity is the extent to which the experimental treatment is actually responsible for the change in value of the dependent variable. You are able to link cause and effect while controlling for the effect of outside variables (usually by randomisation). Internal validity also concerns whether the experimental procedures were performed correctly. Internal validity tends to be higher in the lab, although the failure of many lab experiments to replicate indicates a problem of low validity.\nConstruct validity concerns whether the data matches the theoretical constructs. If you believe that a social norm triggers someone to pay their tax on time, does your treatment manipulate social norms while holding other constructs (such as prompts) constant?\nExternal validity is the extent that experimental findings can be generalised to the population from which the participants in the experiment were drawn. Field experiments tend to provide higher external validity than those constrained to the lab.\n\n52.0.1 The validity of lab and field experiments\nA core driver of whether a lab or field experiment is more appropriate is whether internal or external validity are more important.\nJohn List and Omar Al-Ubaydli (2014) provide one perspective on this trade-off:\n\nBob wants to purchase Susan’s mug for $5, but they live far apart and so he will need to send a check. The mug is worth $3 to Susan and $9 to Bob, implying a societal surplus of $9 - $3 = $6 if the transaction occurs. However, if Bob sends the money first, will Susan send the mug? If Susan sends the mug first, will Bob send the money? Signing a legally enforceable contract would facilitate the trade.\nHowever, what if property rights are poorly enforced (e.g., if they live in different countries)? Then their fear may result in them forgoing the trade and the surplus of $6. …\n\nHow can we test whether we could expect the trade to occur? Perhaps we could look at the trust game?\n\nThis is what Berg et al. (1995) did using the ‘trust game’ – a microcosm of Bob and Susan’s quandary. The sender starts with $10 and the responder starts with $0. The sender can send any amount to the responder, retaining the remainder. The responder receives triple whatever the sender transfers. The responder then decides how to divide the tripled amount between the two, terminating the game. For example, if the sender transfers $4, the sender retains $6 and the responder receives $4 x 3 = $12; finally, the responder can choose to return anywhere between $0 and $12.\nThe desirable outcome is for the sender to transfer all $10, and for the responder to return at least $10 ($15 under egalitarianism). This mimics Bob and Susan trading the mug. It leaves the two with $30 between them – much more than just $10 with the sender. But the sender may doubt the responder’s trustworthiness. The responder may just choose to pocket whatever the sender transfers since the sender has no legal recourse. Anticipating this, like Bob and Susan, the sender may just decide to avoid transacting, retaining the $10.\nBerg et al.’s results suggest that such fears were potentially ill-founded. Even when the trust game was played with anonymous strangers, on average, senders would send $5.16, and responders would return $4.66. The authors appealed to altruism to explain the results, i.e., the players feel bad about the other party receiving a low payoff, motivating behaviour closer to what emerges under fully enforced property rights. Subsequent studies (Fehr et al. 1993) have confirmed these results, and interviews reveal that participants’ altruism is their most common stated motivation.\n\nBut should this experiment provide comfort to Bob? List and Al-Ubaydli describe a related field experiment.\n\nI went to professional sports memorabilia markets and recruited professional traders to play laboratory trust games. Like the literature, I found a modest positive causal effect of property rights.\nI then ran a complementary field experiment. As a sports card enthusiast, I was aware that the market was awash with player cards of different quality (grade), and that the professional sellers were better at discerning grade than your average fan milling around the exhibition. Critically, an individual requesting a high-grade card and receiving a low-grade card was rarely aware at the time of the transaction and, if they found out subsequently, they had no legal recourse. Thus, buying a card required a buyer to trust – like Bob buying the mug – the sender in the trust game. I recruited some archetypal sports fans to go to professional sellers and request a specific card at a specific grade in exchange for a predetermined price, without revealing to the seller that this was an experiment. If traders were completely selfish, then they would return the lowest grade of card whatever price was offered to them, confirming the need to enforce property rights. Alternatively, if the traders behaved ‘altruistically’ like the laboratory experiment participants, then they should offer higher quality when offered higher prices.\nThe results were pretty grim for anyone who believes in the humanity of sports card dealers – card quality was insensitive to price offers. Note that these were the same traders who had apparently exhibited altruistic tendencies in the preceding laboratory experiment. …\nThese results painted a bleaker picture of anonymous trade in the absence of property rights. Were Bob and Susan to read the entire literature, what would be the epistemologically ideal way for them to update their beliefs on the causal effect of property rights on trading behaviour? Which results generalise to their setting more accurately – laboratory or field?\n\n\n\n52.0.2 The validity of randomised controlled trials\nMuch of this unit has focussed on how we can use randomised controlled trials to obtain accurate measures of treatment effects, as opposed to delving into how the results could be used. This is effectively a focus on internal validity.\nThis unit’s focus is matched in much of the literature about randomised controlled trials. External validity if often an afterthought. And in an article arguing against placing randomised controlled trials on a pedestal, Angus Deaton and Nancy Cartwright (2018) agree that this can have value:\n\nSuppose a trial has (probabilistically) established a result in a specific setting. If ‘the same’ result holds elsewhere, it is said to have external validity. External validity may refer just to the replication of the causal connection or go further and require replication of the magnitude of the ATE. Either way, the result holds—everywhere, or widely, or in some specific elsewhere—or it does not.\nThis binary concept of external validity is often unhelpful because it asks the results of an RCT to satisfy a condition that is neither necessary nor sufficient for trials to be useful, and so both overstates and understates their value. It directs us toward simple extrapolation—whether the same result holds elsewhere—or simple generalization—it holds universally or at least widely—and away from more complex but equally useful applications of the results. The failure of external validity interpreted as simple generalization or extrapolation says little about the value of the results of the trial.\n\nBut this paragraph starts to shape the critique:\n\nEstablishing causality does nothing in and of itself to guarantee that the causal relation will hold in some new case, let alone in general. Nor does the ability of an ideal RCT to eliminate bias from selection or from omitted variables mean that the resulting ATE from the trial sample will apply anywhere else.\n\n\n\n\n\nDeaton, A., and Cartwright, N. (2018). Understanding and misunderstanding randomized controlled trials. Social Science & Medicine, 210, 2–21. https://doi.org/10.1016/j.socscimed.2017.12.005\n\n\nList, J., and Al-Ubaydii, O. (2014). The generalisability of experimental results in economics. CEPR. https://cepr.org/voxeu/columns/generalisability-experimental-results-economics\n\n\nSalganik, M. (2018). Bit by Bit. Princeton University Press. https://press.princeton.edu/books/paperback/9780691196107/bit-by-bit",
    "crumbs": [
      "Interpreting trial results",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Validity</span>"
    ]
  },
  {
    "objectID": "interpreting-trial-results/generalisability-effect-sizes.html",
    "href": "interpreting-trial-results/generalisability-effect-sizes.html",
    "title": "53  Generalisability: effect sizes",
    "section": "",
    "text": "A term often used in substitute of validity, particularly external validity, is generalisability. The term is most commonly used when describing whether the results of a field trial can be taken to inform how an intervention might work in another context or at scale.\nThere are limited systematic evaluations of the generalisability of applied behavioural science trials. However, research on the generalisability of impact evaluations in a development context provides insight into this question.\nEva Vivalt (2020) reviewed 635 papers containing 15,024 estimates of effect sizes relating to 20 types of interventions in international development. In her paper, she assessed the extent the results from a particular intervention could be used to predict the sign of the effect or the magnitude of the effect of a similar study in another context. This question is effectively an examination of the Type M and Type S errors we discussed.\nShe found that an inference about a study’s effect using another similar study will have the correct sign 61% of the time (comparing the median intervention-outcome pair in each study). When comparing effect sizes, a naive prediction of the result in the new study is likely to be wrong by about 249%.\nListen to Eva Vivalt on the 80,000 Hours podcast, or read the transcript.\n\n\nAnother perspective on generalisability comes from analysis by DellaVigna and Linos (2022) of the implementation of behavioural interventions by two “Nudge Units” in the United States. They compared the results from 126 randomised controlled trials run by the Nudge Units to a sample of trials in academic journals. They wrote:\nIn the Academic Journals papers, the average impact of a nudge is very large—an 8.7 percentage point take-up effect, which is a 33.4% increase over the average control. In the Nudge Units sample, the average impact is still sizable and highly statistically significant, but smaller at 1.4 percentage points, an 8.0% increase. We document three dimensions which can account for the difference between these two estimates: (i) statistical power of the trials; (ii) characteristics of the interventions, such as topic area and behavioral channel; and (iii) selective publication. A meta-analysis model incorporating these dimensions indicates that selective publication in the Academic Journals sample, exacerbated by low statistical power, explains about 70 percent of the difference in effect sizes between the two samples. Different nudge characteristics account for most of the residual difference.\n\n\n\n\nDellaVigna, S., and Linos, E. (2022). RCTs to Scale: Comprehensive Evidence From Two Nudge Units. Econometrica, 90(1), 81–116. https://doi.org/10.3982/ECTA18709\n\n\nVivalt, E. (2020). How much can we generalize from impact evaluations? Journal of the European Economic Association, 18(6), 3045–3089. https://doi.org/10.1093/jeea/jvaa019",
    "crumbs": [
      "Interpreting trial results",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Generalisability: effect sizes</span>"
    ]
  },
  {
    "objectID": "interpreting-trial-results/generalisability-context.html",
    "href": "interpreting-trial-results/generalisability-context.html",
    "title": "54  Generalisability: context",
    "section": "",
    "text": "Recall the jam experiment we discussed previously.\nOn two Saturdays in a California supermarket, Sheena Iyengar and Mark Lepper (2000) set up tasting displays of either six or 24 jars of jam. Consumers could taste as many jams as they wished, and if they approached the tasting table they received a $1 discount coupon to buy the jam.\nFor attracting initial interest, the large display of 24 jams did a better job, with 60 per cent of people who passed the display stopping. Forty per cent of people stopped at the six jam display. But only three per cent of those who stopped at the 24 jam display purchased any of the jam, compared with almost 30 per cent who stopped at the six jam display.\nThis experiment has gained famed as showing the paradox of choice. But how much weight should we place on this one experiment? In his book Uncontrolled, Jim Manzi (2012) writes:\n\nFirst, note that all of the inference is built on the purchase of a grand total of thirty-five jars of jam. Second, note that if the results of the jam experiment were valid and applicable with the kind of generality required to be relevant as the basis for economic or social policy, it would imply that many stores could eliminate 75 percent of their products and cause sales to increase by 900 percent. That would be a fairly astounding result—and indicates that there may be a problem with the measurement.\nMeasurement problems could easily arise because the experiment was done for a total of ten hours in only one store, and shoppers were grouped in hourly chunks. There could be all kinds of reasons that those people who happened to show up during the five hours of limited assortment could have systematically different propensity to respond to $ 1 off a specific line of jams than those who arrived in the other five-hour period: a soccer game finished at some specific time, and several of the parents who share similar propensities versus the average shopper came in nearly together; a bad traffic jam in one part of town with non-average propensity to respond to the coupon dissuaded several people from going to the store at one time versus another; etc. This is one reason retail experiments for such in-store promotional tactics are typically executed for twenty or thirty randomly assigned stores for a period of weeks.\n\nBenjamin Scheibehenne and friends (2010) surveyed the broader literature on the choice overload hypothesis. In some cases, choice increased purchases. In others it reduced them. Scheibehenne and friends determined that the mean effect size of changing the number of choices across the studies was effectively zero.\nFrom this result, Manzi continues:\n\nFirst, individual experiments need to encompass as much variation in background conditions as is feasible. It is almost impossible to run an experiment in one store that can produce valid conclusions. Social science RFTs that are executed across several school districts, court systems, welfare offices, or whatever are a much more reliable guide to action than single-site experiments. Experiments also need to run long enough to encompass changing background conditions over time. The combination of more sites and more time creates many more observations, and therefore reliability.\nSecond, the ultimate test of the validity of causal conclusions derived from an experiment is the ability to predict the results of future tests. We need to build the kind of distribution of multiple experiments that were summarized for the impact of breadth of choice on sales and satisfaction in Scheibehenne’s meta-analysis. Such a distribution allows us to measure the scope (if any) of reliable prediction based on some sequence of experiments. In the case of the jam experiment, the researchers in the original experiment themselves were careful about their explicit claims of generalizability, and significant effort has been devoted to the exact question of finding conditions under which choice overload occurs consistently, but popularizers telescoped the conclusions derived from one coupon-plus-display promotion in one store on two Saturdays, up through assertions about the impact of product selection for jam for this store, to the impact of product selection for jam for all grocery stores in America, to claims about the impact of product selection for all retail products of any kind in every store, ultimately to fairly grandiose claims about the benefits of choice to society.\n\n\n\n\n\nIyengar, S. S., and Lepper, M. R. (2000). When choice is demotivating: Can one desire too much of a good thing? Journal of Personality and Social Psychology, 79(6), 995–1006. https://doi.org/10.1037/0022-3514.79.6.995\n\n\nManzi, J. (2012). Uncontrolled. Basic Books. https://www.hachettebookgroup.com/titles/jim-manzi/uncontrolled/9780465029310/?lens=basic-books\n\n\nScheibehenne, B., Greifeneder, R., and Todd, P. M. (2010). Can there ever be too many options? A meta-analytic review of choice overload. Journal of Consumer Research, 37(3), 409–425. https://doi.org/10.1086/651235",
    "crumbs": [
      "Interpreting trial results",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Generalisability: context</span>"
    ]
  },
  {
    "objectID": "interpreting-trial-results/heterogeneity.html",
    "href": "interpreting-trial-results/heterogeneity.html",
    "title": "55  Heterogeneity",
    "section": "",
    "text": "Experimental treatments can have different effects on different people.\nIn most randomised controlled trials, however, there is a focus on average treatment effects. This is partly a legacy of the sample sizes we use. Partition into smaller groups requires a larger total sample size for accurate inference.\nThere is considerable benefit in considering heterogeneity in experiments. It can provide insight into how the intervention can be targeted to those who will benefit most, how the treatment works, or how the treatment can be improved.\nAs one illustration, Costa and Kahn (2013) examined how people’s change in energy use in response to a home energy report varied with their political ideology. Their analysis is summarised in the following chart.\n\nKnowledge of the political leanings of the recipients of the energy report could help targeting the reports and influence their design. It might also provide fodder to develop further hypotheses to make them more effective.\n\n55.0.1 Heterogeneity across cultures\nOne way in which heterogeneity shows itself is across cultures. The artefactual field trial we examined earlier, where Joe Henrich and friends (2001) played the ultimatum game in small scale societies, demonstrates this.\nHowever, there is also a commonality across cultures. In the Many Labs 2 study (Klein et al., 2018), researchers performed replications of various findings in behavioural science across a range of different countries and cultures. A core theme of the result was that the same studies tended to replicate across countries and cultures, or tended to fail everywhere.\n\nOne important implication of this is that, contrary to some claims, heterogeneity of study participants is unlikely to be an explanation for the replication crisis.\n\n\n\n\nCosta, D. L., and Kahn, M. E. (2013). Energy conservation ’nudges’ and environmentalist ideology: Evidence from a randomized residential electricity field experiment. Journal of the European Economic Association, 11(3), 680–702. https://doi.org/10.1111/jeea.12011\n\n\nHenrich, J., Boyd, R., Bowles, S., Camerer, C., Fehr, E., Gintis, H., and McElreath, R. (2001). In Search of Homo Economicus: Behavioral Experiments in 15 Small-Scale Societies. American Economic Review, 91(2), 73–78. https://doi.org/10.1257/aer.91.2.73\n\n\nKlein, R. A., Vianello, M., Hasselman, F., Adams, B. G., Adams, R. B., Alper, S., Aveyard, M., Axt, J. R., Babalola, M. T., Bahník, Š., Batra, R., Berkics, M., Bernstein, M. J., Berry, D. R., Bialobrzeska, O., Binan, E. D., Bocian, K., Brandt, M. J., Busching, R., … Nosek, B. A. (2018). Many Labs 2: Investigating Variation in Replicability Across Samples and Settings. Advances in Methods and Practices in Psychological Science, 1(4), 443–490. https://doi.org/10.1177/2515245918810225",
    "crumbs": [
      "Interpreting trial results",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Heterogeneity</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Allcott, H., and Kessler, J. B. (2019). The Welfare Effects of Nudges: A\nCase Study of Energy Use Social Comparisons. American Economic\nJournal: Applied Economics, 11(1), 236–276. https://doi.org/10.1257/app.20170328\n\n\nAlter, A. L., Oppenheimer, D. M., Epley, N., and Eyre, R. N. (2007).\nOvercoming intuition: Metacognitive difficulty activates analytic\nreasoning. Journal of Experimental Psychology: General,\n136(4), 569–576. https://doi.org/10.1037/0096-3445.136.4.569\n\n\nBem, D. J. (2011). Feeling the future: Experimental evidence for\nanomalous retroactive influences on cognition and affect. Journal of\nPersonality and Social Psychology, 100(3), 407–425. https://doi.org/10.1037/a0021524\n\n\nBertrand, M., and Mullainathan, S. (2004). Are Emily and Greg More\nEmployable Than Lakisha and Jamal? A Field Experiment on Labor Market\nDiscrimination. American Economic Review, 94(4),\n991–1013. https://doi.org/10.1257/0002828042002561\n\n\nBrownback, A., and Novotny, A. (2018). Social desirability bias and\npolling errors in the 2016 presidential election. Journal of\nBehavioral and Experimental Economics, 74, 38–56. https://doi.org/10.1016/j.socec.2018.03.001\n\n\nCamerer, C. F., Dreber, A., Forsell, E., Ho, T.-H., Huber, J.,\nJohannesson, M., Kirchler, M., Almenberg, J., Altmejd, A., Chan, T.,\nHeikensten, E., Holzmeister, F., Imai, T., Isaksson, S., Nave, G.,\nPfeiffer, T., Razen, M., and Wu, H. (2016). Evaluating replicability of\nlaboratory experiments in economics. Science,\n351(6280), 1433–1436. https://doi.org/10.1126/science.aaf0918\n\n\nCosta, D. L., and Kahn, M. E. (2013). Energy conservation ’nudges’ and\nenvironmentalist ideology: Evidence from a randomized residential\nelectricity field experiment. Journal of the European Economic\nAssociation, 11(3), 680–702. https://doi.org/10.1111/jeea.12011\n\n\nCressey, D. (2017). Tool for detecting publication bias goes under\nspotlight. Nature. https://doi.org/10.1038/nature.2017.21728\n\n\nDeaton, A., and Cartwright, N. (2018). Understanding and\nmisunderstanding randomized controlled trials. Social Science &\nMedicine, 210, 2–21. https://doi.org/10.1016/j.socscimed.2017.12.005\n\n\nDellaVigna, S., and Linos, E. (2022). RCTs to Scale: Comprehensive\nEvidence From Two Nudge Units. Econometrica, 90(1),\n81–116. https://doi.org/10.3982/ECTA18709\n\n\nDietvorst, B. J., Simmons, J. P., and Massey, C. (2015). Algorithm\naversion: People erroneously avoid algorithms after seeing them err.\nJournal of Experimental Psychology: General, 144,\n114–126. https://doi.org/10.1037/xge0000033\n\n\nFunnell, S. C., and Rogers, P. J. (2011). The essence of program\ntheory. Jossey-Bass. https://www.wiley.com/en-au/Purposeful+Program+Theory%3A+Effective+Use+of+Theories+of+Change+and+Logic+Models-p-9780470478578\n\n\nGelman, A., and Carlin, J. (2014). Beyond Power Calculations: Assessing\nType S (Sign) and Type M (Magnitude) Errors. Perspectives on\nPsychological Science, 9(6), 641–651. https://doi.org/10.1177/1745691614551642\n\n\nGelman, A., Hill, J., and Vehtari, A. (2020). Regression and Other\nStories. Cambridge University Press. https://doi.org/10.1017/9781139161879\n\n\nGelman, A., and Loken, E. (2013). The garden of forking paths: Why\nmultiple comparisons can be a problem, even when there is no\n“fishing expedition” or\n“p-hacking” and the research hypothesis was\nposited ahead of time. https://sites.stat.columbia.edu/gelman/research/unpublished/p_hacking.pdf\n\n\nGlennerster, R., and Takavarasha, K. (2013a). Asking the right\nquestions. In Running randomized evaluations: A practical guide\n(pp. 66–97). Princeton University Press. https://www.jstor.org.ezproxy.lib.uts.edu.au/stable/j.ctt4cgd52.7\n\n\nGlennerster, R., and Takavarasha, K. (2013b). Outcomes and instruments.\nIn Running randomized evaluations: A practical guide (pp.\n180–240). Princeton University Press. https://www.jstor.org/stable/j.ctt4cgd52.9\n\n\nGlennerster, R., and Takavarasha, K. (2013c). Randomizing. In\nRunning randomized evaluations: A practical guide (pp. 98–179).\nPrinceton University Press. https://www.jstor.org/stable/j.ctt4cgd52.8\n\n\nGlennerster, R., and Takavarasha, K. (2013d). Why randomize? In\nRunning randomized evaluations: A practical guide (pp. 24–65).\nPrinceton University Press. https://www.jstor.org/stable/j.ctt4cgd52.6\n\n\nHarrison, G. W., and List, J. A. (2004). Field Experiments. Journal\nof Economic Literature, 47.\n\n\nHaynes, L., Service, O., Goldacre, B., and Torgerson, D. (2012).\nTest, Learn, Adapt: Developing Public Policy with Randomised\nControlled Trials. https://www.gov.uk/government/publications/test-learn-adapt-developing-public-policy-with-randomised-controlled-trials\n\n\nHenrich, J., Boyd, R., Bowles, S., Camerer, C., Fehr, E., Gintis, H.,\nand McElreath, R. (2001). In Search of Homo Economicus: Behavioral\nExperiments in 15 Small-Scale Societies. American Economic\nReview, 91(2), 73–78. https://doi.org/10.1257/aer.91.2.73\n\n\nIyengar, S. S., and Lepper, M. R. (2000). When choice is demotivating:\nCan one desire too much of a good thing? Journal of Personality and\nSocial Psychology, 79(6), 995–1006. https://doi.org/10.1037/0022-3514.79.6.995\n\n\nKahneman, D., and Tversky, A. (1979). Prospect theory: An analysis of\ndecision under risk. Econometrica, 47(2), 263–291. https://doi.org/10.2307/1914185\n\n\nKenneally, E., and Dittrich, D. (2012). The Menlo Report: Ethical\nPrinciples Guiding Information and Communication Technology\nResearch.\n\n\nKlein, R. A., Vianello, M., Hasselman, F., Adams, B. G., Adams, R. B.,\nAlper, S., Aveyard, M., Axt, J. R., Babalola, M. T., Bahník, Š., Batra,\nR., Berkics, M., Bernstein, M. J., Berry, D. R., Bialobrzeska, O.,\nBinan, E. D., Bocian, K., Brandt, M. J., Busching, R., … Nosek, B. A.\n(2018). Many Labs 2: Investigating Variation in Replicability Across\nSamples and Settings. Advances in Methods and Practices in\nPsychological Science, 1(4), 443–490. https://doi.org/10.1177/2515245918810225\n\n\nLee, S. M. (2018). Sliced And Diced: The Inside Story Of How An Ivy\nLeague Food Scientist Turned Shoddy Data Into Viral Studies.\nBuzzFeed News. https://www.buzzfeednews.com/article/stephaniemlee/brian-wansink-cornell-p-hacking\n\n\nList, J. A. (2011). Why Economists Should Conduct Field Experiments and\n14 Tips for Pulling One Off. Journal of Economic Perspectives,\n25(3), 3–16. https://doi.org/10.1257/jep.25.3.3\n\n\nList, J., and Al-Ubaydii, O. (2014). The generalisability of\nexperimental results in economics. CEPR. https://cepr.org/voxeu/columns/generalisability-experimental-results-economics\n\n\nManzi, J. (2012). Uncontrolled. Basic Books. https://www.hachettebookgroup.com/titles/jim-manzi/uncontrolled/9780465029310/?lens=basic-books\n\n\nMazar, N., Amir, O., and Ariely, D. (2008). The Dishonesty of Honest\nPeople: A Theory of Self-Concept Maintenance. Journal of Marketing\nResearch, 45(6), 633–644. https://doi.org/10.1509/jmkr.45.6.633\n\n\nMeyer, A., Frederick, S., Burnham, T. C., Guevara Pinto, J. D., Boyer,\nT. W., Ball, L. J., Pennycook, G., Ackerman, R., Thompson, V. A., and\nSchuldt, J. P. (2015). Disfluent fonts don’t help people\nsolve math problems. Journal of Experimental Psychology:\nGeneral, 144(2), e16–e30. https://doi.org/10.1037/xge0000049\n\n\nMeyer, M. N., Heck, P. R., Holtzman, G. S., Anderson, S. M., Cai, W.,\nWatts, D. J., and Chabris, C. F. (2019). Objecting to experiments that\ncompare two unobjectionable policies or treatments. Proceedings of\nthe National Academy of Sciences, 116(22), 10723–10728. https://doi.org/10.1073/pnas.1820701116\n\n\nOpen Science Collaboration. (2015). Estimating the reproducibility of\npsychological science. Science, 349(6251), aac4716. https://doi.org/10.1126/science.aac4716\n\n\nRogers, P. J., Petrosino, A., Huebner, T. A., and Hacsi, T. A. (2000).\nProgram theory evaluation: Practice, promise, and problems. New\nDirections for Evaluation, 2000(87), 5–13. https://doi.org/10.1002/ev.1177\n\n\nRoth, A. E. (1995). Introduction to experimental economics (J.\nH. Kagel and A. E. Roth, Eds.). Princeton University Press. http://doi.org/10.2307/j.ctvzsmff5.5\n\n\nSalganik, M. (2018). Bit by Bit. Princeton University Press. https://press.princeton.edu/books/paperback/9780691196107/bit-by-bit\n\n\nScheibehenne, B., Greifeneder, R., and Todd, P. M. (2010). Can there\never be too many options? A meta-analytic review of choice overload.\nJournal of Consumer Research, 37(3), 409–425. https://doi.org/10.1086/651235\n\n\nSilberzahn, R., Uhlmann, E. L., Martin, D. P., Anselmi, P., Aust, F.,\nAwtrey, E., Bahník, Š., Bai, F., Bannard, C., Bonnier, E., Carlsson, R.,\nCheung, F., Christensen, G., Clay, R., Craig, M. A., Dalla Rosa, A.,\nDam, L., Evans, M. H., Flores Cervantes, I., … Nosek, B. A. (2018). Many\nAnalysts, One Data Set: Making Transparent How Variations in Analytic\nChoices Affect Results. Advances in Methods and Practices in\nPsychological Science, 1(3), 337–356. https://doi.org/10.1177/2515245917747646\n\n\nSimmons, J. P., Nelson, L. D., and Simonsohn, U. (2011). False-Positive\nPsychology: Undisclosed Flexibility in Data Collection and Analysis\nAllows Presenting Anything as Significant. Psychological\nScience, 22(11), 1359–1366. https://doi.org/10.1177/0956797611417632\n\n\nThe National Commission for the Protection of Human Subjects of\nBiomedical and Behavioral Research. (1979). The Belmont Report.\nU.S. Department of Health, Education, & Welfare, USA. https://www.hhs.gov/ohrp/regulations-and-policy/belmont-report/read-the-belmont-report/index.html\n\n\nVerschuere, B., Meijer, E. H., Jim, A., Hoogesteyn, K., Orthey, R.,\nMcCarthy, R. J., Skowronski, J. J., Acar, O. A., Aczel, B., Bakos, B.\nE., Barbosa, F., Baskin, E., Bègue, L., Ben-Shakhar, G., Birt, A. R.,\nBlatz, L., Charman, S. D., Claesen, A., Clay, S. L., … Yıldız, E.\n(2018). Registered Replication Report on Mazar, Amir, and Ariely (2008).\nAdvances in Methods and Practices in Psychological Science,\n1(3), 299–317. https://doi.org/10.1177/2515245918781032\n\n\nVivalt, E. (2020). How much can we generalize from impact evaluations?\nJournal of the European Economic Association, 18(6),\n3045–3089. https://doi.org/10.1093/jeea/jvaa019",
    "crumbs": [
      "References"
    ]
  }
]