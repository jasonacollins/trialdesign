# Trial basics

A good experiment tends to have a number of design features. These include that:

-   Your trial is simple
-   Your trial is designed to test specific hypothesis or hypotheses
-   Your trial is designed to test or controls for alternative hypotheses.

In this chapter, we will look at the basic structure of a trial that has these features. We will look at the concept of control and how this is implemented in randomised controlled trials. We will examine how to develop a question and how to measure trial outcomes.

We will also cover another important consideration at the beginning of any trial, ethics.



## Randomisation

In its simplest form, randomised trials work by splitting trial participants into two or more groups by random lot. Each group is then given a different intervention, with one of those groups typically a "control group" or "test group" that receives no intervention or the status quo.

Groups might be allocated randomly by drawing numbers out of a hat, flipping a coin or, more commonly in experimental work, using a pseudo-random number generator. It is not the experimenter that decides who gets an intervention or not, but rather chance.

Randomisation works as a control technique because it enables experimenters to hold approximately equal the sources of experimental bias, such as uncontrolled variables, between the control and treatment groups. These might even be variables of which we are ignorant.

In *Uncontrolled: The Surprising Payoff of Trial-and-Error for Business Politics and Society*, Jim Manzi gives the following example:

> \[S\]uppose researchers in 1950 wanted to test the efficacy of a pill designed to reduce blood pressure but did not know that about 10 percent of the human species has a specific gene variant that predisposes them to adult-onset hypertension. If the researchers selected 3,000 people, and randomly assigned 1,500 to a test group who are given the pill and 1,500 to a control group who received a placebo, then about 150 patients in each group should have the gene variant of interest (though the researchers would have no explicit information about this and wouldn't even have thought to investigate it). Therefore, when these researchers compared the change in blood pressure before and after taking the pill for the test group versus the control group, their estimate would not be biased by a much higher proportion of patients with the gene variant of interest in one group or the other

Randomisation also avoids the need for a detailed understanding of the mechanism underlying the difference in outcomes between groups. James Lind conducted what many considered to be the first clinical trial in 1747 when he gave six scurvy stricken sailors citrus juice, while denying the treatment to another six. He did not need to know that vitamin C was the mechanism, nor anything about human biology to see if it worked.

Randomisation relies on the law of large numbers, the idea that as sample size increases the sample average converges to the expected value. In the case of the experiment to reduce blood pressure, as the size of the groups increased, we would expect the proportion of people with the hypertension genetic variant to converge to around 10% in each group.

To think of what that means intuitively, if you had only ten in each group, there is a material chance the groups could have zero, one, or more people with the hypertension variant, although it would rarely be three or more. Therefore, with a small sample, the relative proportions of the variant vary markedly between groups. It is only be collecting large samples that we can expect the groups to approximately equal proportions.

In the third week of this module (week four of this unit), we will examine how to determine the required sample size.

After randomisation and application of the treatment, outcomes are then measured for each group. Within certain statistical parameters (also to be covered later in this unit), we can then take the differences between the two groups to be as a result of the different interventions we received. The treatment "caused" the differences, although as noted above, we may not understand the mechanism.

![](img/TLA_Figure_3.jpg)



## Developing an intervention for trial

Applied behavioural science projects are typically designed to change behaviour (or at least to improve an outcome caused by a certain behaviour). The trial is one step of that project.

Through that project, you will ask many questions that will inform the interventions that you wish to trial. For instance, your sequence of questions might be:

-   What is the problem you are trying to solve?
-   What behaviour is leading to the outcome?
-   What is our theory of the current behaviour?
-   What interventions might influence the behaviour?

You might answer those questions through process such as that described in Module 1 \[add link to page 1.4\]. Once you have answered them, you will have a shortlist of interventions that you could trial. The purpose of the trial is to differentiate between the effectiveness of those behavioural interventions. Are any of the interventions more effective in improving your outcome of interest?

There are some other practical questions you need to ask in selecting interventions for trial.

First, you want the choice of interventions to help answer an interesting question. You want to learn something. For example, has the intervention been found effective or ineffective in similar contexts before? Will you learn something new from trying it again? If text message reminders have been found consistently effective in similar scenarios, a test of text messages versus a control of no message may not be informative. However, a trial varying the content of the message and the theoretical underpinning of that message might be.

Second, you want to select interventions that you can deliver consistently at scale. For the experiment, you want everyone within each group (control and treatments) to receive the same interventions as others in the group. But more importantly, what would happen if your trial was successful? What will it practically look like at scale relative to your perfect world conception of the intervention? If you have found an intervention to be highly effective, but it is not feasible to roll out at scale, your experiment is not useful.

The process of selecting interventions for an applied behavioural science trial is usually conceptually simpler than developing interventions for an academic experiment designed to test a theory. In that case, it is important that there is not a confounding theory that gives an equally plausible rationale for the behaviour observed in the experiment. The experimenter needs to understand the most plausible hypotheses that should be controlled for, which might depend on recent developments in theory. An experiment may only appear good at the time, as subsequent work may expose its theoretical flaws.

But that is not to say that you shouldn't think about the conceptual and theoretical underpinnings of your interventions in an applied behavioural science trial. There will typically be a theoretical basis for your choice of interventions. That basis will help determine what you should measure. It will inform the interpretation of your results. It can provide the foundation for you to take those interventions into other contexts.

### Watch

This video from UNICEF discusses some of the issues in this week's content, plus foreshadows many of the topics we will be covering over the next few weeks.

https://youtu.be/Wy7qpJeozec

### Required reading

Ames and Hiscox (2016) *Guide to developing behavioural interventions for randomised controlled trials: Nine guiding questions*, Canberra: Department of the Prime Minister and Cabinet, https://behaviouraleconomics.pmc.gov.au/sites/default/files/files/guide-to-developing-behavioural-interventions-for-randomised-controlled-trials.pdf



## Are randomised controlled trials ethical?

> Randomized experiments---long the gold standard in medicine---are increasingly used throughout the social sciences and professions to evaluate business products and services, government programs, education and health policies, and global aid. We find robust evidence---across 16 studies of 5,873 participants from three populations spanning nine domains---that people often approve of untested policies or treatments (A or B) being universally implemented but disapprove of randomized experiments (A/B tests) to determine which of those policies or treatments is superior. This effect persists even when there is no reason to prefer A to B and even when recipients are treated unequally and randomly in all conditions (A, B, and A/B). This experimentation aversion may be an important barrier to evidence-based practice.
>
> Meyer et al (2019)

Is it ethical to withhold an intervention that may benefit someone by assigning them to a control group?

A common response to this question is that this effectively already occurs in many instances without trials:

-   Interventions are often piloted, which is equivalent to excluding a group of people who could benefit.
-   Interventions are often scaled up, meaning that it takes time for everyone to receive the intervention.

A randomised controlled trial might be considered more ethical than either of those scenarios as it is similarly a phased introduction, but with a mechanism to determine its effectiveness and improve future outcomes. Absent a robust test, we need to be clear about the limits of our knowledge. There are many cases where an intervention assumed to be helpful was later found to be ineffective or harmful.

Further, trials often have protocols that in the case of large early effects indicating success or harm, they can be ceased or implemented more rapidly.

Another related question is whether it is fair to experiment on people at all? Don't we risk harm?

One response is that every time you roll out a new program, product, communication or other tool that may change behaviour or affect their wellbeing, you are running an experiment. It's just that if you're doing it absent a control group or some other mechanism to determine effectiveness, your experiment does not even have the benefit of enabling you to know whether it works, or is helping or harming people.

Discussion

Do you find the above arguments compelling?

### References

Meyer et al (2019) "Objecting to experiments that compare two unobjectionable policies or treatments" *Proceedings of the National Academy of Sciences*, 116(22), 10723-10728, https://doi.org/10.1073/pnas.1820701116

## Ethical principles for trial design

Experimental research is full of examples of ethical failures. One of the most notorious is the Tuskegee Syphilis Study, in which almost 400 hundred African American men were denied safe and effective treatment for almost 40 years.

Today there are many frameworks for ethical conduct of trials. One landmark framework comes from the Belmont Report (1979), a US Congress initiated national commission into ethical guidelines for research involving human subjects. It proposed three central principles for the conduct of trials: respect for persons, beneficence, and justice.

The Menlo Report (2012) was later developed in the light of increased digitisation of research practices and the difficulty in applying the ideas in the Belmont report in the digital age. It affirmed the three principles from the Menlo report and proposed a fourth: respect for law and public interest.

These principles form the basis of many Institutional Review Board ethical approvals in university and government. Each principle is described below:

### Respect for Persons

The Menlo report described respect for persons as having the following components: - Participation as a research subject is voluntary, and follows from informed consent - Treat individuals as autonomous agents and respect their right to determine their own best interests - Respect individuals who are not targets of research yet are impacted - Individuals with diminished autonomy, who are incapable of deciding for themselves, are entitled to protection.

Practically, respect for persons has tended to revolve around the principle of informed consent. People should be given information about the experiment in a comprehensible format and then voluntarily agree to participate.

It is easy to come up with scenarios where informed consent does not appear appropriate or would undermine the very purpose of the experiment. For example, in a previous unit (23713 Behavioural Economics and Corporate Decision Making) we discussed research by Marianne Bertrand and Sendhil Mullainathan (2004), who sent fictitious CVs in response to ads with randomly assigned African American or White sounding names. White names received 50% more callbacks for interviews. Obtaining informed consent from employers would be impractical and make the experiment pointless.

There have been hundreds of discrimination studies of this nature. The lack of consent has been justified for reasons including the limited harm to employers and the social benefit of an accurate measure of discrimination. These justifications have enabled experiments of this nature to pass Institution Review Board processes.

### Beneficence

The Menlo report described beneficience as having the following components: - Do not harm - Maximize probable benefits and minimize probable harms - Systematically assess both risk of harm and benefit.

One major potential source of harm is "informational risk", the risk that information gathered in a trial may be disclosed. This is often dealt with through anonymisation, although there are ample examples of failures or re-identification of data after anonymisation processes. This leads to requirements to develop data protection plans and sharing protocols.

### Justice

The Menlo report described justice as having the following components: - Each person deserves equal consideration in how to be treated, and the benefits of research should be fairly distributed according to individual need, effort, societal contribution, and merit - Selection of subjects should be fair, and burdens should be allocated equitably across impacted subjects.

Early conceptions of this concept focused on *protection*. More focus today, however, is on *access*, with groups such as women and minority groups needing to be explicity included in trials so that they can benefit from the knowledge gained.

### Respect for Law and Public Interest

The Menlo report described respect for law and public interest as having the following components: - Engage in legal due diligence - Be transparent in methods and results - Be accountable for actions.

The Belmont report took respect for law and public interest to be part of beneficience, but the Menlo report argues it deserves explicit consideration. It extends beyond the participants to society and law more generally.

### An example

> For one week in January 2012, approximately 700,000 Facebook users were placed in an experiment to study "emotional contagion," the extent to which a person's emotions are impacted by the emotions of the people with whom they interact. ... Participants in the Emotional Contagion experiment were put into four groups: a "negativity-reduced" group, for whom posts with negative words (e.g., sad) were randomly blocked from appearing in the News Feed; a "positivity-reduced" group, for whom posts with positive words (e.g., happy) were randomly blocked; and two control groups, one for the positivity-reduced group and one for the negativity-reduced group. The researchers found that people in the positivity-reduced group used slightly fewer positive words and slightly more negative words relative to the control group. Likewise, they found that people in the negativity-reduced group used slightly more positive words and slightly fewer negative words. Thus, the researchers found evidence of emotional contagion ...
>
> Salganik (2018) *Bit by Bit: Social Research in the Digital Age*

Using the principles above, what ethical questions arise in that experiment?

### Optional reading

Kenneally and Dittrich (2012) *The Menlo Report: Ethical Principles Guiding Information and Communication Technology Research*, Tech. Report., U.S. Department of Homeland Security, https://www.dhs.gov/sites/default/files/publications/CSD-MenloPrinciplesCORE-20120803_1.pdf

## References

Allcott and Kessler (2019) "The Welfare Effects of Nudges: A Case Study of Energy Use Social Comparisons", *American Economic Journal: Applied Economics*, 11(1), 236-276, https://doi.org/10.1257/app.20170328

Belmont Report (1979) *The Belmont Report: Ethical Principles and Guidelines for the Protection of Human Subjects of Research*, US Department of Health, Education, and Welfare, https://videocast.nih.gov/pdf/ohrp_belmont_report.pdf

Bertrand and Mullainathan (2004) "Are Emily and Greg More Employable Than Lakisha and Jamal? A Field Experiment on Labor Market Discrimination", *The American Economic Review*, 94(4), 991-1013, https://doi.org/10.1257/0002828042002561

Haynes et al (2012) *Test, Learn, Adapt: Developing Public Policy with Randomised Controlled Trials*, Cabinet Office, https://www.gov.uk/government/publications/test-learn-adapt-developing-public-policy-with-randomised-controlled-trials

Manzi et al (2012) *Uncontrolled: The Surprising Payoff of Trial-and-Error for Business Politics and Society*, Basic Books

Salganik (2018) *Bit by Bit: Social Research in the Digital Age* Princeton University Press
