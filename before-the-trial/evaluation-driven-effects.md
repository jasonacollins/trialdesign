# Evaluation-driven effects

Being part of an evaluation can change the way people behave, independent of any impacts of the program.

**Evaluation-driven effects include:**

- **Hawthorne effects**: The treatment group works harder than normal. For example, a student who wins a scholarship through a lottery among many eligible students may work especially hard because she feels she has been given a "rare opportunity" and does not want to fail the program.
- **John Henry effects**: The comparison group starts competing with the treatment group.
- **Resentment and demoralization effects**: The comparison group resents missing out on the treatment.
- **Demand effects**: The participants change their behaviour in response to their perception of the evaluators' objective.
- **Anticipation effects**: The comparison group changes their behaviour because they expect to receive the treatment later.
- **Survey effects**: Being surveyed (frequently) changes the subsequent behaviour of the treatment or the comparison group.

Evaluation effects can undermine power and generalizability. Evaluation-driven behaviour can lead to outcome changes that would not occur without the evaluation, which reduces the generalizability of the results.

Evaluation-driven effects can undermine comparability. If the evaluation-driven behaviour is group specific (affecting only the treatment or only the comparison group), it undermines the comparability of the two groups.

Evaluation-driven effects can bias impact estimates. Hawthorne effects and social desirability effects can inflate the estimated impact of a program compared to its true impact by artificially boosting outcomes among the treatment group. John Henry effects deflate the estimated impact of the program by artificially boosting outcomes in the comparison group.

**We can limit evaluation-driven effects by:**

- Identifying potential sources of evaluation-driven behaviour change, such as interactions between the evaluation team and the treatment and comparison groups that would exist only in the evaluation context.
- Using a different level of randomization. Limit the treatment-comparison interactions that generate demoralization, anticipation, and competition.
- Not announcing the phase-in.
- Making sure the evaluation staff are impartial.
- Making sure the treatment and comparison groups get equivalent interaction with staff.
- Measuring the evaluation-driven effects on a subset of the evaluation sample. Create random variation in evaluation procedures to measure the effects.

## References

Glennerster, R., & Takavarasha, K. (2013) "[Threats](https://doi-org.ezproxy.lib.uts.edu.au/10.2307/j.ctt4cgd52.11)" in Running Randomized Evaluations: A Practical Guide (pp. 298-323). Princeton University Press
