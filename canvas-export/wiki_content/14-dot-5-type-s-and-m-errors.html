<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<title>14.5 Type S and M errors</title>
<meta name="identifier" content="gf361d3aa2b1264840c7b01bde997553f"/>
<meta name="editing_roles" content="teachers"/>
<meta name="workflow_state" content="active"/>
</head>
<body>
<p><img id="4158552" src="$IMS-CC-FILEBASE$/Updated%20banners/23717_Trial_Design_Banner-11.png" alt="23717_Trial_Design_Banner-11.png" data-api-endpoint="https://canvas.uts.edu.au/api/v1/courses/25795/files/4158552" data-api-returntype="File" loading="lazy"></p>
<p>When a null hypothesis is rejected, people tend to report and make decisions based on the point estimate of the sign and magnitude of the effect. We have already highlighted that effect sizes should be treated with caution, but there are alternative ways to think about the properties of the estimated effect size.</p>
<p>One of these is the concept of Type S (Sign) and Type M (Magnitude) errors.</p>
<div class="content-box">
<div class="grid-row">
<div class="col-xs-12 col-md-6">
<div class="styleguide-section__grid-demo-element">
<div style="background-color: #efe5f9; padding: 15px 30px; min-height: 150px;">
<p>A <strong>Type S</strong> <strong>error</strong> occurs when the sign of the estimated effect is in the opposite direction to the true effect. The Type S error rate is the probability of the sign being in the opposite direction.</p>
</div>
</div>
</div>
<div class="col-xs-12 col-md-6">
<div class="styleguide-section__grid-demo-element">
<div style="background-color: #f7e5f9; padding: 15px 30px; min-height: 150px;">
<p>A <strong>Type M error</strong> occurs when the magnitude of the estimated effect is much different from (larger than) the true effect. Type M errors are expressed in terms of the expected exaggeration factor, the expected ratio of the size of the estimated effect divided by the size of the underlying effect.</p>
</div>
</div>
</div>
</div>
</div>
<p>These errors tend to occur in low powered studies. As one example, suppose we have an effect size that cannot realistically be more than 2 percentage points, and a standard error of 8 percentage points. The below diagram shows the distribution of estimated effect sizes that would occur with this underlying data.</p>
<div class="content-box">
<div class="grid-row">
<div class="col-xs-12 col-md-7">
<div class="styleguide-section__grid-demo-element">
<p><img src="$IMS-CC-FILEBASE$/Type%20S%20and%20Type%20M%20error-1.png" alt="Type S and Type M error" width="631" height="307" data-api-endpoint="https://canvas.uts.edu.au/api/v1/courses/25795/files/6396919" data-api-returntype="File" loading="lazy"></p>
</div>
</div>
<div class="col-xs-12 col-md-5">
<div class="styleguide-section__grid-demo-element">
<p>For a statistically significant result, the effect size needs to be ~16 percentage points (around 2 standard deviations greater than zero). This is an 8-fold exaggeration of the true effect size. There is also a 24% probability that, in the case of a significant result, it is in the wrong direction.</p>
<p>This shows that the problem with a low-powered study is not just the high probability of a Type II error. The problem is that even if the researcher gets a statistical significant result, the effect size can have a high probability of being massively exaggerated or even in the opposite direction. This is not just a case of bad luck: in the case of a low powered study, the effect size can only be significant if it is exaggerated.</p>
<p>The net result of this framework is that low-powered studies will always generate at least one form of error, be that Type II or Type M.</p>
</div>
</div>
</div>
</div>
<div class="ic-flash-info" style="border-color: #7519d1; max-width: 80%; text-align: center;">
<h2>An example</h2>
<p style="text-align: left;">Continuing the example of the ovulatory cycle and voting, Andrew Gelman, Jennifer Hill and Aki Vehtari write:</p>
<blockquote style="background: #f9f9f9; margin: 25px 0px; padding: 15px 25px 15px 25px; border-left: 6px solid #ab2567;">
<p style="text-align: left;">In addition, relative to our understanding of the vast literature on voting behavior, the claimed effects seem implausibly large — a type M error. For example, the paper reports that, among women in relationships, 40% in the ovulation period supported Romney, compared to 23% in the non-fertile part of their cycle. Given that opinion polls find very few people switching their vote preferences during the campaign for any reason, these numbers seem unrealistic. The authors might respond that they don’t care about the magnitude of the difference, just the sign, but (a) with a magnitude of this size, we are talking noise (not just sampling error but also errors in measurement), and (b) one could just as easily explain this as a differential nonresponse pattern: maybe liberal or conservative women in different parts of their cycle are more or less likely to participate in a survey. It would be easy enough to come up with a story about that.</p>
</blockquote>
</div>
<p>&nbsp;</p>
<div style="background-color: #f5f5f5; padding: 15px 30px;">
<p><strong>References</strong></p>
<p>Gelman and Carlin (2014) "Beyond Power Calculations: Assessing Type S (Sign) and Type M (Magnitude) Errors", <em>Perspectives on Psychological Science</em>, 9(6), 641-51, <a href="https://doi.org/10.1177/1745691614551642">https://doi.org/10.1177/1745691614551642</a></p>
<p>Gelman, Hill and Vehtari (2020) <em>Regression and Other Stories</em>, Cambridge University Press, Cambridge</p>
</div>
</body>
</html>