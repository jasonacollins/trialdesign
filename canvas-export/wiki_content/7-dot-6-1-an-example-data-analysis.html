<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<title>7.6.1 An example data analysis</title>
<meta name="identifier" content="g4ddd07d03181030de090c5889d5c978a"/>
<meta name="editing_roles" content="teachers"/>
<meta name="workflow_state" content="active"/>
</head>
<body>
<p><img id="4158525" src="$IMS-CC-FILEBASE$/Updated%20banners/23717_Trial_Design_Banner-03.png" alt="23717_Trial_Design_Banner-03.png" loading="lazy"></p>
<p>In this part, I will illustrate the data analysis process by reproducing the analysis in Study 1 of Dietvorst, Simmons and Massey's (2015) paper on algorithm aversion.&nbsp; The experiment tested the hypothesis that "seeing the model perform, and therefore err, would decrease participants’ tendency to bet on it rather than the human forecaster, despite the fact that the model was more accurate than the human."</p>
<div class="content-box">
<div class="grid-row">
<div class="col-xs-12 col-md-4">
<div class="styleguide-section__grid-demo-element">
<p><img src="$IMS-CC-FILEBASE$/Uploaded%20Media/flow.png" alt="flow.png" width="292" height="640" data-api-endpoint="https://canvas.uts.edu.au/api/v1/courses/25795/files/6397077" data-api-returntype="File" loading="lazy"></p>
</div>
</div>
<div class="col-xs-12 col-md-8">
<div class="styleguide-section__grid-demo-element">
<p>Participants were informed that they would play the part of an MBA admissions officer with a task to forecast the success of each applicant (as a percentile). Success was defined as an equal weighting of GPA, respect, and employer prestige.</p>
<p>Participants were also told that "thoughtful analysts" had built a statistical model to forecast performance using data that the participants would receive.</p>
<p>After consent and information provision, participants were allocated into one of four conditions: three treatment conditions and a control.</p>
<p><span>Participants in each of the three treatment conditions saw and/or made 15 forecasts on which they received feedback on performance.</span></p>
<p>All participants, including the control, were asked whether they would like to use their own or the model's forecasts for all 10 incentivised forecasts. After their choice, participants would make the forecasts (even if they had chosen the model for awarding incentives). After making their forecasts, they were asked questions about their confidence in their own forecasts and that of the model. They then learned the bonus earned.</p>
<p>There were four experimental conditions, with the difference in conditions occurring in the first stage of the experiment. For each of the 15 MBA applicants, they would gain experience as follows.</p>
<ul>
<li><strong>Model:</strong><span>&nbsp;</span>Participants would get feedback showing the model’s prediction and the applicant’s true percentile</li>
<li><strong>Human:</strong><span>&nbsp;</span>Participants would make a forecast and then get feedback showing their own prediction and the applicant’s true percentile</li>
<li><strong>Model and human:</strong><span>&nbsp;</span>Participants would make a forecast and then get feedback showing their own prediction, the model’s prediction, and the applicant’s true percentile.</li>
<li><strong>Control:</strong><span>&nbsp;</span>No experience.</li>
</ul>
</div>
</div>
</div>
</div>
<p>Besides illustrating the analysis process for this subject, reproducing the analysis of a paper has other benefits.</p>
<p>First, it allows you to check that the original analysis is correct. This is often not the case. Most times the errors are minor, but sometimes the original finding may not even hold. In that case, there may be limited value to the replication. You may also determine that an inappropriate analysis methodology has been used.</p>
<p>Second, reproducing the analysis is another way to understand better the experimental methodology and what the analysis shows. You can see what variables were recorded and how they are used.</p>
<p>It is normally only possible to reproduce the analysis if the original data is available on a public repository or from the authors.</p>
<h2>Reproducing the algorithm aversion paper</h2>
<p>The data for Dietvorst et al. (2015) is available from <a class="inline_disabled" href="https://researchbox.org/379&amp;PEER_REVIEW_passcode=MOQTEQ" target="_blank">ResearchBox</a> and the <a class="inline_disabled" href="https://supp.apa.org/psycarticles/supplemental/xge0000033/xge0000033_supp.html" target="_blank">Journal of Experimental Psychology: General website</a>.</p>
<p>The download includes the Stata code that Dietvorst et al (2015) used to analyse the data. I do not know Stata, so I asked <a class="inline_disabled" href="https://chat.openai.com" target="_blank">ChatGPT</a> to translate the code into R. While that translation contained some errors, I made it operational with only minor changes.</p>
<p>In what follows I provide the R code I used to reproduce the analysis. You can expand each "Code" item to see the code or copy it to replicate the results yourself.</p>
<h2>Analysis</h2>
<p>I start by downloading and unzipping the data file and loading the CSV for Study 1 into the R environment.</p>
<details>
<summary style="padding: 15px; color: #fff; background-color: #ed8232; cursor: pointer;"><strong style="color: #000000;">Code</strong></summary>
<div style="background-color: #fffff; padding-top: 2%; border-radius: 0px 0px 10px 10px;">
<div style="width: 100%;">
<div class="cell">
<div id="cb1" class="sourceCode cell-code">
<pre><code>
     # Data downloaded from https://researchbox.org/379&amp;PEER_REVIEW_passcode=MOQTEQ
     # Open Zip file from research box
     # unzip("ResearchBox_379.zip")

     #Load data from csv
     data &lt;- read.csv("ResearchBox 379/Data/Study 1 Data.csv", header = TRUE, sep = ",", na.strings = ".")
         </code></pre>
</div>
</div>
</div>
</div>
</details>
<p>I then amended the variable names to make them more readable (e.g. replacing the numbers for each condition with the words they represent: 1=control, etc.) and convert them into the correct variable type (e.g. convert to factor).</p>
<details>
<summary style="padding: 15px; color: #fff; background-color: #ed8232; cursor: pointer;"><strong style="color: #000000;">Code</strong></summary>
<div style="background-color: #fffff; padding-top: 2%; border-radius: 0px 0px 10px 10px;">
<div style="width: 100%;">
<div class="cell">
<div id="cb1" class="sourceCode cell-code">
<pre><code>
     # Change to camel case
     colnames(data) &lt;- paste(tolower(substring(colnames(data), 1, 1)), substring(colnames(data), 2), sep = "")
     names(data)[1] &lt;- "ID"

     # Define label mappings
     condition &lt;- c("1" = "control", "2" = "human", "3" = "model", "4" = "model&amp;human")
     modelBonus &lt;- c("0" = "choseHuman", "1" = "choseModel")
     binary &lt;- c("0" = "no", "1" = "yes")
     betterStage2Bonus &lt;- c("1" = "model", "2" = "equal", "0" = "human")
     confidence &lt;- c("1" = "none", "2" = "little", "3" = "some", "4" = "fairAmount", "5" = "aLot")

     # Apply label mappings to variables - have done more here than required for analysis, but might be useful later
     data$condition &lt;- factor(data$condition, labels = condition)
     data$modelBonus &lt;- factor(data$modelBonus, labels = modelBonus)
     data$humanAsGoodStage1 &lt;- factor(data$humanAsGoodStage1, labels = binary)
     data$humanBetterStage1 &lt;- factor(data$humanBetterStage1, labels = binary)
     data$humanBetterStage2 &lt;- factor(data$humanBetterStage2, labels = binary)
     data$betterStage2Bonus &lt;- factor(data$betterStage2Bonus, labels = betterStage2Bonus)
     data$model &lt;- factor(data$model, labels = binary)
     data$human &lt;- factor(data$human, labels = binary)
     data$modelConfidence &lt;- factor(data$modelConfidence, labels = confidence)
     data$humanConfidence &lt;- factor(data$humanConfidence, labels = confidence)
         </code></pre>
</div>
</div>
</div>
</div>
</details>
<p>I then run a quick check on the number of observations. I note from the paper that I should find 369 participants at the start, but 8 of these participants did not complete enough questions to get to the dependent variable. That leaves 361 participants across each of the conditions.</p>
<details>
<summary style="padding: 15px; color: #fff; background-color: #ed8232; cursor: pointer;"><strong style="color: #000000;">Code</strong></summary>
<div style="background-color: #fffff; padding-top: 2%; border-radius: 0px 0px 10px 10px;">
<div style="width: 100%;">
<div class="cell">
<div id="cb1" class="sourceCode cell-code">
<pre><code>
     # Total participants
     length(data$ID)
                
     # Number of participants who chose between model and human
     table(data$modelBonus)
                </code></pre>
</div>
</div>
</div>
</div>
</details>
<p>&nbsp;</p>
<div class="cell-output cell-output-stdout">
<pre><code>
[1] 369</code></pre>
</div>
<p>&nbsp;</p>
<div class="cell-output cell-output-stdout">
<pre><code>
choseHuman choseModel 
       201        160 </code></pre>
</div>
<p>As expected, I find 369 participants and 201+160=361 dependent variable observations.</p>
<p>I then broke down the dependent variable findings in more detail to see what each participant chose (human or model) in each condition.</p>
<details>
<summary style="padding: 15px; color: #fff; background-color: #ed8232; cursor: pointer;"><strong style="color: #000000;">Code</strong></summary>
<div style="background-color: #fffff; padding-top: 2%; border-radius: 0px 0px 10px 10px;">
<div style="width: 100%;">
<div class="cell">
<div id="cb1" class="sourceCode cell-code">
<pre><code>
     # Choice of participants in each condition
     conditions &lt;- table(data$modelBonus, data$condition)
     conditions &lt;- rbind(conditions, total=colSums(conditions))
     conditions
        </code></pre>
</div>
</div>
</div>
</div>
</details>
<p>&nbsp;</p>
<div class="cell-output cell-output-stdout">
<pre><code>           control human model model&amp;human
choseHuman      32    33    67          69
choseModel      59    57    23          21
total           91    90    90          90</code></pre>
</div>
<div>
<p>Two chi-squared tests were then run to see if there was a significant difference between the conditions. The results are shown in the Study 1 diagram in Figure 3.</p>
<p>The first was a test of whether there was a difference between the two conditions without and the two conditions with the model. This is a test of the core hypothesis.</p>
</div>
<details>
<summary style="padding: 15px; color: #fff; background-color: #ed8232; cursor: pointer;"><strong style="color: #000000;">Code</strong></summary>
<div style="background-color: #fffff; padding-top: 2%; border-radius: 0px 0px 10px 10px;">
<div style="width: 100%;">
<div class="cell">
<div id="cb1" class="sourceCode cell-code">
<pre><code>     # chisq.test between conditions 1 and 2 (control and human) and 3 and 4 (model and model&amp;human)
     chisq.test(table(data$modelBonus, data$model), correct=FALSE)
      </code></pre>
</div>
</div>
</div>
</div>
</details>
<p>&nbsp;</p>
<div class="cell-output cell-output-stdout">
<pre><code> Pearson's Chi-squared test
    data: table(data$modelBonus, data$model)
    X-squared = 57.477, df = 1, p-value = 3.419e-14
    </code></pre>
</div>
<p>The second was a test of whether there was a difference between the two conditions without and the two conditions with the human experience. This test was run to see whether the experience in seeing their own errors changed the participants' choices.</p>
<details>
<summary style="padding: 15px; color: #fff; background-color: #ed8232; cursor: pointer;"><strong style="color: #000000;">Code</strong></summary>
<div style="background-color: #fffff; padding-top: 2%; border-radius: 0px 0px 10px 10px;">
<div style="width: 100%;">
<div class="cell">
<div id="cb1" class="sourceCode cell-code">
<pre><code>     # chisq.test between conditions 1 and 3 (control and model) and 2 and 4 (human and model&amp;human)
     chisq.test(table(data$modelBonus, data$human), correct=FALSE)
      </code></pre>
</div>
</div>
</div>
</div>
</details>
<p>&nbsp;</p>
<div class="cell-output cell-output-stdout">
<pre><code>Pearson's Chi-squared test
     data: table(data$modelBonus, data$human)
     X-squared = 0.14201, df = 1, p-value = 0.7063
    </code></pre>
</div>
<p>I then plotted that data in a bar chart to illustrate the results. This matches Figure 3 in the paper.&nbsp;</p>
<details>
<summary style="padding: 15px; color: #fff; background-color: #ed8232; cursor: pointer;"><strong style="color: #000000;">Code</strong></summary>
<div style="background-color: #fffff; padding-top: 2%; border-radius: 0px 0px 10px 10px;">
<div style="width: 100%;">
<div class="cell">
<div id="cb1" class="sourceCode cell-code">
<pre><code> 
     ## load ggplot2 for chart and tidyverse packages for data manipulation
     library(ggplot2)
     library(dplyr)
     library(tidyr)
            
     # convert table to data frame for future operations
     conditions &lt;- as.data.frame.matrix(conditions)
     
     #percentage choosing model in each condition
     conditions &lt;- rbind(conditions, percent=conditions[2,]/conditions[3,])
     conditions &lt;- tibble::rownames_to_column(conditions, "variable")
     conditions_long &lt;- pivot_longer(conditions, cols = -variable, names_to = "treatment", values_to = "value", cols_vary = "slowest")
     conditions_long &lt;- conditions_long[c(2, 1, 3)]
                  
     # ggplot2 bar plot of percent rows of conditions data frame
     plot_data &lt;- filter(conditions_long, variable=="percent")
                
     ggplot(plot_data, aes(x=treatment, y=value)) +
       geom_col()+
       labs(title="Percent choosing model in each condition", x="Condition", y="Percent choosing model") +
       theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
       # Set the theme
       theme_minimal()+
       geom_hline(yintercept = 0, linewidth=0.25)
             </code></pre>
</div>
</div>
</div>
</div>
</details>
<p><img src="$IMS-CC-FILEBASE$/Uploaded%20Media/Figure%203.png" alt="Figure 3.png" width="400" height="286" data-api-endpoint="https://canvas.uts.edu.au/api/v1/courses/25795/files/6397087" data-api-returntype="File" loading="lazy"></p>
<p>Finally, I replicate the analysis related to Study 1 as shown in Table 3. The table includes several elements:</p>
<ul>
<li>A t-test of whether the bonus would have been higher if the model was chosen rather than the human</li>
<li>A t-test of whether the average absolute error was higher for the model or human in the Stage 1 unincentivised forecasts</li>
<li>A t-test of whether the average absolute error was higher for the model or human in the Stage 3 incentivised forecasts</li>
</ul>
<p>The code generates the numbers required to fill the table.</p>
<details>
<summary style="padding: 15px; color: #fff; background-color: #ed8232; cursor: pointer;"><strong style="color: #000000;">Code</strong></summary>
<div style="background-color: #fffff; padding-top: 2%; border-radius: 0px 0px 10px 10px;">
<div style="width: 100%;">
<div class="cell">
<div id="cb1" class="sourceCode cell-code">
<pre><code> 
     # Calculate human and model mean rewards
     m &lt;- round(mean(data$bonusFromModel, na.rm=TRUE), 2)
     h &lt;- round(mean(data$bonusFromHuman, na.rm=TRUE), 2)
      
     # Bonus if chose model vs. human
     bonusModel &lt;- t.test(data$bonusFromModel, data$bonusFromHuman, paired=TRUE)
     p &lt;- signif(bonusModel$p.value, 2)
     t &lt;- round(bonusModel$statistic, 2)
     d &lt;- round(bonusModel$estimate, 2)

     # Participants' AAE compared to model's AAE for stage 1 forecasts
     stage1AAE &lt;- t.test(data$humanAAE1, data$modelAAE1, paired=TRUE)
     p1 &lt;- signif(stage1AAE$p.value, 2)
     t1 &lt;- round(stage1AAE$statistic, 2)
     d1 &lt;- round(stage1AAE$estimate, 2)
     AAEm &lt;- round(mean(data[data$condition=='model&amp;human', 'modelAAE1'], na.rm=TRUE), 2)
     AAEh &lt;- round(mean(data[data$condition=='model&amp;human', 'humanAAE1'], na.rm=TRUE), 2)

     # Participants' AAE compared to model's AAE for stage 2 forecasts
     stage2AAE &lt;- t.test(data$humanAAE2, data$modelAAE2, paired=TRUE)
     p2 &lt;- signif(stage2AAE$p.value, 2)
     t2 &lt;- round(stage2AAE$statistic, 2)
     d2 &lt;- round(stage2AAE$estimate, 2)
     AAEm2 &lt;- round(mean(data$modelAAE2, na.rm=TRUE), 2)
     AAEh2 &lt;- round(mean(data$humanAAE2, na.rm=TRUE), 2)
                         </code></pre>
</div>
</div>
</div>
</div>
</details>
<p>&nbsp;</p>
<div class="cell-output cell-output-stdout">
<p><strong>Table 3 results</strong></p>
<table class="table" style="width: 100%;">
<thead>
<tr class="header">
<th style="width: 20%;"></th>
<th style="width: 17.552584%;">Model</th>
<th style="width: 14.055846%;">Human</th>
<th style="width: 18.426178%;">Difference</th>
<th style="width: 10.824331%;">t score</th>
<th style="width: 14.859811%;">p value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="width: 20%;">Bonus</td>
<td style="width: 16%;">$1.78</td>
<td style="width: 16%;">$1.38</td>
<td style="width: 16%;">$0.4</td>
<td style="width: 16%;">4.98</td>
<td style="width: 16%;">9.7^{-7}</td>
</tr>
<tr class="even">
<td style="width: 20%;">Stage 1 error</td>
<td style="width: 16%;">23.13</td>
<td style="width: 16%;">26.67</td>
<td style="width: 16%;">3.53</td>
<td style="width: 16%;">5.52</td>
<td style="width: 16%;">3.3^{-7}</td>
</tr>
<tr class="odd">
<td style="width: 20%;">Stage 2 error</td>
<td style="width: 16%;">22.07</td>
<td style="width: 16%;">26.61</td>
<td style="width: 16%;">4.54</td>
<td style="width: 16%;">11.52</td>
<td style="width: 16%;">2.4^{-26}</td>
</tr>
</tbody>
</table>
</div>
<p>The numbers match those provided in the paper.</p>
<h2>Regression</h2>
<p>An alternative approach is to compare treatment and comparison groups within a regression framework. This will give us the same results but in a different format.</p>
<h3>Regression for test of proportions</h3>
<p>To determine whether there is a treatment effect, we use the logistic regression:</p>
<p>$$<br>\ln\frac{p}{1-p}=c+\beta T+\epsilon <br>$$</p>
<p>Where \(p\) is the probability that a participant chooses the model, \(c\) is a constant or intercept, and \(T\) is a treatment dummy. \(T=0\) if participant \(i\) is in the control group and 1 if participant \(i\) is in the treatment group. \(\beta\) is the coefficient of the treatment dummy.</p>
<p>Logistic regression is used here as we are considering the probability of an event taking place (choosing the model). We would use linear regression if we were considering a continuous variable (e.g., the amount saved due to an intervention).</p>
<details>
<summary style="padding: 15px; color: #fff; background-color: #ed8232; cursor: pointer;"><strong style="color: #000000;">Code</strong></summary>
<div style="background-color: #fffff; padding-top: 2%; border-radius: 0px 0px 10px 10px;">
<div style="width: 100%;">
<div class="cell">
<div id="cb1" class="sourceCode cell-code">
<pre><code>
     # Regression to determine whether there is a treatment effect
     reg1 &lt;- glm(modelBonus ~ model, data=data, family=binomial)

     # Show results
     summary(reg1)
      </code></pre>
</div>
</div>
</div>
</div>
</details>
<p>&nbsp;</p>
<div class="cell-output cell-output-stdout">
<pre><code> Call:
glm(formula = modelBonus ~ model, family = binomial, data = data)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.4312  -0.7487  -0.7487   0.9433   1.6785  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   0.5792     0.1549   3.738 0.000185 ***
modelyes     -1.7077     0.2326  -7.343 2.09e-13 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 495.79  on 360  degrees of freedom
Residual deviance: 436.57  on 359  degrees of freedom
  (8 observations deleted due to missingness)
AIC: 440.57

Number of Fisher Scoring iterations: 4
    </code></pre>
</div>
<p>The p-value of 2.09e-13 is not identical to that from the chi-squared test above (3.419e-14) as there are slight differences in the underlying tests, but either way, the result is significant. We can see the equivalence of the underlying result by examining the parameter estimates.</p>
<p>The intercept of 0.5792 implies that the odds of choosing the model are \(e^{0.5792}=1.784\) for those in the control condition, which is equal to the observed proportion of 116 choosing the model compared to 65 choosing the human (116/65=1.784). The estimate of \(\beta\) of -1.7077 implies that the ratio of the odds of those choosing the model in the control treatment condition relative to those in the control condition is \(e^{-1.7077}=0.181\), which is equal to the observed proportion of 44 choosing the model and 136 choosing the model in the treatment condition, compared to the 116 choosing the model and 65 choosing the human in the control condition ((44/136)/(116/65)=0.181).</p>
<p>The benefit of using a regression approach is that other covariates can be added to the analysis. For example, if there was a difference between male and female respondents, we could add a dummy for gender to the analysis.</p>
<p>Including covariates in the regression can give us a more precise estimate of the impact of the intervention as they can reduce unexplained variance. However, including covariates can reduce the precision of our estimate of the treatment effect if they don’t explain enough error variance.</p>
<p>Often, the best covariate to include is a baseline measure of the outcome variable. With stratified randomisation, adding an indicator for each different stratum is often recommended.</p>
<h3>Regression for test of quantities</h3>
<p>We can also run a regression for the above t-tests. In that case, we use linear regression:</p>
<p>$$<br>Y_i=c+\epsilon_i<br>$$</p>
<p>Where \(Y_i\) is the tested outcome, such as the difference in bonus between the model and humans, and \(c\) is a constant. There is no dummy for the conditions in this equation as it is a simple test of difference across all conditions.</p>
<p>Below is code for the regression of the difference in bonus between the model and human.</p>
<details>
<summary style="padding: 15px; color: #fff; background-color: #ed8232; cursor: pointer;"><strong style="color: #000000;">Code</strong></summary>
<div style="background-color: #fffff; padding-top: 2%; border-radius: 0px 0px 10px 10px;">
<div style="width: 100%;">
<div class="cell">
<div id="cb1" class="sourceCode cell-code">
<pre><code>
     # Regression to determine whether the bonus is different between the model and human

     diff &lt;- data$bonusFromModel - data$bonusFromHuman
     reg2 &lt;- lm(diff ~ 1, data=data)

     # Show results
     summary(reg2)
      </code></pre>
</div>
</div>
</div>
</div>
</details>
<p>&nbsp;</p>
<div class="cell-output cell-output-stdout">
<pre><code> Call:
lm(formula = diff ~ 1, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-4.3989 -1.3989 -0.3989  0.6011  4.6011 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  0.39889    0.08004   4.984 9.71e-07 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.521 on 360 degrees of freedom
  (8 observations deleted due to missingness)
    </code></pre>
</div>
<p>This gives us the same results as the t-test above. The intercept estimate of 0.40 reflects the $0.40 difference in bonus between the human participant and the model. The p-value of 9.71e-7 is the same as the t-test above.</p>
<p>As noted above, when using a regression approach we can add covariates. Below, I add gender to the analysis.</p>
<p>$$<br>Y_i=c+\beta G_i+\epsilon_i<br>$$</p>
<p>Where \(G\) is a dummy for gender. \(G=0\) if participant \(i\) is female and 1 if participant \(i\) is male. \(\beta\) is the coefficient of the gender dummy.</p>
<details>
<summary style="padding: 15px; color: #fff; background-color: #ed8232; cursor: pointer;"><strong style="color: #000000;">Code</strong></summary>
<div style="background-color: #fffff; padding-top: 2%; border-radius: 0px 0px 10px 10px;">
<div style="width: 100%;">
<div class="cell">
<div id="cb1" class="sourceCode cell-code">
<pre><code>
     # Regression to determine whether the bonus is different between the model and human with gender control

     diff &lt;- data$bonusFromModel - data$bonusFromHuman
     reg3 &lt;- lm(diff ~ gender, data=data)

     # Show results
     summary(reg3)
      </code></pre>
</div>
</div>
</div>
</div>
</details>
<p>&nbsp;</p>
<div class="cell-output cell-output-stdout">
<pre><code> Call:
lm(formula = diff ~ gender, data = data)

Residuals:
   Min     1Q Median     3Q    Max 
-4.593 -1.285 -0.285  0.715  4.407 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)   
(Intercept)   0.2850     0.1053   2.707  0.00711 **
gender        0.3083     0.1624   1.898  0.05846 . 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.515 on 355 degrees of freedom
  (12 observations deleted due to missingness)
Multiple R-squared:  0.01005,   Adjusted R-squared:  0.007261 
F-statistic: 3.604 on 1 and 355 DF,  p-value: 0.05846
    </code></pre>
</div>
<p>In this case, there is no significant difference between the male and female (\(p&gt;0.05\)). However, this may be due to a lack of power (the difference is $0.31). If you wanted to test this hypothesis, it would be worth testing with a larger sample.</p>
<p>You can download a .qmd file with the above code <a id="5691604" class="instructure_file_link inline_disabled" href="$IMS-CC-FILEBASE$/algorithm-aversion.qmd?canvas_=1&amp;canvas_qs_wrap=1" target="_blank" data-api-endpoint="https://canvas.uts.edu.au/api/v1/courses/25795/files/6397092" data-api-returntype="File">here</a>.</p>
<div class="content-box pad-box-mini" style="background-color: #f5f5f5; padding: 20px;">
<p><strong>References</strong></p>
<p>Dietvorst, Simmons and Massey (2015) "Algorithm aversion: People erroneously avoid algorithms after seeing them err", Journal of Experimental Psychology: General, 144, 114–126, <a style="font-family: inherit; font-size: 1rem;" href="https://doi.org/10.1037/xge0000033">https://doi.org/10.1037/xge0000033</a></p>
<p>Glennerster, R., and Takavarasha, K. (2013) "<a class="inline_disabled" href="https://doi-org.ezproxy.lib.uts.edu.au/10.2307/j.ctt4cgd52.12" target="_blank">Analysis</a>" In <em>Running Randomized Evaluations: A Practical Guide</em> (pp. 324–385). Princeton University Press. <a href="https://doi.org/10.2307/j.ctt4cgd52.12" target="_blank">https://doi.org/10.2307/j.ctt4cgd52.12</a>&nbsp;</p>
</div>
</body>
</html>