<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<title>12.5 An example power analysis</title>
<meta name="identifier" content="gc4500a832a7332ab346ee20579385de0"/>
<meta name="editing_roles" content="teachers"/>
<meta name="workflow_state" content="active"/>
</head>
<body>
<section id="power-analysis" class="level2">
<p><img id="4158564" src="$IMS-CC-FILEBASE$/Updated%20banners/23717_Trial_Design_Banner-28.png" alt="23717_Trial_Design_Banner-28.png" data-api-endpoint="https://canvas.uts.edu.au/api/v1/courses/25795/files/4158564" data-api-returntype="File" loading="lazy"></p>
<p>Below, I run a power analysis for Study 1 in Dietvorst, Simmons and Massey's (2015) paper on algorithm aversion. The experiment tested the hypothesis that "seeing the model perform, and therefore err, would decrease participants’ tendency to bet on it rather than the human forecaster, despite the fact that the model was more accurate than the human."</p>
<p>Experimental participants were given a judgment task, with one group allowed to see the algorithm in action before undertaking the task. Participants were given the option of using the algorithm's predictions or their own. Those who had seen the algorithm perform were less likely to use it in the task.</p>
<p>First I run a power analysis using data from the original experiment. Post-experiment power analysis should not be done to justify the sample size in the original study, as any significant effect in an underpowered study is likely to be exaggerated in magnitude. If you then use this exaggerated effect to calculate power, it may give the impression that the experiment was adequately powered.</p>
<p>However, this post-experiment analysis may still provide some information about the study’s robustness. It also provides a starting point for further analysis where we adjust the effect size within a plausible range to test how sensitive the assessment of power is to the effect size.</p>
<p>To calculate power, we need to know the sample size (per group) and the proportion choosing the model in each group.</p>
<p>In Dietvorst et al (2015) Study 1, the smallest group had 90 members. In those groups, 59 of 91 in the control group chose the model, and 23 of 90 who had seen the model perform chose the model.</p>
<div class="enhanceable_content tabs">
<ul>
<li style="border: solid 1px #C7CDD1;"><a style="font-size: 14pt; text-decoration: none;" href="#tab-1"><strong>R</strong></a></li>
<li style="border: solid 1px #C7CDD1;"><a style="font-size: 14pt; text-decoration: none;" href="#tab-2"><strong>G*Power</strong></a></li>
</ul>
<div id="tab-1" style="font-size: 1rem; border: solid 1px #C7CDD1; border-width: 0px 1px 1px 1px;">
<p>I will use the R command <code>power.prop.test</code> to do these calculations. That function requires that we provide all except one of the number of observations per group (n), the probability in one group (p1), the probability in the other group (p2), the power of the test and the significance level. The function then calculates the left-out parameter.</p>
<p>The power of that comparison was:</p>
<div class="cell">
<div id="cb1" class="sourceCode cell-code">
<pre class="sourceCode r code-with-copy" style="padding-left: 40px;"><code class="sourceCode r"><span id="cb1-1"><span class="fu">power.prop.test</span>(<span class="at">n=</span><span class="dv">90</span>, <span class="at">p1=</span><span class="dv">59</span><span class="sc">/</span><span class="dv">91</span>, <span class="at">p2=</span><span class="dv">23</span><span class="sc">/</span><span class="dv">90</span>)</span></code><i class="bi"></i></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
     Two-sample comparison of proportions power calculation 

              n = 90
             p1 = 0.6483516
             p2 = 0.2555556
      sig.level = 0.05
          power = 0.9998577
      alternative = two.sided

      NOTE: n is number in *each* group</code></pre>
</div>
</div>
<p>A alternative is to use the <code>pwrss</code> package. You can use the <code>pwrss</code> package either directly via the R command line or using this <a class="inline_disabled" href="https://pwrss.shinyapps.io/index/" target="_blank">Shiny app-based website</a>.</p>
<div class="cell">
<div id="cb1" class="sourceCode cell-code">
<pre class="sourceCode r code-with-copy" style="padding-left: 40px;">library(pwrss)<br><br>pwrss.z.2props(p1 = 59/91, p2 = 23/90, n2=90, kappa=1,<br>               alpha = 0.05,<br>               alternative = "not equal",<br>               arcsin.trans = TRUE<br>               )</pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
     Approach: Arcsine Transformation 
     Cohen's h = 0.812 
     Difference between Two Proportions 
     (Independent Samples z Test) 
     H0: p1 = p2 
     HA: p1 != p2 
     ------------------------------ 
      Statistical power = 1 
      n1 = 90 
      n2 = 90 
     ------------------------------ 
     Alternative = "not equal" 
     Non-centrality parameter = 5.447 
     Type I error rate = 0.05 
     Type II error rate = 0 
 </code></pre>
</div>
</div>
</div>
<div id="tab-2" style="font-size: 1rem; border: solid 1px #C7CDD1; border-width: 0px 1px 1px 1px;">
<p>In G*Power, we select the type of test that was conducted (z-test, difference between two independent proportions) and select the analysis as a post hoc calculation of achieved power. Pressing calculate gives us the result and a graph of the distributions.</p>
<p><img src="$IMS-CC-FILEBASE$/Uploaded%20Media/GPower-test-1.png" alt="GPower-test-1.png" width="80%" height="80%" data-api-endpoint="https://canvas.uts.edu.au/api/v1/courses/25795/files/6397086" data-api-returntype="File" loading="lazy"></p>
</div>
</div>
<p>The power of the test was over 99.9%.</p>
<p>However, this effect size may not be representative. The other studies in Dietvorst et al (2015) had smaller effect sizes, as did the replication of Study 3b by Jung and Seiter (2021).</p>
<p>The effect size of Dietvorst et al (2015) Study 3a was 16%, with 57% in the control and 41% in the model group choosing the model. Assuming that is the true effect size in Study 1, we would have the following power.</p>
<div class="enhanceable_content tabs">
<ul>
<li style="border: solid 1px #C7CDD1;"><a style="font-size: 14pt; text-decoration: none;" href="#tab-3"><strong>R</strong></a></li>
<li style="border: solid 1px #C7CDD1;"><a style="font-size: 14pt; text-decoration: none;" href="#tab-4"><strong>G*Power</strong></a></li>
</ul>
<div id="tab-3" style="font-size: 1rem; border: solid 1px #C7CDD1; border-width: 0px 1px 1px 1px;">
<div class="cell">
<div id="cb3" class="sourceCode cell-code">
<pre class="sourceCode r code-with-copy" style="padding-left: 40px;"><code class="sourceCode r"><span id="cb3-1"><span class="fu">power.prop.test</span>(<span class="at">n=</span><span class="dv">90</span>, <span class="at">p1=</span><span class="fl">0.57</span>, <span class="at">p2=</span><span class="fl">0.41</span>)</span></code><i class="bi"></i></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
     Two-sample comparison of proportions power calculation 

              n = 90
             p1 = 0.57
             p2 = 0.41
      sig.level = 0.05
          power = 0.5751626
      alternative = two.sided

      NOTE: n is number in *each* group<br></code></pre>
<p>Using the <code>pwrss</code> package:</p>
<div class="cell">
<div id="cb1" class="sourceCode cell-code">
<pre class="sourceCode r code-with-copy" style="padding-left: 40px;">pwrss.z.2props(p1 = 0.57, p2 = 0.41, n2=90, kappa=1,<br>               alpha = 0.05,<br>               alternative = "not equal",<br>               arcsin.trans = TRUE<br>               )</pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
     Approach: Arcsine Transformation 
     Cohen's h = 0.321 
     Difference between Two Proportions 
     (Independent Samples z Test) 
     H0: p1 = p2 
     HA: p1 != p2 
     ------------------------------ 
      Statistical power = 0.578 
      n1 = 90 
      n2 = 90 
     ------------------------------ 
     Alternative = "not equal" 
     Non-centrality parameter = 2.156 
     Type I error rate = 0.05 
     Type II error rate = 0.422</code></pre>
</div>
</div>
</div>
</div>
</div>
<div id="tab-4" style="font-size: 1rem; border: solid 1px #C7CDD1; border-width: 0px 1px 1px 1px;">
<p><img src="$IMS-CC-FILEBASE$/Uploaded%20Media/GPower-test-2.png" alt="GPower-test-2.png" width="80%" height="80%" data-api-endpoint="https://canvas.uts.edu.au/api/v1/courses/25795/files/6396972" data-api-returntype="File" loading="lazy"></p>
</div>
</div>
<p>If that effect size was the true effect size, that gives only 57% power.</p>
<p>Similarly, Jung and Seiter (2021) found that 67% of the control group and 47% of the treatment group chose the model. This is a smaller effect size than Study 1, but larger than that is Study 3b of Dietvorst et al.&nbsp;Again, if we assumed that is the true effect size in Study 1, we would have the following power.</p>
<div class="enhanceable_content tabs">
<ul>
<li style="border: solid 1px #C7CDD1;"><a style="font-size: 14pt; text-decoration: none;" href="#tab-5"><strong>R</strong></a></li>
<li style="border: solid 1px #C7CDD1;"><a style="font-size: 14pt; text-decoration: none;" href="#tab-6"><strong>G*Power</strong></a></li>
</ul>
<div id="tab-5" style="font-size: 1rem; border: solid 1px #C7CDD1; border-width: 0px 1px 1px 1px;">
<div class="cell">
<div id="cb5" class="sourceCode cell-code">
<pre class="sourceCode r code-with-copy" style="padding-left: 40px;"><code class="sourceCode r"><span id="cb5-1"><span class="fu">power.prop.test</span>(<span class="at">n=</span><span class="dv">90</span>, <span class="at">p1=</span><span class="fl">0.67</span>, <span class="at">p2=</span><span class="fl">0.47</span>)</span></code><i class="bi"></i></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
     Two-sample comparison of proportions power calculation 

              n = 90
             p1 = 0.67
             p2 = 0.47
      sig.level = 0.05
          power = 0.7780998
      alternative = two.sided

      NOTE: n is number in *each* group<br></code></pre>
<p>Using the <code>pwrss</code> package:</p>
<div class="cell">
<div id="cb1" class="sourceCode cell-code">
<pre class="sourceCode r code-with-copy" style="padding-left: 40px;">pwrss.z.2props(p1 = 0.67, p2 = 0.47, n2=90, kappa=1,<br>               alpha = 0.05,<br>               alternative = "not equal",<br>               arcsin.trans = TRUE<br>               )</pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
     Approach: Arcsine Transformation 
     Cohen's h = 0.407 
     Difference between Two Proportions 
     (Independent Samples z Test) 
     H0: p1 = p2 
     HA: p1 != p2 
     ------------------------------ 
      Statistical power = 0.779 
      n1 = 90 
      n2 = 90 
     ------------------------------ 
     Alternative = "not equal" 
     Non-centrality parameter = 2.73 
     Type I error rate = 0.05 
     Type II error rate = 0.221 </code></pre>
</div>
</div>
</div>
</div>
</div>
<div id="tab-6" style="font-size: 1rem; border: solid 1px #C7CDD1; border-width: 0px 1px 1px 1px;">
<p><img src="$IMS-CC-FILEBASE$/Uploaded%20Media/GPower-test-3.png" alt="GPower-test-3.png" width="80%" height="80%" data-api-endpoint="https://canvas.uts.edu.au/api/v1/courses/25795/files/6397074" data-api-returntype="File" loading="lazy"></p>
</div>
</div>
<p>This results in a power of 77%, which is below the common “rule of thumb” of 80% power.</p>
<p>For caution, let us assume that the smallest effect size, that in Dietvorst et al (2015) Study 3a is representative of the true effect size for Study 1. We can then calculate the sample size required to achieve 90% power as:</p>
<div class="enhanceable_content tabs">
<ul>
<li style="border: solid 1px #C7CDD1;"><a style="font-size: 14pt; text-decoration: none;" href="#tab-7"><strong>R</strong></a></li>
<li style="border: solid 1px #C7CDD1;"><a style="font-size: 14pt; text-decoration: none;" href="#tab-8"><strong>G*Power</strong></a></li>
</ul>
<div id="tab-7" style="font-size: 1rem; border: solid 1px #C7CDD1; border-width: 0px 1px 1px 1px;">
<p>Here I again use the R command <code>power.prop.test</code>, but in this case specify the desired power such that the function outputs the required sample size.</p>
<div class="cell">
<div id="cb7" class="sourceCode cell-code">
<pre class="sourceCode r code-with-copy" style="padding-left: 40px;"><code class="sourceCode r"><span id="cb7-1"><span class="fu">power.prop.test</span>(<span class="at">p1=</span><span class="fl">0.57</span>, <span class="at">p2=</span><span class="fl">0.41</span>, <span class="at">power=</span><span class="fl">0.9</span>)</span></code><i class="bi"></i></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
     Two-sample comparison of proportions power calculation 

              n = 203.0558
             p1 = 0.57
             p2 = 0.41
      sig.level = 0.05
          power = 0.9
      alternative = two.sided

      NOTE: n is number in *each* group<br></code></pre>
<p>Using the <code>pwrss</code> package:</p>
<div class="cell">
<div id="cb1" class="sourceCode cell-code">
<pre class="sourceCode r code-with-copy" style="padding-left: 40px;">pwrss.z.2props(p1 = 0.57, p2 = 0.41, kappa=1,<br>               alpha = 0.05, power=0.9,<br>               alternative = "not equal",<br>               arcsin.trans = TRUE<br>               )</pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
     Approach: Arcsine Transformation 
     Cohen's h = 0.321 
     Difference between Two Proportions 
     (Independent Samples z Test) 
     H0: p1 = p2 
     HA: p1 != p2 
     ------------------------------ 
      Statistical power = 0.9 
      n1 = 204 
      n2 = 204 
     ------------------------------ 
     Alternative = "not equal" 
     Non-centrality parameter = 3.242 
     Type I error rate = 0.05 
     Type II error rate = 0.1 </code></pre>
</div>
</div>
</div>
</div>
</div>
<div id="tab-8" style="font-size: 1rem; border: solid 1px #C7CDD1; border-width: 0px 1px 1px 1px;">
<p><img src="$IMS-CC-FILEBASE$/Uploaded%20Media/GPower-test-4.png" alt="GPower-test-4.png" width="80%" height="80%" data-api-endpoint="https://canvas.uts.edu.au/api/v1/courses/25795/files/6397075" data-api-returntype="File" loading="lazy"></p>
</div>
</div>
<p>Ninety per cent power would require a sample of 200.</p>
</section>
<div class="content-box pad-box-mini" style="background-color: #f5f5f5; padding: 20px;">
<p><strong>References</strong></p>
<p>Dietvorst, Simmons and Massey (2015) "Algorithm aversion: People erroneously avoid algorithms after seeing them err", Journal of Experimental Psychology: General, 144, 114–126, <a style="font-family: inherit; font-size: 1rem;" href="https://doi.org/10.1037/xge0000033">https://doi.org/10.1037/xge0000033</a></p>
<p>Jung and Seiter (2021) "Towards a better understanding on mitigating algorithm aversion in forecasting: An experimental study",&nbsp;<i>Journal of Management Control</i>, <i>32</i>(4), 495–516,&nbsp;<a href="https://doi.org/10.1007/s00187-021-00326-3">https://doi.org/10.1007/s00187-021-00326-3</a></p>
</div>
</body>
</html>