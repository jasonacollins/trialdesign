<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<title>5.2 Attrition</title>
<meta name="identifier" content="ga7eff442e34c9643f87c76020194a18a"/>
<meta name="editing_roles" content="teachers"/>
<meta name="workflow_state" content="unpublished"/>
</head>
<body>
<p><span style="font-size: 10pt;"><strong>Module 2: Running a trial</strong></span></p>
<p><img id="2772220" role="presentation" src="$IMS-CC-FILEBASE$/Updated%20banners/23717_Trial_Design_Banner-06-1.png" alt="" width="100%" height="100%" data-api-endpoint="https://canvas.uts.edu.au/api/v1/courses/25795/files/4158563" data-api-returntype="File" loading="lazy"></p>
<p>Attrition is the absence of data because the researchers are unable to collect some or all of the outcome measures from some people in the sample. This can occur when participants drop out or refuse to be interviewed. Attrition creates a problem of missing data.</p>
<p>Attrition can reduce the comparability of treatment and comparison groups. Consider the following example:</p>
<p>Imagine that a program increased the test scores of low-achieving students from an average score of 10 to an average score of 15 (image below). Overall, the average score for the class increased from 15 to 17.5. And because the low-achieving students in the treatment group were given support, they did not drop out of school, as many of their peers in the comparison group did. However, if we had measured the test scores only of those children who stayed in school, we would have concluded that the program worsened test scores, from 18.3 in the comparison group to 17.5 in the treatment group.</p>
<p><img role="presentation" src="$IMS-CC-FILEBASE$/Uploaded%20Media/001_TitleCopy%202-1.jpg" alt="" width="1015" height="364" data-api-endpoint="https://canvas.uts.edu.au/api/v1/courses/25795/files/5480509" data-api-returntype="File" loading="lazy"></p>
<p>Attrition also reduces the sample size, lowering statistical power.</p>
<div class="content-box pad-box-mini" style="background-color: #fbe9ff; padding: 20px;"><p>We can limit attrition by:</p>
<ul>
    <li>Using a research design that promises access to the program to all over time. People who anticipate receiving a program in the future may be less likely to refuse to participate in the study. We could use a design that creates this expectation (phase-in design)</li>
    <li>Changing the level of randomization. It is possible that if people see their neighbours being treated but are not treated themselves, this can lead to resentment, and resentment can lead to attrition.</li>
    <li>Improving data collection (survey design, administration, and tracking). Pilot the data collection instruments and procedures. Poorly designed data collection instruments and procedures can lead to missing data. Donâ€™t wait too long to follow up. The longer the gap between surveys, the higher the attrition rate is likely to be.</li>
    <li>Providing incentives. Small incentives can have large impacts on behaviour</li>
</ul></div>
<div style="background-color: #f5f5f5; padding: 15px 30px; margin-bottom: 20px; margin-top: 20px;">
    <p><strong>References</strong></p>
    <p><i class="icon-link icon-solid" style="color: #8826ab;"></i><span>Glennerster, R., &amp; Takavarasha, K. (2013) "</span><a class="inline_disabled" href="https://doi-org.ezproxy.lib.uts.edu.au/10.2307/j.ctt4cgd52.11" target="_blank">Threats</a><span>" in Running Randomized Evaluations: A Practical Guide (pp. 298-323). Princeton University Press</span></p>
</div>
</body>
</html>