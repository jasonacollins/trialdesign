<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<title>15.1 Generalisability: effect sizes</title>
<meta name="identifier" content="g122969dad378dd42b6fdf8627e2e46dc"/>
<meta name="editing_roles" content="teachers"/>
<meta name="workflow_state" content="active"/>
</head>
<body>
<p><img id="4158518" src="$IMS-CC-FILEBASE$/Updated%20banners/23717_Trial_Design_Banner-17.png" alt="23717_Trial_Design_Banner-17.png" data-api-endpoint="https://canvas.uts.edu.au/api/v1/courses/25795/files/4158518" data-api-returntype="File" loading="lazy"></p>
<div class="ic-flash-info" style="border-color: #7519d1; max-width: 100%; text-align: center;">
<p>A term often used in substitute of validity, particularly external validity, is generalisability. The term is most commonly used when describing whether the results of a field trial can be taken to inform how an intervention might work in another context or at scale.</p>
</div>
<p>There are limited systematic evaluations of the generalisability of applied behavioural science trials. However, research on the generalisability of impact evaluations in a development context provides insight into this question.</p>
<div class="content-box">
<div class="grid-row">
<div class="col-xs-12 col-md-6">
<div class="styleguide-section__grid-demo-element">
<p>Eva Vivalt (2020) reviewed 635 papers containing 15,024 estimates of effect sizes relating to 20 types of interventions in international development. In her paper, she assessed the extent the results from a particular intervention could be used to predict the sign of the effect or the magnitude of the effect of a similar study in another context. This question is effectively an examination of the <strong>Type M</strong> and <strong>Type S</strong> errors we discussed.</p>
<p>She found that inference about a study's effect using another similar study will have the correct sign 61% of the time (comparing the median intervention-outcome pair in each study). When comparing effects, a naive prediction of the result in the new study is likely to be wrong by about 249%.</p>
</div>
</div>
<div class="col-xs-12 col-md-6">
<div class="styleguide-section__grid-demo-element">
<p><span style="font-size: 14pt;">&nbsp;<strong>Optional Listening </strong><span style="font-size: 14pt;"><span class="pill" style="font-size: 10pt;">&nbsp; 2 hours&nbsp;&nbsp;</span> </span> </span></p>
<p>Listen to Eva Vivalt on the 80,000 Hours podcast, or read <a href="https://80000hours.org/podcast/episodes/eva-vivalt-social-science-generalizability/" target="_blank">the transcript on the webpage</a>.</p>
<p><iframe src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/444213924&amp;color=%23ff5500&amp;auto_play=false&amp;hide_related=false&amp;show_comments=true&amp;show_user=true&amp;show_reposts=false&amp;show_teaser=true" width="100%" height="166" allow="autoplay" loading="lazy"></iframe></p>
</div>
</div>
</div>
</div>
<p>Another perspective on generalisability comes from analysis by DellaVigna and Linos (2022) of the implementation of behavioural interventions by two "Nudge Units" in the United States. They compared the results from 126 randomised controlled trials run by the Nudge Units to a sample of trials in academic journals. They wrote:</p>
<blockquote>
<p>In the Academic Journals papers, the average impact of a nudge is very large—an 8.7 percentage point take-up effect, which is a 33.4% increase over the average control. In the Nudge Units sample, the average impact is still sizable and highly statistically significant, but smaller at 1.4 percentage points, an 8.0% increase. We document three dimensions which can account for the difference between these two estimates: (i) statistical power of the trials; (ii) characteristics of the interventions, such as topic area and behavioral channel; and (iii) selective publication. A meta-analysis model incorporating these dimensions indicates that selective publication in the Academic Journals sample, exacerbated by low statistical power, explains about 70 percent of the difference in effect sizes between the two samples. Different nudge characteristics account for most of the residual difference.</p>
</blockquote>
<div style="background-color: #f5f5f5; padding: 15px 30px;">
<p><strong>References</strong></p>
<p>DellaVigna and Linos (2022) "RCTs to Scale: Comprehensive Evidence From Two Nudge Units", <em>Econometrica</em>, 90(1), 81–116. <a href="https://doi.org/10.3982/ECTA18709" target="_blank">https://doi.org/10.3982/ECTA18709</a>&nbsp;</p>
<p>Vivalt (2020) "How Much Can We Generalise From Impact Evaluations", <em>Journal of the European Economic Association</em>, 18(6), 3045–3089,&nbsp;&nbsp;<a href="https://doi.org/10.1093/jeea/jvaa019">https://doi.org/10.1093/jeea/jvaa019</a></p>
</div>
</body>
</html>