{
  "hash": "221f31a0007826153994c80423cdf746",
  "result": {
    "engine": "knitr",
    "markdown": "# An example data analysis\n\nIn this part, I will illustrate the data analysis process by reproducing the analysis in Study 1 of Dietvorst, Simmons and Massey's [-@dietvorst2015] paper on algorithm aversion. The experiment tested the hypothesis that \"seeing the model perform, and therefore err, would decrease participants' tendency to bet on it rather than the human forecaster, despite the fact that the model was more accurate than the human.\"\n\nExperimental participants were given a judgment task, with one group allowed to see the algorithm in action before undertaking the task. Participants were given the option of using the algorithm's predictions or their own. Those who had seen the algorithm perform were less likely to use it in the task.\n\n## The experimental design\n\nParticipants were informed that they would play the part of an MBA admissions officer with a task to forecast the success of each applicant (as a percentile). Success was defined as an equal weighting of GPA, respect, and employer prestige.\n\nParticipants were also told that \"thoughtful analysts\" had built a statistical model to forecast performance using data that the participants would receive.\n\nAfter consent and information provision, participants were allocated into one of four conditions: three treatment conditions and a control.\n\nParticipants in each of the three treatment conditions saw and/or made 15 forecasts on which they received feedback on performance.\n\nAll participants, including the control, were asked whether they would like to use their own or the model's forecasts for all 10 incentivised forecasts. After their choice, participants would make the forecasts (even if they had chosen the model for awarding incentives). After making their forecasts, they were asked questions about their confidence in their own forecasts and that of the model. They then learned the bonus earned.\n\nThere were four experimental conditions, with the difference in conditions occurring in the first stage of the experiment. For each of the 15 MBA applicants, they would gain experience as follows.\n\n- **Model:** Participants would get feedback showing the model's prediction and the applicant's true percentile\n- **Human:** Participants would make a forecast and then get feedback showing their own prediction and the applicant's true percentile\n- **Model and human:** Participants would make a forecast and then get feedback showing their own prediction, the model's prediction, and the applicant's true percentile.\n- **Control:** No experience.\n\n![Experimental flow diagram](img/flow.png)\n\nBesides illustrating the analysis process for this subject, reproducing the analysis of a paper has other benefits.\n\nFirst, it allows you to check that the original analysis is correct. This is often not the case. Most times the errors are minor, but sometimes the original finding may not even hold. In that case, there may be limited value to the replication. You may also determine that an inappropriate analysis methodology has been used.\n\nSecond, reproducing the analysis is another way to understand better the experimental methodology and what the analysis shows. You can see what variables were recorded and how they are used.\n\nIt is normally only possible to reproduce the analysis if the original data is available on a public repository or from the authors.\n\n## Reproducing the algorithm aversion paper\n\nThe data for Dietvorst et al. (2015) is available from [ResearchBox](https://researchbox.org/379&PEER_REVIEW_passcode=MOQTEQ) and the [Journal of Experimental Psychology: General website](https://supp.apa.org/psycarticles/supplemental/xge0000033/xge0000033_supp.html).\n\nThe download includes the Stata code used by Dietvorst et al. (2015) to analyse the data. I do not know Stata, so I asked [ChatGPT](https://chat.openai.com) to translate the code into R. While that translation contained some errors, I made it operational with only minor changes.\n\nIn what follows I provide the R code that I used to reproduce the analysis. You can expand each \"Code\" item to see the code or copy it to replicate the results yourself.\n\n## Analysis\n\nI start by downloading and unzipping the data file and loading the CSV for Study 1 into the R environment. I then made some amendments to the variable names to make them more readable (e.g. replacing the numbers for each condition with the words they represent: 1=control, etc.).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data downloaded from https://researchbox.org/379&PEER_REVIEW_passcode=MOQTEQ\n\n#Load data from csv\ndata <- read.csv(\"data/Study 1 Data.csv\", header = TRUE, sep = \",\", na.strings = \".\")\n\n# Change to camel case\ncolnames(data) <- paste(tolower(substring(colnames(data), 1, 1)), substring(colnames(data), 2), sep = \"\")\nnames(data)[1] <- \"ID\"\n\n# Define label mappings\ncondition <- c(\"1\" = \"control\", \"2\" = \"human\", \"3\" = \"model\", \"4\" = \"model&human\")\nmodelBonus <- c(\"0\" = \"choseHuman\", \"1\" = \"choseModel\")\nbinary <- c(\"0\" = \"no\", \"1\" = \"yes\")\nbetterStage2Bonus <- c(\"1\" = \"model\", \"2\" = \"equal\", \"0\" = \"human\")\nconfidence <- c(\"1\" = \"none\", \"2\" = \"little\", \"3\" = \"some\", \"4\" = \"fairAmount\", \"5\" = \"aLot\")\n\n# Apply label mappings to variables - have done more here than required for analysis, but might be useful later\ndata$condition <- factor(data$condition, labels = condition)\ndata$modelBonus <- factor(data$modelBonus, labels = modelBonus)\ndata$humanAsGoodStage1 <- factor(data$humanAsGoodStage1, labels = binary)\ndata$humanBetterStage1 <- factor(data$humanBetterStage1, labels = binary)\ndata$humanBetterStage2 <- factor(data$humanBetterStage2, labels = binary)\ndata$betterStage2Bonus <- factor(data$betterStage2Bonus, labels = betterStage2Bonus)\ndata$model <- factor(data$model, labels = binary)\ndata$human <- factor(data$human, labels = binary)\ndata$modelConfidence <- factor(data$modelConfidence, labels = confidence)\ndata$humanConfidence <- factor(data$humanConfidence, labels = confidence)\n```\n:::\n\n\nI then run a quick check on the number of observations. I note from the paper that I should find 369 participants at the start, but 8 of these participants did not complete enough questions to get to the dependent variable. That leaves 361 participants across each of the conditions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Total participants\nlength(data$ID)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 369\n```\n\n\n:::\n\n```{.r .cell-code}\n# Number of participants who chose between model and human\ntable(data$modelBonus)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nchoseHuman choseModel \n       201        160 \n```\n\n\n:::\n:::\n\n\nAs expected, I find 369 participants and 201+160=361 dependent variable observations.\n\nI then broke down the dependent variable findings in more detail to see what each participant chose (human or model) in each condition.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Choice of participants in each condition\n\nconditions <- table(data$modelBonus, data$condition)\nconditions <- rbind(conditions, total=colSums(conditions))\nconditions\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           control human model model&human\nchoseHuman      32    33    67          69\nchoseModel      59    57    23          21\ntotal           91    90    90          90\n```\n\n\n:::\n:::\n\n\nTwo chi-squared tests were then run to see if there was a significant difference between the conditions. The results are shown in the Study 1 diagram in Figure 3.\n\nThe first was a test of whether there was a difference between the two conditions without and the two conditions with the model. This is a test of the core hypothesis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# chisq.test between conditions 1 and 2 (control and human) and 3 and 4 (model and model&human)\n\nchisq.test(table(data$modelBonus, data$model), correct=FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's Chi-squared test\n\ndata:  table(data$modelBonus, data$model)\nX-squared = 57.477, df = 1, p-value = 3.419e-14\n```\n\n\n:::\n:::\n\n\nThe second was a test of whether there was a difference between the two conditions without and the two conditions with the human experience. This test was run to see whether the experience in seeing their own errors changed the participants' choices.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# chisq.test between conditions 1 and 3 (control and model) and 2 and 4 (human and model&human)\n\nchisq.test(table(data$modelBonus, data$human), correct=FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's Chi-squared test\n\ndata:  table(data$modelBonus, data$human)\nX-squared = 0.14201, df = 1, p-value = 0.7063\n```\n\n\n:::\n:::\n\n\nI then plotted that data in a bar chart to illustrate the results. This matches Figure 3 in the paper.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## load ggplot2 for chart and tidyverse packages for data manipulation\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# convert table to data frame for future operations\nconditions <- as.data.frame.matrix(conditions)\n\n#percentage choosing model in each condition\n\nconditions <- rbind(conditions, percent=conditions[2,]/conditions[3,])\nconditions <- tibble::rownames_to_column(conditions, \"variable\")\nconditions_long <- pivot_longer(conditions, cols = -variable, names_to = \"treatment\", values_to = \"value\", cols_vary = \"slowest\")\nconditions_long <- conditions_long[c(2, 1, 3)]\n\n# ggplot2 bar plot of percent rows of conditions data frame\nplot_data <- filter(conditions_long, variable==\"percent\")\n\nggplot(plot_data, aes(x=treatment, y=value)) +\n  geom_col()+\n  labs(title=\"Percent choosing model in each condition\", x=\"Condition\", y=\"Percent choosing model\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n\n  # Set the theme\n  theme_minimal()+\n  geom_hline(yintercept = 0, linewidth=0.25)\n```\n\n::: {.cell-output-display}\n![](an-example-data-analysis_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nFinally, I replicate the analysis related to Study 1 as shown in Table 3.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate human and model mean rewards\nm<-round(mean(data$bonusFromModel, na.rm=TRUE), 2)\nh<-round(mean(data$bonusFromHuman, na.rm=TRUE), 2)\n\n# Bonus if chose model vs. human\nbonusModel <- t.test(data$bonusFromModel, data$bonusFromHuman, paired=TRUE)\np <- signif(bonusModel$p.value, 2)\nt <- round(bonusModel$statistic, 2)\nd <- round(bonusModel$estimate, 2)\n\n# Participants' AAE compared to model's AAE for stage 1 forecasts\nstage1AAE <- t.test(data$humanAAE1, data$modelAAE1, paired=TRUE)\n\np1 <- signif(stage1AAE$p.value, 2)\nt1 <- round(stage1AAE$statistic, 2)\nd1 <- round(stage1AAE$estimate, 2)\n\nAAEm<-round(mean(data[data$condition=='model&human', 'modelAAE1'], na.rm=TRUE), 2)\nAAEh<-round(mean(data[data$condition=='model&human', 'humanAAE1'], na.rm=TRUE), 2)\n\n# Participants' AAE compared to model's AAE for stage 2 forecasts\nstage2AAE <- t.test(data$humanAAE2, data$modelAAE2, paired=TRUE)\n\np2 <- signif(stage2AAE$p.value, 2)\nt2 <- round(stage2AAE$statistic, 2)\nd2 <- round(stage2AAE$estimate, 2)\n\nAAEm2<-round(mean(data$modelAAE2, na.rm=TRUE), 2)\nAAEh2<-round(mean(data$humanAAE2, na.rm=TRUE), 2)\n```\n:::\n\n\nTable 3 results\n\n|               | Model          | Human          | Difference | t score | p value |\n|---------------|----------------|----------------|------------|---------|---------|\n| Bonus         | $1.78         | $1.38         | $0.4     | 4.98   | 9.7\\times 10^{-7}   |\n| Stage 1 error | 23.13       | 26.67       | 3.53     | 5.52  | 3.3\\times 10^{-7}  |\n| Stage 2 error | 22.07      | 26.61      | 4.54     | 11.52  | 2.4\\times 10^{-26}  |\n\nThe numbers match those provided in the paper.\n\n## Regression\n\nAn alternative approach is to do the comparison between treatment and comparison groups within a regression framework. This will give us the same results but in a different format.\n\nTo determine whether there is a treatment effect, we use the regression:\n\n$$\nY_i=c+\\beta T_i+\\epsilon_i\n$$\n\nWhere $Y_i$ is the number of participants that chose the model, $c$ is a constant, and $T_i$ is a treatment dummy. $T_i=0$ if participant $i$ is in the control group and 1 if participant $i$ is in the treatment group. $\\beta$ is the coefficient of the treatment dummy.\n\nLogistic regression is used here as we are considering the probability of an event taking place (choosing the model). If we were considering a continuous variable (e.g. amount saved due to an intervention), we would use linear regression, which is typically easier to interpret.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Regression to determine whether there is a treatment effect\nreg <- glm(modelBonus ~ model, data=data, family=binomial)\n\n# Show results\nsummary(reg)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = modelBonus ~ model, family = binomial, data = data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   0.5792     0.1549   3.738 0.000185 ***\nmodelyes     -1.7077     0.2326  -7.343 2.09e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 495.79  on 360  degrees of freedom\nResidual deviance: 436.57  on 359  degrees of freedom\n  (8 observations deleted due to missingness)\nAIC: 440.57\n\nNumber of Fisher Scoring iterations: 4\n```\n\n\n:::\n:::\n\n\nThe p-value of 2.09e-13 is not identical to that from the chi-squared test above (3.419e-14) as there are slight differences in the underlying tests, but either way, the result is significant. We can see the equivalence of the underlying result by examining the parameter estimates.\n\nThe intercept of 0.5792 implies that the odds of choosing the model are $e^{0.5792}$ for those in the control condition, which is equal to the observed proportion of 116 choosing the model compared to 65 choosing the human (116/65=1.784). The estimate of $\\beta$ of -1.7077 implies that the ratio of the odds of those choosing the model in the control treatment condition relative to those in the control condition is $e^{-1.7077}=0.181$, which is equal to the observed proportion of 44 choosing the model and 136 choosing the model in the treatment condition, compared to the 116 choosing the model and 65 choosing the human in the control condition ((44/136)/(116/65)=0.181).\n\nThe benefit of using a regression approach is that other covariates can be added to the analysis. For example, if there was a difference between male and female respondents, we could add a dummy for gender to the analysis.\n\nIncluding covariates in the regression can give us a more precise estimate of the impact of the intervention as they can reduce unexplained variance. However, including covariates can reduce the precision of our estimate of the treatment effect if they don't explain enough error variance.\n\nOften, the best covariate to include is a baseline measure of the outcome variable. With stratified randomisation, it is often recommended to add an indicator for each different stratum.\n\n",
    "supporting": [
      "an-example-data-analysis_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}