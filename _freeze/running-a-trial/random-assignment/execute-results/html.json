{
  "hash": "29be1da7f794022f03a3af99283d0a5c",
  "result": {
    "engine": "knitr",
    "markdown": "# Random assignment\n\nIn its simplest form, randomised trials work by splitting trial participants into two or more groups by random lot. Each group is then given a different intervention, with one of those groups typically a \"control group\" or \"test group\" that receives no intervention or the status quo.\n\nGroups might be allocated randomly by drawing numbers out of a hat, flipping a coin or, more commonly in experimental work, using a pseudo-random number generator. It is not the experimenter that decides who gets an intervention or not, but rather chance.\n\nRandomisation works as a control technique because it enables experimenters to hold approximately equal the sources of experimental bias, such as uncontrolled variables, between the control and treatment groups. These might even be variables of which we are ignorant.\n\nIn *Uncontrolled: The Surprising Payoff of Trial-and-Error for Business Politics and Society*, Jim Manzi [-@manzi2012] gives the following example:\n\n> \\[S\\]uppose researchers in 1950 wanted to test the efficacy of a pill designed to reduce blood pressure but did not know that about 10 percent of the human species has a specific gene variant that predisposes them to adult-onset hypertension. If the researchers selected 3,000 people, and randomly assigned 1,500 to a test group who are given the pill and 1,500 to a control group who received a placebo, then about 150 patients in each group should have the gene variant of interest (though the researchers would have no explicit information about this and wouldn't even have thought to investigate it). Therefore, when these researchers compared the change in blood pressure before and after taking the pill for the test group versus the control group, their estimate would not be biased by a much higher proportion of patients with the gene variant of interest in one group or the other\n\nRandomisation relies on the law of large numbers, the idea that as sample size increases the sample average converges to the expected value. In the case of the experiment to reduce blood pressure, as the size of the groups increased, we would expect the proportion of people with the hypertension genetic variant to converge to around 10% in each group.\n\nTo think of what that means intuitively, if you had only ten in each group, there is a material chance the groups could have zero, one, or more people with the hypertension variant, although it would rarely be three or more. Therefore, with a small sample, the relative proportions of the variant vary markedly between groups. It is only be collecting large samples that we can expect the groups to approximately equal proportions.\n\n![@haynes2012, Figure 3](img/TLA_Figure_3.jpg)\n\nAfter randomisation and application of the treatment, outcomes are then measured for each group. Within certain statistical parameters (also to be covered later in this unit), we can then take the differences between the two groups to be as a result of the different interventions we received. The treatment \"caused\" the differences, although as noted above, we may not understand the mechanism.\n\nYou can read more about why we randomise in @glennerster2013b.\n\n## What is involved in practice?\n\nIn practice, random assignment includes the following steps:\n\n-   Define those eligible for a program.\n-   Decide the level of randomisation (such as individuals, communities, schools, call centres, bank branches). When you design a randomized evaluation, you need to decide whether to randomise individuals in and out of the program or to randomise the whole group in and out of the program.\n-   Power Analysis: Decide the sample size.\n-   Randomly assign which units (individuals or groups) are in the treatment, and the control group. Groups might be allocated randomly by drawing numbers out of a hat, flipping a coin or, more commonly in experimental work, using a pseudo-random number generator. It is not the experimenter that decides who gets an intervention or not, but rather chance.\n\nWe will cover some of these steps, such as power analysis, in more detail later in these notes.\n\n## Random sample versus random assignment\n\nIn both **random assignment** and **random sampling**, we use a random process to select units (individuals, households, schools, etc.). But there is a crucial difference.\n\nIn **random sampling**, we take a population and use a random process to select units and create a group that is representative of the entire population. We can then measure the characteristics of this group and infer from them the characteristics of the entire population. (Most surveys use this approach.)\n\nIn **random assignment**, we take a study population—a pool of eligible units—and use a random process to assign units to different groups, such as treatment and comparison groups (i.e., we use it to determine access to a program).\n\n**Random assignment:**\n\n-   Units (people, schools, etc.) are randomly assigned to different groups (e.g. treatment and comparison).\n-   Creates two or more comparable groups.\n-   Basis of randomised evaluation.\n\n**Random sampling:**\n\n-   Want to measure the characteristics of a group (e.g. average height).\n-   Measure a random sample of the group.\n-   Often used during randomised evaluations, especially group level randomisation.\n\n## What can be randomised?\n\nThere are three basic ways in which a program can be randomised.\n\n**Access**\n\nWe can choose which people are offered **access** to a program. This is the most common way to randomise.\n\nFor example: \"Draw a list of 200 eligible branches and then randomly select 100 to receive the new product.\"\n\n**Timing**\n\nWe can choose **when** people are offered access.\n\nSometimes, everyone needs access to the program (e.g. by law, fairness). We can randomly assign the time when people get access to the program.\n\nFor example, suppose a new medical treatment is proposed. Group A could get the treatment in Year 1, Group B in Year 2 and Group C in Year 3. Groups B and C are control groups in Year 1. Group C is a control group in Year 2.\n\nPhase-in designs may result in \"anticipatory effects\". The anticipation of treatment may affect the behaviour of the control group, leading to an overestimate or underestimate of the impact.\n\nFor example, consider a program that provides a laptop to each student at schools. Suppose some parents of children in the control group that planned to purchase a home computer before the evaluation decided not to purchase it and wait for their child to receive a laptop. This behavioural change is important: some parents in control group behaved differently than they would have if the program did not exist. They are no longer the best representation of the counterfactual.\n\nAnother implication of phase-in designs is that, since the control group receives treatment after a fixed time frame, there is a limited time over which impact can be measured. The evaluation of the program’s impact will be of short-term outcomes.\n\n**Encouragement**\n\nWe can choose which people are **encouraged** to participate\n\nEncouragement might be a small incentive, a letter, or a phone call that reminds people of their eligibility and details the steps to enrol in the program. Messaging is a form of encouragement design.\n\nEncouragement is useful to evaluate a program already open to all eligible recipients, but only some are currently using it. Effective encouragement leads to higher take-up of the program in the treatment group than in the control group.\n\nIt is important to note that the impact of receiving encouragement to take up the program is evaluated (and its indirect effect on program take-up), rather than the direct impact of the program itself.\n\nWhen studying the program's impact, comparing the entire treatment group to the entire control group is important. When analysing the results, individuals in the treatment group who receive encouragement but do not apply for the program must still be considered a part of the treatment group. Similarly, individuals in the control group who apply for the program without special encouragement must remain in the control group for analysis.\n\nEncouragement designs attract the following considerations:\n\n-   The program to be evaluated must be undersubscribed.\n\n-   To generate impact estimates, the encouragement must induce significantly higher take-up rates in the treatment group compared to the control group.\n\n-   The encouragement should not have a direct effect on the outcome.\n\n-   Everyone must be affected by the encouragement incentive in the same direction. If the encouragement itself increases the take-up of some groups and reduces the take-up of others, impact estimates will likely be biased.\n\nYou can read more about how to randomise in @glennerster2013c.\n\n## Opportunities to randomise\n\n@glennerster2013c listed ten of the most common opportunities to randomise:\n\n1.  **New program design:** A problem has been identified, and a new solution is required. This might provide an opportunity to contribute to the program design and then test.\n\n2.  **New programs:** When a program is new, and its effects are unknown, a randomised evaluation is the best way to estimate its impact.\n\n3.  **New service:** When an existing program offers a new service, rollout of the new service can be randomised.\n\n4.  **New people:** Programs often expand by adding new people. When there are not enough resources for everyone, randomising may be the fairest way to decide who will be served first. For example, Oregon had limited funding to extend Medicaid (health insurance for low-income people) to more people. They held a lottery to decide who would be offered the service.\n\n5.  **New locations:** Programs often expand to new locations. People in the new location may be added progressively through randomisation.\n\n6.  **Oversubscription:** When demand outstrips supply, random assignment may be the fairest way to choose participants. The government in Colombia uses a lottery to decide who receives secondary school tuition vouchers. The US government uses a lottery to decide who receives vouchers for housing in more affluent neighbourhoods.\n\n7.  **Undersubscription:** When the take-up of a program is low, we can increase demand by encouraging a random group. For example, a large American university encourages some employees to attend an information session on their retirement savings program.\n\n8.  **Rotation:** If people take turns accessing the program, the order in which program resources rotate can be decided randomly, with the group on rotation serving as the treatment group. If we do this, we need to consider effects lingering after the program has rotated to another group.\n\n9.  **Admission cutoffs:** Some programs have admission cutoffs. Those just below the cutoff could be randomly admitted. For example. if a bank approves loans to applicants with a credit score above 60, a random selection of applicants with scores between 55 and 59 could be approved. Those with scores in that range but not approved would form the control group.\n\n10. **Admission in phases:** If resources are going to grow, people may be admitted to the program as resources become available.\n\n## Level of randomisation\n\nA question that you might need to consider is the level of randomisation. Do you randomise individuals, or do you cluster? In cluster randomisation, groups of subjects are randomised. For example, if you have 20 call centres, randomise those 20 into the treatment and control rather than individual call centre staff.\n\nCluster randomisation can introduce greater complexity into the analysis, including potential intracluster correlation that should be accounted for. @glennerster2013c identify considerations as to the appropriate level of randomisation as including\\:\n\n1. Unit of measurement: What is the unit at which your outcomes will be measured? Randomisation cannot occur at a lower level than our measured outcome. For example, we need to randomise at the firm (not the worker) level to study the effect of worker training on firm profits.\n\n2. Spillovers: Are there spillovers in that the intervention changes outcomes for those not directly participating? Do we want the impact estimate to capture them? Do we want to measure them? Cluster randomisation is used to control for contamination across individuals, as a change in behaviour in one might change the behaviour of others. For example, suppose you are implementing a trial to improve on-time tax return submissions by phoning taxpayers. You want to test what scripts and tools for call centre staff most effectively increase on-time submission. If you randomise at the staff--member level, staff in the control group may become aware of other approaches in their centre and change their behaviour.\n\n3. Attrition: Attrition is when we cannot collect data on all our sample. Which level is best to keep participants from dropping out of the sample? For example, sometimes providing benefits to a person but not their neighbour is considered unfair. If people feel a study is unfair, they can refuse to cooperate, leading to attrition. Randomising at a higher level can help.\n\n4. Compliance: compliance is when all those in the treatment group get the program and all those in the comparison don’t. Which level is best to limit departures from the study protocol? For example, if iron supplements are provided to one family member, what if they share with others? They would receive less than intended and the other family member more.\n\n5. Statistical power: Statistical power is the ability to detect an effect of a given size. Which level gives us the greatest probability of detecting a treatment? Cluster randomisation reduces power by effectively reducing the sample size.\n\n6. Feasibility: Which level is ethically, financially, politically and logistically feasible?\n\n   - Ethics: Randomising at an individual level may create tensions that lead to harm if some are seen as unfairly chosen. Randomising at the group level raises issues about seeking informed consent.\n   - Politics: Allocation may seem arbitrary when people with equal needs interacting regularly are assigned to different groups.\n    - Logistics: Sometimes, it is not practicable to randomise experimental subjects individually. In the staff training example above, training in the tools is provided at staff briefings at the beginning of each shift, making it impracticable to randomise across call centre staff within the centre.\n    - Cost: If you randomise at the suburb level, we usually have to treat everyone in the suburb.\n\n## Three research designs\n\nWe have discussed three aspects of the intervention that we can randomise: access, timing and encouragement.\n\nAnother dimension to consider is how we can create variation in exposure to the program. Here I discuss three: the basic lottery, a lottery around the cutoff and the phase-in design.\n\n**The basic lottery**\n\nThe basic lottery randomises access to the intervention and leaves the treatment status unchanged throughout. We compare those with and without access to the intervention.\n\nThe basic lottery is most workable when a program is oversubscribed, resources are constant for the evaluation period, and it is acceptable that some receive no intervention.\n\nAdvantages are that it is familiar, understood, generally seems fair, easy to implement and allows for estimation of long-term impacts.\n\nA disadvantage is differential attrition, as units in the control group may have little reason to cooperate with the survey.\n\n**Lottery around the cutoff**\n\nA lottery around the cutoff randomises the intervention among those close to an eligibility cutoff. This strategy is workable when eligibility is determined by a scoring system, and there are many participants.\n\nA disadvantage of this strategy is that it measures the impact of the intervention only on those close to the eligibility cutoff.\n\n**Phase-in**\n\nThe phase-in strategy randomises the timing of access and switches groups from control to treatment over time. It compares those with access to those who have not yet received access. This strategy is most workable when everyone must eventually receive the program and resources are growing over time.\n\nAn advantage of this approach is that phased roll-outs of interventions are common, and the control group may be more willing to cooperate in anticipation of future benefits.\n\nA disadvantage is that the control group is of limited duration, meaning there is a limited time over which impact can be measured. Anticipation of the treatment may also affect the behaviour of the control group.\n\n**Question**\n\nOnly 5% of people in your company are taking advantage of a new mindfulness program provided by your employer. You would like to evaluate if this program improves employee productivity.\n\nWhat type of design would be suitable for this evaluation?\n\n\n::: {.cell}\n\n:::\n\n\n<div class='webex-radiogroup' id='radio_NQKRBARYOJ'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_NQKRBARYOJ\" value=\"\"></input> <span>A phase-in design</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_NQKRBARYOJ\" value=\"answer\"></input> <span>an encouragement design</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_NQKRBARYOJ\" value=\"\"></input> <span>A lottery that would give access to the program to some employees</span></label></div>\n\n\n\n<div class='webex-solution'><button>Click here to see an explanation</button>\n\n\nAn encouragement design would be suitable as the program is available to everyone but not taken up.\n\n\n</div>\n",
    "supporting": [
      "random-assignment_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}