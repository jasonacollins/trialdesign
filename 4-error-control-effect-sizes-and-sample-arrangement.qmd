# Error control, effect sizes and sample arrangement

In this chapter, I explore some particular elements of hypothesis testing. I will discuss how to reduce error, interpret effect size and arrange the random sample. I will close the chapter with a discussion of how pre-registration and pre-analysis plans can improve our practices.

## Type 1 and Type 2 errors

Statistical tests are often thought of in terms of the errors they can generate.

The first error is where the test rejects a null hypothesis that is true. You find an effect where none exists. This is known as a **Type I error**, or false positive.

We set the rate at which Type I errors occur. The significance level $\alpha$ is the rate of Type I errors. If we use a significance level of 0.05, we have a 5% probability of rejecting the null hypothesis when it is true, generating a Type I error.

The second error is when we fail to reject a false null hypothesis. We do not find an effect where one exists. This is known as a **Type II error**, or false negative.

The Type II error rate is unknown, but can be calculated if we make a number of assumptions. We will examine this in the following pages. The type II error rate is denoted by $\beta$.

The relationship between these errors and correct inference is shown in the following table.

|                        | Null hypothesis ($H_0$) is true                       | Null hypothesis ($H_1$) is false                      |
|------------------------|------------------------|------------------------|
| **Don't reject** $H_0$ | True negative. Probability = $1-\alpha$               | Type II error (false negative). Probability = $\beta$ |
| **Reject** $H_0$       | Type I error (false positive). Probability = $\alpha$ | True positive. Probability = $1-\beta$                |

The following diagrams provide another view on these errors.

As per our running example, suppose we are estimating two sample means for how many people submit their tax return on time. We want to know whether the difference between them represents a true effect of the intervention. Let us suppose that the null hypothesis is true and there is no effect.

As we have discussed, the estimate of the effect is with error. Our estimate may vary from the true value. That estimate will fall within a probability distribution of mean $\mu$ and standard deviation $\frac{\sigma}{\sqrt{n}}$. The curve below represents that probability distribution.

When we set $\alpha=0.05$, we are setting a critical value such that there is a 0.05 chance that the estimate will be above the critical value, despite the null hypothesis being true. The red shaded area is the probability of type I error.

\[For this version of the diagram, only show the bell curve on the left. Remove the curve on the right and the shaded green area. Change "Any mean" to "Critical value".\]

![](img/errors.jpg)

Let us now assume there is an effect of our intervention. The alternative hypothesis is true.

Our estimate of this effect will again be with error, falling within a probability distribution of mean $\mu$ and standard deviation $\frac{\sigma}{\sqrt{n}}$, but this time with $\mu$ representing a positive effect. This is represented by the curve on the right.

As you can see in the diagram, there is a probability that even if the effect is true, the measured value of the effect will fall below the critical value. You will fail to reject the null even though the alternative hypothesis is true. The green shaded area represents this probability of a Type II error.

\[Tweaks to make to diagram: change "Any mean" to "Critical value"; delete "Null" and "Theoretical non-null value" with $\bar{x}_1$\]

![](img/errors.jpg)

Versions of diagram: 1. As in week 3.4 - normal distribution 2. As in week 3.5 - critical value 3. As here - version just null hypothesis curve 4. As here - version both curves 5. (for 4.3) Shifting H1 curve to the left or right - illustrate more/less chance of type II error if small/large effect size 6. (for 4.3) Larger sample size - reduce chance of type II error - bell curves getting taller and narrower, so green area shrinks 7. Trade-off between type 1 and 2 errors - changing significance level - slider??

## Type I error control

We set the rate of Type 1 errors through the significance level. A standard significance level of $\alpha=0.05$ gives a 5% false positive rate if the null hypothesis is true.

There are many recent arguments that $\alpha$ should be smaller than 0.05, such as Benjamin et al's (2018) argument that $\alpha$ should be set at 0.005. Many of these relate to the replication crisis that we will cover this later in this course. We simply want to generate less false positives.

### Multiple comparisons

One scenario where there is a longer history of using a smaller alpha is where you are testing multiple hypotheses. This could arise because you have multiple treatment groups or because you are measuring many potential outcomes.

If you are testing many hypotheses, it becomes increasingly likely that at least one of them will meet the statistical threshold merely by chance. If you conduct 20 tests and all of null hypothesis for each test is true, you expect one false positive.

https://imgs.xkcd.com/comics/significant.png

A common correction applied to $\alpha$ in instances of multiple comparisons is the Bonferroni correction. If you are testing *m* hypotheses, set the significance level for each hypothesis at $\frac{\alpha}{m}$. The Bonferroni correction is conservative in that it decreases the family-wise error rate - which is the probability of making one or more false discoveries - to below 0.05.

A Bonferroni type correction is typically applied where there is large-scale multiple testing. In genomic association studies, where they are testing of the order of a million genetic variants, they will normally set the significance level at 5x10$^{-8}$

A smaller significance level and associated higher critical value means, however, that we will get a higher rate of Type II errors. We will discuss this in the following pages.

### Replication data sets

An alternative method of Type I error control is use of a replication sample, which is a subset of the data that is excluded from the initial analysis. If there are any significant results in the first analysis, testing for those hypotheses that were significant in the first is conducted on the replication sample.

This approach is common in machine learning applications, but is starting to become more common in statistical analysis. However, it is rarely used in experimental analysis.

We will cover the concept of replication more broadly in week 6.

## Type II error control

The probability of a Type II error is $\beta$, the probability that you will not reject the null hypothesis when it is false.

A related concept is power, which is the probability that you will *reject* the null hypothesis when it is false. Power is equal to 1 - $\beta$.

Questions related to the Type II error rate for a trial are typically framed in terms of power. How can I design my trial so that it has enough power to reject the null hypothesis in circumstances where it is not true? Alternatively, how can I increase the probability that my trial will achieve statistical significance when the null hypothesis is not true?

### Calculating power

Statistical power can be calculated with the following variables:

**The significance level**: The stricter the significance level that you use in your experiment, the lower the power of the experiment. There is a trade-off between Type I and Type II errors. As you decrease the probability of false positives by using a stricter significance level, you also decrease the probability of true positives. You should not relax the significance level to achieve power, but should be aware of the consequences of making it more strict.

**The effect size**: What is the magnitude of the effect of your intervention? The larger the effect size, the more power the experiment has to detect an effect and generate a statistically significant outcome.

Unfortunately, as you have not yet run the experiment, you do not know what the effect size is. If there are other studies with which parallels can be drawn, you can make an educated guess as to its likely size. You might generate your estimate through a literature review, pilot studies or other related experiments.

Estimates of the effect size should be conservative. As we will see later in this unit, the experimental literature is full of over-exaggerated effect sizes. An overestimate of the effect size will overestimate the power of the experiment.

You also need to understand the variability of the effect size (such as its standard deviation in the population of interest) to calculate power. A highly-variable effect will have a higher standard error in its measurement, so more probability of getting an extreme result that generates an error.

As a result, when an effect size estimate is made, it it typically made as a standardised effect size. This is the magnitude of the effect divided by the standard deviation. How many standard deviations is the effect?

You don't have the ability to increase the effect size. However, variation in the effect size is affected by things such as measurement error. More precise measurement can increase power.

**The sample size**: Power is a function of the sample size of the experiment. It is the factor over which you have the most control.

Increasing the sample size reduces the standard error of your estimated effect size. This means that, if the alternative hypothesis is true, you will have a greater probability of detecting an effect of any given size. This can be seen in the following diagram: a larger sample size results in lower standard error, which is reflected in a narrower probability distribution for your estimate.

\[ADD GRAPHIC OF NORMAL DISTRIBUTION GETTING NARROWER\]

Beyond increasing power, larger samples have other benefits such as reducing the extent to which we overestimate the effect size. We will discuss this more in coming material.

Power is calculated by asking, given the critical value implied by the chosen significance level, what effect size is required such that 80% of the probability distribution for the estimated effect size will be above the critical value.

### Pre- and post-experiment power analysis

Power calculations should normally be done **before** an experiment. It is used to determine what sample size is required to obtain the requisite power. A common practice is obtaining a sample size sufficient to achieve 80% power, although there is little reason to limit yourself to that level of power if you can increase power at modest cost.

Power calculations are also often done after an experiment. You will see in the literature that post-experiment power calculations are regularly used to justify that the experiment had sufficient power, with the effect size found in the experiment the basis for the power calculation. This is poor practice, as a significant result in an underpowered experiment will tend to exaggerate the effect size. If you then use this exaggerated effect to calculate power, it gives the impression that the experiment was adequately powered. We will discuss this exaggeration of effect size later in the unit.

If a power calculation is done after the experiment, it should only be done using a well-grounded assumed effect size, not the effect size observed in the experiment.

An alternative use of post-experiment power calculations is to examine whether published experiments should be showing the proportion of significant results that they do. If a set of experiments has average power of 80%, only 80% of them should find a statistically significant effect. A higher proportion suggests publication bias (a topic later in the unit). The reading below examines a chapter of Daniel Kahneman's *Thinking, Fast and Slow* using this tool.

### Optional reading

Schimmack (2017) "Reconstruction of a Train Wreck: How Priming Research Went Off the Rails", *Replicability-Index*, https://replicationindex.com/2017/02/02/reconstruction-of-a-train-wreck-how-priming-research-went-of-the-rails/ (Note Daniel Kahneman's [response in the comments](https://replicationindex.com/2017/02/02/reconstruction-of-a-train-wreck-how-priming-research-went-of-the-rails/comment-page-1/#comment-1454).)

## Effect sizes

Statistical significance is not the same as practical importance. A statistically significant result may not be large enough to matter in practice. You are interested not just in whether a treatment affects people, but also how much.

For example, which is the more interesting result? A statistically significant experimental treatment that could boost the financial wellbeing of all Australian by \$2 (p=0.04). Or a non-significant experimental treatment that could boost the financial wellbeing of all Australians by \$1000 (p=0.06)?

Effect sizes from experiments should be interpreted and reported with caution. While often reported as a point estimate, we can provide a confidence interval around the estimated effect size. The confidence interval for an effect size with a p-value marginally below 0.05 will have a confidence interval with a lower end only marginally above 0. As we will discuss later in this unit, effect sizes are often exaggerations of the true effect size.

### Cohen's *d*

One common way in which effect size's are talked about is "Cohen's *d*". Cohen's *d* is defined as the difference of two means divided by the standard deviation of the data. It is calculated as:

$d=\frac{\bar{x}_1-\bar{x}_0}{s}=\frac{\mu_1-\mu_0}{s}$

where *s* is the pooled standard deviation of the data (you don't have to know how to calculate that.)

Cohen's *d* has the benefit of translating effect sizes in different experiments onto a common scale. You can speak of how many standard deviations an effect size is.

One legacy of Cohen's that you will often encounter is that he also labelled different sizes of Cohen's *d*. A Cohen's *d* of 0.2 is a small effect, 0.5 is medium, and 0.8 is a large. When people are calculating power for an experiment, they will often think in terms of whether the effect is small, medium or large, and use the associated number. One way this can mislead, however, is that many effect sizes in the social sciences are far less than 0.2.

### Summarising effect sizes

One place you often see transparent communication of effect sizes are in meta-analyses (summaries of the literature) or multi-lab replications. (More on these later in the unit.) Below is one example. (Note that the effect sizes are standardised as Cohen's *d*.)

#### The jam experiment

On two Saturdays in a California supermarket, Mark Lepper and Sheena Iyengar set up tasting displays of either six or 24 jars of jam. Consumers could taste as many jams as they wished, and if they approached the tasting table they received a \$1 discount coupon to buy the jam.

For attracting initial interest, the large display of 24 jams did a better job, with 60 per cent of people who passed the display stopping. Forty per cent of people stopped at the six jam display. But only three per cent of those who stopped at the 24 jam display purchased any of the jam, compared with almost 30 per cent who stopped at the six jam display.

This result has become the classical example of the "paradox of choice". More choice can lead us to fail to make a choice.

Later, Benjamin Scheibehenne and friends surveyed the literature on the choice overload hypothesis. This chart is a a plot of the effect sizes across the literature. Of those that are significant - that is, those for which the 95% confidence interval does not contain zero - they have a large point estimate of effect size, yet a 95% confidence interval barely excluding zero.

Looking across these experiments, in some cases, choice increases purchases. In others it reduces them. Scheibehenne and friends determined that the mean effect size of changing the number of choices across the studies was effectively zero.

![](img/scheibehenne.jpg)



### Cluster randomisation

Sometimes it is not practicable to randomise experimental subjects individually. For example, suppose you are implementing a trial to improve on-time tax return submissions by phoning taxpayers. You want to test what scripts and tools for call centre staff are most effective in increasing on-time submission. Training in the tools are provided at staff briefings at the beginning of each shift, making it impracticable to randomise across call centre staff within the centre.

A common approach to deal with this problem is *cluster randomisation*. In cluster randomisation, groups of subjects are randomised. If you have 20 call centres, randomise those 20 into the treatment and control.

Cluster randomisation is also used to control for contamination across individuals, as a change in behaviour in one might change the behaviour of others. In our example above, even if it were possible to train staff separately with different treatments, they may become aware of other approaches of staff in their centre and change their behaviour as a result. If that is a concern, cluster randomisation might address this.

Cluster randomisation has costs. It introduces greater complexity into the analysis, including introducing potential intracluster correlation that should be accounted for. Cluster randomisation also reduces power by effectively reducing the sample size.

#

## Module 2 summary

In this module, we have examined the process of conducting a randomised controlled trial, how to test hypotheses, and the types of errors that can be generated.

As a warm up for the next module, watch this interview of Professor Dorothy Bishop by Sabine Hossenfelder.

https://youtu.be/v778svukrtU

## References

Benjamin et al (2018) "Redefine statistical significance", *Nature Human Behaviour*, 2, 6-10, https://doi.org/10.1038/s41562-017-0189-z

Iyengar and Lepper (2000) "When choice is demotivating: Can one desire too much of a good thing?", *Journal of Personality and Social Psychology*, 79(6), 995--1006, https://doi.org/10.1037/0022-3514.79.6.995

Kristal et al (2020) "When We're Wrong, It's Our Responsibility as Scientists to Say So", *Scientific American*, https://blogs.scientificamerican.com/observations/when-were-wrong-its-our-responsibility-as-scientists-to-say-so/

List, Sadoff and Magner (2010) "So you want to run an experiment, now what?Some simple rules of thumb for optimal experimental design", *Experimental Economics*, https://doi.org/10.1007/s10683-011-9275-7

Scheibehenne, Greifeneder and Todd (2010) "Can There Ever Be Too Many Options? A Meta-Analytic Review of Choice Overload", *Journal of Consumer Research*, 37(3), 409--425, https://doi.org/10.1086/651235
